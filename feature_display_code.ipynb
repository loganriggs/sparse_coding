{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "\n",
    "features = 1\n",
    "autoencoder_path = \"/mnt/ssd-cluster/longrun2408/tied_residual_l2_r6/_31/learned_dicts.pt\"\n",
    "autoencoder_index = 5\n",
    "layer = 10\n",
    "dataset_name = \"NeelNanda/pile-10k\"\n",
    "device = \"cuda:0\"\n",
    "# model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "model_name=\"usvsnsp/pythia-6.9b-rm-full-hh-rlhf\"\n",
    "max_seq_length=30\n",
    "input_setting = \"input_only\"\n",
    "model_type=\"reward_model\"\n",
    "# input_setting = \"both\"\n",
    "\n",
    "# model_id = \"Elriggs/pythia-70m-deduped\"\n",
    "model_id = \"Elriggs/pythia-6.9-rm\"\n",
    "filename = f\"pythia-6.9b-rm-full-hh-rlhf_sp409_r4_gpt_neox.layers.{layer}.pt\"\n",
    "# filename = f\"tied_residual_l{layer}_r6/_63/learned_dicts.pt\" \n",
    "ae_download_location = hf_hub_download(repo_id=model_id, filename=filename)\n",
    "# all_autoencoders = torch.load(ae_download_location)\n",
    "# num_l1s = len(all_autoencoders)\n",
    "# all_l1s = [hyperparams[\"l1_alpha\"] for autoencoder, hyperparams in all_autoencoders]\n",
    "# print(all_l1s)\n",
    "# auto_num = 5\n",
    "# autoencoder, hyperparams = all_autoencoders[auto_num]\n",
    "autoencoder = torch.load(ae_download_location)\n",
    "autoencoder.to_device(device)\n",
    "cache_name = f\"gpt_neox.layers.{layer}\"\n",
    "\n",
    "if(model_type == \"reward_model\"):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# model_name=\"usvsnsp/pythia-6.9b-rm-full-hh-rlhf\"\n",
    "# device = \"cuda:0\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# text = \"The dog\"\n",
    "# tokens = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "# output = model(tokens)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(dataset_name, tokenizer, max_length=256, num_datapoints=None):\n",
    "    if(num_datapoints):\n",
    "        split_text = f\"train[:{num_datapoints}]\"\n",
    "    else:\n",
    "        split_text = \"train\"\n",
    "    dataset = load_dataset(dataset_name, split=split_text).map(\n",
    "        lambda x: tokenizer(x['text']),\n",
    "        batched=True,\n",
    "    ).filter(\n",
    "        lambda x: len(x['input_ids']) > max_length\n",
    "    ).map(\n",
    "        lambda x: {'input_ids': x['input_ids'][:max_length]}\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "print(f\"Downloading {dataset_name}\")\n",
    "dataset = download_dataset(dataset_name, tokenizer=tokenizer, max_length=max_seq_length, num_datapoints=None) # num_datapoints grabs all of them if None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_interp import *\n",
    "import os\n",
    "# make features/ dir if not exist\n",
    "save_path = \"features/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "num_feature_datapoints = 10\n",
    "dictionary_activations, tokens_for_each_datapoint = get_dictionary_activations(model, dataset, cache_name, max_seq_length, autoencoder, batch_size=32)\n",
    "\n",
    "# features = [1,2,3,4,5,6,7,8,9,10]\n",
    "# features = [5,6]\n",
    "# features = None\n",
    "num_features = 30\n",
    "feature = 0\n",
    "for _ in range(num_features):\n",
    "    # Check if feature is dead (<10 activations)\n",
    "    dead_threshold = 10\n",
    "    # if(dictionary_activations[:, current_feature].count_nonzero() < dead_threshold):\n",
    "    while(dictionary_activations[:, feature].count_nonzero() < dead_threshold):\n",
    "        print(f\"Feature {feature} is dead\")\n",
    "        feature += 1\n",
    "    uniform_indices = get_feature_indices(feature, dictionary_activations, k=num_feature_datapoints, setting=\"uniform\")\n",
    "    text_list, full_text, token_list, full_token_list, partial_activations, full_activations = get_feature_datapoints(uniform_indices, dictionary_activations[:, feature], tokenizer, max_seq_length, dataset)\n",
    "    get_token_statistics(feature, dictionary_activations[:, feature], dataset, tokenizer, max_seq_length, tokens_for_each_datapoint, save_location = save_path, num_unique_tokens=10)\n",
    "    if(input_setting == \"input_only\"):\n",
    "        # Calculate logit diffs on this feature for the full_token_list\n",
    "        logit_diffs = ablate_feature_direction(model, full_token_list, cache_name, max_seq_length, autoencoder, feature = feature, batch_size=32, setting=\"sentences\", model_type=model_type)\n",
    "        save_token_display(full_token_list, full_activations, tokenizer, path =f\"{save_path}uniform_{feature}.png\", logit_diffs = logit_diffs, model_type=model_type)\n",
    "    else:\n",
    "        logit_diffs = ablate_feature_direction(model, dataset, cache_name, max_seq_length, autoencoder, feature = feature, batch_size=32, setting=\"dataset\")\n",
    "        _, _, _, full_token_list_ablated, _, full_activations_ablated = get_feature_datapoints(uniform_indices, logit_diffs, tokenizer, max_seq_length, dataset, model_type=model_type)\n",
    "        get_token_statistics(feature, logit_diffs, dataset, tokenizer, max_seq_length, tokens_for_each_datapoint, save_location = save_path, setting=\"output\", num_unique_tokens=10)\n",
    "        save_token_display(full_token_list_ablated, full_activations, tokenizer, path =f\"{save_path}uniform_{feature}.png\", logit_diffs = full_activations_ablated)\n",
    "    combine_images(feature, setting=input_setting)\n",
    "    feature += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_token_statistics(feature, dictionary_activations[:, feature], dataset, tokenizer, max_seq_length, tokens_for_each_datapoint, save_location = save_path, num_unique_tokens=10)\n",
    "setting = \"input\"\n",
    "feature_activation = dictionary_activations[:, feature]\n",
    "save_location=\"\"\n",
    "num_unique_tokens=10\n",
    "negative_threshold=-0.01\n",
    "\n",
    "if(setting==\"input\"):\n",
    "    nonzero_indices = feature_activation.nonzero()[:, 0]  # Get the nonzero indices\n",
    "else:\n",
    "    nonzero_indices = (feature_activation < negative_threshold).nonzero()[:, 0]\n",
    "nonzero_values = feature_activation[nonzero_indices].abs()  # Get the nonzero values\n",
    "\n",
    "# Unravel the indices to get the token IDs\n",
    "datapoint_indices = [np.unravel_index(i, (dataset.num_rows, max_seq_length)) for i in nonzero_indices]\n",
    "all_tokens = [dataset[int(md)][\"input_ids\"][int(s_ind)] for md, s_ind in datapoint_indices]\n",
    "\n",
    "# Find the max value for each unique token\n",
    "token_value_dict = defaultdict(int)\n",
    "for token, value in zip(all_tokens, nonzero_values):\n",
    "    token_value_dict[token] = max(token_value_dict[token], value)\n",
    "# if(setting==\"input\"):\n",
    "sorted_tokens = sorted(token_value_dict.keys(), key=lambda x: -token_value_dict[x])\n",
    "# else:\n",
    "#     sorted_tokens = sorted(token_value_dict.keys(), key=lambda x: token_value_dict[x])\n",
    "# Take the top 10 (or fewer if there aren't 10)\n",
    "max_tokens = sorted_tokens[:min(num_unique_tokens, len(sorted_tokens))]\n",
    "total_sums = nonzero_values.abs().sum()\n",
    "max_token_sums = []\n",
    "token_activations = []\n",
    "assert len(max_tokens) > 0, \"poo poo pee pee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_activation.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_activations.count_nonzero(dim=0)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(full_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run example text through model\n",
    "text = \"I like to eat pizza.\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "# outputs = model(tokens)\n",
    "# outputs.logits\n",
    "\n",
    "device = model.device\n",
    "def less_than_rank_1_ablate(value, hook):\n",
    "    # Only ablate the feature direction up to the negative bias\n",
    "    # ie Only subtract when it activates above that negative bias.\n",
    "\n",
    "    # Rearrange to fit autoencoder\n",
    "    int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "    # Run through the autoencoder\n",
    "    act = autoencoder.encode(int_val)\n",
    "    dictionary_for_this_autoencoder = autoencoder.get_learned_dict()\n",
    "    feature_direction = torch.outer(act[:, feature].squeeze(), dictionary_for_this_autoencoder[feature].squeeze())\n",
    "    batch, seq_len, hidden_size = value.shape\n",
    "    feature_direction = rearrange(feature_direction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "    value -= feature_direction\n",
    "    return value\n",
    "\n",
    "# with Trace(model, cache_name, edit_output=less_than_rank_1_ablate) as ret:\n",
    "#     _ = model(batch).logits\n",
    "#     representation = ret.output\n",
    "#     # check if instance tuple\n",
    "#     if(isinstance(representation, tuple)):\n",
    "#         representation = representation[0]\n",
    "\n",
    "\n",
    "batch = tokens\n",
    "original_logits = model(batch).logits.log_softmax(dim=-1)\n",
    "# with Trace(model, cache_name, edit_output=less_than_rank_1_ablate) as ret:\n",
    "with Trace(model, cache_name, edit_output=less_than_rank_1_ablate) as ret:\n",
    "    ablated_logits = model(batch).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def ablate_feature_direction(model, dataset, cache_name, max_seq_length, autoencoder, feature, batch_size=32):\n",
    "\n",
    "device = model.cfg.device\n",
    "def less_than_rank_1_ablate(value, hook):\n",
    "    # Only ablate the feature direction up to the negative bias\n",
    "    # ie Only subtract when it activates above that negative bias.\n",
    "    # Rearrange to fit autoencoder\n",
    "    int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "    # Run through the autoencoder\n",
    "    act = autoencoder.encode(int_val)\n",
    "    dictionary_for_this_autoencoder = autoencoder.get_learned_dict()\n",
    "    feature_direction = torch.outer(act[:, feature].squeeze(), dictionary_for_this_autoencoder[feature].squeeze())\n",
    "    batch, seq_len, hidden_size = value.shape\n",
    "    feature_direction = rearrange(feature_direction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "    value -= feature_direction\n",
    "    return value\n",
    "\n",
    "# tokens_batched = full_tokens_list\n",
    "tokens_batched = torch.stack(full_token_list)\n",
    "logit_diffs = torch.zeros((tokens_batched.shape[0], tokens_batched.shape[1]-1))\n",
    "\n",
    "with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "    original_logits = model(tokens_batched.to(device)).log_softmax(dim=-1)\n",
    "    ablated_logits = model.run_with_hooks(tokens_batched.to(device), fwd_hooks=[(cache_name, less_than_rank_1_ablate)]).log_softmax(dim=-1)\n",
    "    diff_logits = ablated_logits  - original_logits# ablated > original -> negative diff\n",
    "    gather_tokens = rearrange(tokens_batched[:,1:].to(device), \"b s -> b s 1\")\n",
    "    gathered = diff_logits[:, :-1].gather(-1,gather_tokens)\n",
    "    # append all 0's to the beggining of gathered\n",
    "    gathered = torch.cat([torch.zeros((gathered.shape[0],1,1)).to(device), gathered], dim=1)\n",
    "    logit_diffs = rearrange(gathered, \"b s n -> (b s n)\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_colorbar(min_value, max_value, white = 255, red_blue_ness = 250, positive_threshold = 0.01, negative_threshold = 0.01):\n",
    "    # Add color bar\n",
    "    colorbar = \"\"\n",
    "    num_colors = 4\n",
    "    if(min_value < -negative_threshold):\n",
    "        for i in range(num_colors, 0, -1):\n",
    "            ratio = i / (num_colors)\n",
    "            value = round((min_value*ratio),1)\n",
    "            text_color = \"255,255,255\" if ratio > 0.5 else \"0,0,0\"\n",
    "            colorbar += f'<span style=\"background-color:rgba(255, {int(red_blue_ness-(red_blue_ness*ratio))},{int(red_blue_ness-(red_blue_ness*ratio))},1); color:rgb({text_color})\">&nbsp{value}&nbsp</span>'\n",
    "    # Do zero\n",
    "    colorbar += f'<span style=\"background-color:rgba({white},{white},{white},1);color:rgb(0,0,0)\">&nbsp0.0&nbsp</span>'\n",
    "    # Do positive\n",
    "    if(max_value > positive_threshold):\n",
    "        for i in range(1, num_colors+1):\n",
    "            ratio = i / (num_colors)\n",
    "            value = round((max_value*ratio),1)\n",
    "            text_color = \"255,255,255\" if ratio > 0.5 else \"0,0,0\"\n",
    "            colorbar += f'<span style=\"background-color:rgba({int(red_blue_ness-(red_blue_ness*ratio))},{int(red_blue_ness-(red_blue_ness*ratio))},255,1);color:rgb({text_color})\">&nbsp{value}&nbsp</span>'\n",
    "    return colorbar\n",
    "\n",
    "def value_to_color(activation, max_value, min_value, white = 255, red_blue_ness = 250, positive_threshold = 0.01, negative_threshold = 0.01):\n",
    "    if activation > positive_threshold:\n",
    "        ratio = activation/max_value\n",
    "        text_color = \"0,0,0\" if ratio <= 0.5 else \"255,255,255\"  \n",
    "        background_color = f'rgba({int(red_blue_ness-(red_blue_ness*ratio))},{int(red_blue_ness-(red_blue_ness*ratio))},255,1)'\n",
    "    elif activation < -negative_threshold:\n",
    "        ratio = activation/min_value\n",
    "        text_color = \"0,0,0\" if ratio <= 0.5 else \"255,255,255\"  \n",
    "        background_color = f'rgba(255, {int(red_blue_ness-(red_blue_ness*ratio))},{int(red_blue_ness-(red_blue_ness*ratio))},1)'\n",
    "    else:\n",
    "        text_color = \"0,0,0\"\n",
    "        background_color = f'rgba({white},{white},{white},1)'\n",
    "    return text_color, background_color\n",
    "\n",
    "def convert_token_array_to_list(array):\n",
    "    if isinstance(array, torch.Tensor):\n",
    "        if array.dim() == 1:\n",
    "            array = [array.tolist()]\n",
    "        elif array.dim()==2:\n",
    "            array = array.tolist()\n",
    "        else: \n",
    "            raise NotImplementedError(\"tokens must be 1 or 2 dimensional\")\n",
    "    elif isinstance(array, list):\n",
    "        # ensure it's a list of lists\n",
    "        if isinstance(array[0], int):\n",
    "            array = [array]\n",
    "    return array\n",
    "\n",
    "def tokens_and_activations_to_html(toks, activations, tokenizer, logit_diffs=None):\n",
    "    # text_spacing = \"0.07em\"\n",
    "    text_spacing = \"0.00em\"\n",
    "    toks = convert_token_array_to_list(toks)\n",
    "    activations = convert_token_array_to_list(activations)\n",
    "    # toks = [[tokenizer.decode(t).replace('Ġ', '&nbsp').replace('\\n', '↵') for t in tok] for tok in toks]\n",
    "    toks = [[tokenizer.decode(t).replace('Ġ', '&nbsp').replace('\\n', '\\\\n') for t in tok] for tok in toks]\n",
    "    highlighted_text = []\n",
    "    # Make background black\n",
    "    # highlighted_text.append('<body style=\"background-color:black; color: white;\">')\n",
    "    highlighted_text.append(\"\"\"\n",
    "<body style=\"background-color: black; color: white;\">\n",
    "\"\"\")\n",
    "    max_value = max([max(activ) for activ in activations])\n",
    "    min_value = min([min(activ) for activ in activations])\n",
    "    if(logit_diffs):\n",
    "        logit_max_value = max([max(activ) for activ in logit_diffs])\n",
    "        logit_min_value = min([min(activ) for activ in logit_diffs])\n",
    "\n",
    "    # Add color bar\n",
    "    highlighted_text.append(\"Token Activations: \" + make_colorbar(min_value, max_value))\n",
    "    if(logit_diffs):\n",
    "        highlighted_text.append('<div style=\"margin-top: 0.1em;\"></div>')\n",
    "        highlighted_text.append(\"Logit Diff: \" + make_colorbar(logit_min_value, logit_max_value))\n",
    "    \n",
    "    highlighted_text.append('<div style=\"margin-top: 0.5em;\"></div>')\n",
    "    for seq_ind, (act, tok) in enumerate(zip(activations, toks)):\n",
    "        for act_ind, (a, t) in enumerate(zip(act, tok)):\n",
    "            if(logit_diffs):\n",
    "                highlighted_text.append('<div style=\"display: inline-block;\">')\n",
    "            text_color, background_color = value_to_color(a, max_value, min_value)\n",
    "            highlighted_text.append(f'<span style=\"background-color:{background_color};margin-right: {text_spacing}; color:rgb({text_color})\">{t.replace(\" \", \"&nbsp\")}</span>')\n",
    "            if(logit_diffs):\n",
    "                logit_diffs_act = logit_diffs[seq_ind][act_ind]\n",
    "                _, logit_background_color = value_to_color(logit_diffs_act, logit_max_value, logit_min_value)\n",
    "                highlighted_text.append(f'<div style=\"display: block; margin-right: {text_spacing}; height: 10px; background-color:{logit_background_color}; text-align: center;\"></div></div>')\n",
    "        highlighted_text.append('<div style=\"margin-top: 0.2em;\"></div>')\n",
    "        # highlighted_text.append('<br><br>')\n",
    "    # highlighted_text.append('</body>')\n",
    "    highlighted_text = ''.join(highlighted_text)\n",
    "    return highlighted_text\n",
    "\n",
    "def display_tokens(tokens, activations, tokenizer, logit_diffs=None):\n",
    "    return display(HTML(tokens_and_activations_to_html(tokens, activations, tokenizer, logit_diffs)))\n",
    "\n",
    "def save_token_display(tokens, activations, tokenizer, path, logit_diffs=None):\n",
    "    html = tokens_and_activations_to_html(tokens, activations, tokenizer, logit_diffs)\n",
    "    imgkit.from_string(html, path)\n",
    "    # print(f\"Saved to {path}\")\n",
    "    return\n",
    "save_token_display(full_token_list_ablated, full_activations, model.tokenizer, path =f\"{save_path}uniform_black_{feature}.png\", logit_diffs = full_activations_ablated)\n",
    "# display_tokens(full_token_list_ablated, full_activations, model.tokenizer, logit_diffs = full_activations_ablated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Load an existing image\n",
    "image = Image.open(f\"concatenated_image.png\")\n",
    "\n",
    "# Create a drawing object\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Define the position for the text to be placed (this example puts it at the top-center)\n",
    "text = \"Feature {feature}\"\n",
    "text_position = (image.width // 2, 10)\n",
    "\n",
    "# You can use a truetype or opentype font file if you want to customize\n",
    "# Otherwise, PIL provides a basic built-in font\n",
    "try:\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=30)\n",
    "except IOError:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "# Calculate text size and position to center-align the text\n",
    "text_width, text_height = draw.textsize(text, font=font)\n",
    "text_position = ((image.width - text_width) // 2, 100)\n",
    "\n",
    "# Add the text to image\n",
    "draw.text(text_position, text, font=font, fill=\"white\")\n",
    "\n",
    "# Save or show the image\n",
    "image.save(\"image_with_title.png\")\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font.font.getsize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_position, text, font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# The initial HTML and CSS to set the background to black\n",
    "initial_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>My Page</title>\n",
    "</head>\n",
    "<body style=\"background-color: blue;\">\n",
    "\"\"\"\n",
    "\n",
    "# The HTML for the text spans that you'll append later\n",
    "span_text = \"\"\"\n",
    "    <span style=\"color: white;\">This is some text.</span>\n",
    "    <span style=\"color: green;\">This is some more text.</span>\n",
    "\"\"\"\n",
    "\n",
    "# Closing tags for the HTML\n",
    "closing_html = \"\"\"\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Combine everything\n",
    "full_html = initial_html + span_text + closing_html\n",
    "\n",
    "# Display the HTML\n",
    "display(HTML(full_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cfg.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = model.cfg.d_model\n",
    "assert (d_model == autoencoder.encoder.shape[-1]), f\"Model and autoencoder must have same hidden size. Model: {d_model}, Autoencoder: {autoencoder.encoder.shape[-1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "def get_dictionary_activations(model, dataset, cache_name, autoencoder, batch_size=32):\n",
    "    num_features, d_model = autoencoder.encoder.shape\n",
    "    datapoints = dataset.num_rows\n",
    "    dictionary_activations = torch.zeros((datapoints*max_seq_length, num_features))\n",
    "    token_list = torch.zeros((datapoints*max_seq_length), dtype=torch.int64)\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(tqdm(dl)):\n",
    "            token_list[i*batch_size*max_seq_length:(i+1)*batch_size*max_seq_length] = rearrange(batch, \"b s -> (b s)\")\n",
    "            _, cache = model.run_with_cache(batch.to(device))\n",
    "            batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "            batched_dictionary_activations = autoencoder.encode(batched_neuron_activations)\n",
    "            dictionary_activations[i*batch_size*max_seq_length:(i+1)*batch_size*max_seq_length,:] = batched_dictionary_activations.cpu()\n",
    "    return dictionary_activations, token_list\n",
    "\n",
    "print(\"Getting dictionary activations\")\n",
    "dictionary_activations, tokens_for_each_datapoint = get_dictionary_activations(model, dataset, cache_name, autoencoder, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ablate_feature_direction(model, dataset, cache_name, autoencoder, feature, batch_size=32):\n",
    "    def less_than_rank_1_ablate(value, hook):\n",
    "        # Only ablate the feature direction up to the negative bias\n",
    "        # ie Only subtract when it activates above that negative bias.\n",
    "\n",
    "        # Rearrange to fit autoencoder\n",
    "        int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "        # Run through the autoencoder\n",
    "        act = autoencoder.encode(int_val)\n",
    "        dictionary_for_this_autoencoder = autoencoder.get_learned_dict()\n",
    "        feature_direction = torch.outer(act[:, feature].squeeze(), dictionary_for_this_autoencoder[feature].squeeze())\n",
    "        batch, seq_len, hidden_size = value.shape\n",
    "        feature_direction = rearrange(feature_direction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "        value -= feature_direction\n",
    "        return value\n",
    "\n",
    "    datapoints = dataset.num_rows\n",
    "    logit_diffs = torch.zeros((datapoints*max_seq_length))\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(tqdm(dl)):\n",
    "            original_logits = model(batch.to(device)).log_softmax(dim=-1)\n",
    "            ablated_logits = model.run_with_hooks(batch.to(device), fwd_hooks=[(cache_name, less_than_rank_1_ablate)]).log_softmax(dim=-1)\n",
    "            diff_logits = ablated_logits  - original_logits# ablated > original -> negative diff\n",
    "            gather_tokens = rearrange(batch[:,1:].to(device), \"b s -> b s 1\")\n",
    "            gathered = diff_logits[:, :-1].gather(-1,gather_tokens)\n",
    "            # append all 0's to the beggining of gathered\n",
    "            gathered = torch.cat([torch.zeros((gathered.shape[0],1,1)).to(device), gathered], dim=1)\n",
    "            diff = rearrange(gathered, \"b s n -> (b s n)\")\n",
    "            # Add one to the first position of logit diff, so we're always skipping over the first token (since it's not predicted)\n",
    "            logit_diffs[i*batch_size*max_seq_length:(i+1)*batch_size*max_seq_length] = diff.cpu()\n",
    "    return logit_diffs\n",
    "feature = 1\n",
    "logit_diffs = ablate_feature_direction(model, dataset, cache_name, autoencoder, feature = feature, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from interp_utils import *\n",
    "# if isinstance(features, int):\n",
    "#     features = [features]\n",
    "# for feature in features:\n",
    "#     text_list, full_text, token_list, full_token_list = get_feature_datapoints(feature, dictionary_activations, model.tokenizer, max_seq_length, dataset, setting=\"uniform\")\n",
    "#     # text_list, full_text, token_list, full_token_list = get_feature_datapoints(feature, dictionary_activations, dataset, setting=\"max\")\n",
    "#     # visualize_text(full_text, feature, model, autoencoder, layer)\n",
    "# l = visualize_text(text_list, feature, model, autoencoder, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import imgkit\n",
    "\n",
    "def make_colorbar(min_value, max_value, white = 245, red_blue_ness = 250, positive_threshold = 0.01, negative_threshold = 0.01):\n",
    "    # Add color bar\n",
    "    colorbar = \"\"\n",
    "    num_colors = 4\n",
    "    if(min_value < -negative_threshold):\n",
    "        for i in range(num_colors, 0, -1):\n",
    "            ratio = i / (num_colors)\n",
    "            value = round((min_value*ratio),1)\n",
    "            text_color = \"255,255,255\" if ratio > 0.5 else \"0,0,0\"\n",
    "            colorbar += f'<span style=\"background-color:rgba(255, {int(red_blue_ness-(red_blue_ness*ratio))},{int(red_blue_ness-(red_blue_ness*ratio))},1); color:rgb({text_color})\">&nbsp{value}&nbsp</span>'\n",
    "    # Do zero\n",
    "    colorbar += f'<span style=\"background-color:rgba({white},{white},{white},1);color:rgb(0,0,0)\">&nbsp0.0&nbsp</span>'\n",
    "    # Do positive\n",
    "    if(max_value > positive_threshold):\n",
    "        for i in range(1, num_colors+1):\n",
    "            ratio = i / (num_colors)\n",
    "            value = round((max_value*ratio),1)\n",
    "            text_color = \"255,255,255\" if ratio > 0.5 else \"0,0,0\"\n",
    "            colorbar += f'<span style=\"background-color:rgba({int(red_blue_ness-(red_blue_ness*ratio))},{int(red_blue_ness-(red_blue_ness*ratio))},255,1);color:rgb({text_color})\">&nbsp{value}&nbsp</span>'\n",
    "    return colorbar\n",
    "\n",
    "def value_to_color(activation, max_value, min_value, white = 245, red_blue_ness = 250, positive_threshold = 0.01, negative_threshold = 0.01):\n",
    "    if activation > positive_threshold:\n",
    "        ratio = activation/max_value\n",
    "        text_color = \"0,0,0\" if ratio <= 0.5 else \"255,255,255\"  \n",
    "        background_color = f'rgba({int(red_blue_ness-(red_blue_ness*ratio))},{int(red_blue_ness-(red_blue_ness*ratio))},255,1)'\n",
    "    elif activation < -negative_threshold:\n",
    "        ratio = activation/min_value\n",
    "        text_color = \"0,0,0\" if ratio <= 0.5 else \"255,255,255\"  \n",
    "        background_color = f'rgba(255, {int(red_blue_ness-(red_blue_ness*ratio))},{int(red_blue_ness-(red_blue_ness*ratio))},1)'\n",
    "    else:\n",
    "        text_color = \"0,0,0\"\n",
    "        background_color = f'rgba({white},{white},{white},1)'\n",
    "    return text_color, background_color\n",
    "\n",
    "def convert_token_array_to_list(array):\n",
    "    if isinstance(array, torch.Tensor):\n",
    "        if array.dim() == 1:\n",
    "            array = [array.tolist()]\n",
    "        elif array.dim()==2:\n",
    "            array = array.tolist()\n",
    "        else: \n",
    "            raise NotImplementedError(\"tokens must be 1 or 2 dimensional\")\n",
    "    elif isinstance(array, list):\n",
    "        # ensure it's a list of lists\n",
    "        if isinstance(array[0], int):\n",
    "            array = [array]\n",
    "    return array\n",
    "\n",
    "def tokens_and_activations_to_html(toks, activations, tokenizer, logit_diffs=None):\n",
    "    toks = convert_token_array_to_list(toks)\n",
    "    activations = convert_token_array_to_list(activations)\n",
    "    # toks = [[tokenizer.decode(t).replace('Ġ', '&nbsp').replace('\\n', '↵') for t in tok] for tok in toks]\n",
    "    toks = [[tokenizer.decode(t).replace('Ġ', '&nbsp').replace('\\n', '\\\\n') for t in tok] for tok in toks]\n",
    "    highlighted_text = []\n",
    "    max_value = max([max(activ) for activ in activations])\n",
    "    min_value = min([min(activ) for activ in activations])\n",
    "    if(logit_diffs):\n",
    "        logit_max_value = max([max(activ) for activ in logit_diffs])\n",
    "        logit_min_value = min([min(activ) for activ in logit_diffs])\n",
    "\n",
    "    # Add color bar\n",
    "    highlighted_text.append(\"Token Activations: \" + make_colorbar(min_value, max_value))\n",
    "    if(logit_diffs):\n",
    "        highlighted_text.append('<br><br>')\n",
    "        highlighted_text.append(\"Logit Diff: \" + make_colorbar(logit_min_value, logit_max_value))\n",
    "        \n",
    "    highlighted_text.append('<br><br>')\n",
    "    for seq_ind, (act, tok) in enumerate(zip(activations, toks)):\n",
    "        for act_ind, (a, t) in enumerate(zip(act, tok)):\n",
    "            if(logit_diffs):\n",
    "                highlighted_text.append('<div style=\"display: inline-block;\">')\n",
    "            text_color, background_color = value_to_color(a, max_value, min_value)\n",
    "            highlighted_text.append(f'<span style=\"background-color:{background_color};color:rgb({text_color})\">{t.replace(\" \", \"&nbsp\")}</span>')\n",
    "            if(logit_diffs):\n",
    "                logit_diffs_act = logit_diffs[seq_ind][act_ind]\n",
    "                _, logit_background_color = value_to_color(logit_diffs_act, logit_max_value, logit_min_value)\n",
    "                highlighted_text.append(f'<div style=\"display: block; height: 10px; background-color:{logit_background_color}; text-align: center;\"></div></div>')\n",
    "        highlighted_text.append('<br><br>')\n",
    "    highlighted_text = ''.join(highlighted_text)\n",
    "    return highlighted_text\n",
    "\n",
    "def display_tokens(tokens, activations, tokenizer, logit_diffs=None):\n",
    "    return display(HTML(tokens_and_activations_to_html(tokens, activations, tokenizer, logit_diffs)))\n",
    "\n",
    "def save_token_display(tokens, activations, tokenizer, path, logit_diffs=None):\n",
    "    html = tokens_and_activations_to_html(tokens, activations, tokenizer, logit_diffs)\n",
    "    imgkit.from_string(html, path)\n",
    "    # print(f\"Saved to {path}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_indices(feature_index, dictionary_activations, tokenizer, token_amount, dataset, k=10, setting=\"max\"):\n",
    "    best_feature_activations = dictionary_activations[:, feature_index]\n",
    "    # Sort the features by activation, get the indices\n",
    "    if setting==\"max\":\n",
    "        found_indices = torch.argsort(best_feature_activations, descending=True)[:k]\n",
    "    elif setting==\"uniform\":\n",
    "        # min_value = torch.min(best_feature_activations)\n",
    "        min_value = torch.min(best_feature_activations)\n",
    "        max_value = torch.max(best_feature_activations)\n",
    "\n",
    "        # Define the number of bins\n",
    "        num_bins = k\n",
    "\n",
    "        # Calculate the bin boundaries as linear interpolation between min and max\n",
    "        bin_boundaries = torch.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Assign each activation to its respective bin\n",
    "        bins = torch.bucketize(best_feature_activations, bin_boundaries)\n",
    "\n",
    "        # Initialize a list to store the sampled indices\n",
    "        sampled_indices = []\n",
    "\n",
    "        # Sample from each bin\n",
    "        for bin_idx in torch.unique(bins):\n",
    "            if(bin_idx==0): # Skip the first one. This is below the median\n",
    "                continue\n",
    "            # Get the indices corresponding to the current bin\n",
    "            bin_indices = torch.nonzero(bins == bin_idx, as_tuple=False).squeeze(dim=1)\n",
    "            \n",
    "            # Randomly sample from the current bin\n",
    "            sampled_indices.extend(np.random.choice(bin_indices, size=1, replace=False))\n",
    "\n",
    "        # Convert the sampled indices to a PyTorch tensor & reverse order\n",
    "        found_indices = torch.tensor(sampled_indices).long().flip(dims=[0])\n",
    "    else: # random\n",
    "        # get nonzero indices\n",
    "        nonzero_indices = torch.nonzero(best_feature_activations)[:, 0]\n",
    "        # shuffle\n",
    "        shuffled_indices = nonzero_indices[torch.randperm(nonzero_indices.shape[0])]\n",
    "        found_indices = shuffled_indices[:k]\n",
    "    return found_indices\n",
    "def get_feature_datapoints(found_indices, best_feature_activations, tokenizer, token_amount, dataset):\n",
    "    num_datapoints = dataset.num_rows\n",
    "    datapoint_indices =[np.unravel_index(i, (num_datapoints, token_amount)) for i in found_indices]\n",
    "    all_activations = best_feature_activations.reshape(num_datapoints, token_amount).tolist()\n",
    "    full_activations = []\n",
    "    partial_activations = []\n",
    "    text_list = []\n",
    "    full_text = []\n",
    "    token_list = []\n",
    "    full_token_list = []\n",
    "    for i, (md, s_ind) in enumerate(datapoint_indices):\n",
    "        md = int(md)\n",
    "        s_ind = int(s_ind)\n",
    "        full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "        full_text.append(tokenizer.decode(full_tok))\n",
    "        tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "        full_activations.append(all_activations[md])\n",
    "        partial_activations.append(all_activations[md][:s_ind+1])\n",
    "        text = tokenizer.decode(tok)\n",
    "        text_list.append(text)\n",
    "        token_list.append(tok)\n",
    "        full_token_list.append(full_tok)\n",
    "    return text_list, full_text, token_list, full_token_list, partial_activations, full_activations\n",
    "\n",
    "uniform_indices = get_feature_indices(feature, dictionary_activations, model.tokenizer, max_seq_length, dataset, k=10, setting=\"uniform\")\n",
    "text_list, full_text, token_list, full_token_list, partial_activations, full_activations = get_feature_datapoints(uniform_indices, dictionary_activations[:, feature], model.tokenizer, max_seq_length, dataset)\n",
    "display_tokens(token_list, partial_activations, model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_token_statistics(feature, feature_activation, dataset, max_seq_length, save_location=\"\", num_unique_tokens=10, setting=\"input\", negative_threshold=-0.01):\n",
    "    if(setting==\"input\"):\n",
    "        nonzero_indices = feature_activation.nonzero()[:, 0]  # Get the nonzero indices\n",
    "    else:\n",
    "        nonzero_indices = (feature_activation < negative_threshold).nonzero()[:, 0]\n",
    "    nonzero_values = feature_activation[nonzero_indices].abs()  # Get the nonzero values\n",
    "\n",
    "    # Unravel the indices to get the token IDs\n",
    "    datapoint_indices = [np.unravel_index(i, (dataset.num_rows, max_seq_length)) for i in nonzero_indices]\n",
    "    all_tokens = [dataset[int(md)][\"input_ids\"][int(s_ind)] for md, s_ind in datapoint_indices]\n",
    "\n",
    "    # Find the max value for each unique token\n",
    "    token_value_dict = defaultdict(int)\n",
    "    for token, value in zip(all_tokens, nonzero_values):\n",
    "        token_value_dict[token] = max(token_value_dict[token], value)\n",
    "    # if(setting==\"input\"):\n",
    "    sorted_tokens = sorted(token_value_dict.keys(), key=lambda x: -token_value_dict[x])\n",
    "    # else:\n",
    "    #     sorted_tokens = sorted(token_value_dict.keys(), key=lambda x: token_value_dict[x])\n",
    "    # Take the top 10 (or fewer if there aren't 10)\n",
    "    max_tokens = sorted_tokens[:min(num_unique_tokens, len(sorted_tokens))]\n",
    "    total_sums = nonzero_values.abs().sum()\n",
    "    max_token_sums = []\n",
    "    token_activations = []\n",
    "    for max_token in max_tokens:\n",
    "        # Find ind of max token\n",
    "        max_token_indices = tokens_for_each_datapoint[nonzero_indices] == max_token\n",
    "        # Grab the values for those indices\n",
    "        max_token_values = nonzero_values[max_token_indices]\n",
    "        max_token_sum = max_token_values.abs().sum()\n",
    "        max_token_sums.append(max_token_sum)\n",
    "        token_activations.append(max_token_values)\n",
    "\n",
    "\n",
    "    if(setting==\"input\"):\n",
    "        title_text = \"Input Token Activations\"\n",
    "        save_name = \"input\"\n",
    "        y_label = \"Feature Activation\"\n",
    "    else:\n",
    "        title_text = \"Output Logit Difference\"\n",
    "        save_name = \"logit_diff\"\n",
    "        y_label = \"Logit Difference\"\n",
    "\n",
    "    # Plot a boxplot for each tensor in the list\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.set_title(f'{title_text}: feature={feature}')\n",
    "    max_text = [model.tokenizer.decode([t]).replace(\"\\n\", \"\\\\n\").replace(\" \", \"_\") for t in max_tokens]\n",
    "    # Set x-axis label\n",
    "    ax.set_xlabel('Token')\n",
    "    #rotate x labels\n",
    "    plt.xticks(rotation=30)\n",
    "    # Set y-axis label\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.boxplot(token_activations[::-1], labels=max_text[::-1])\n",
    "    #Save it\n",
    "    plt.savefig(f'{save_location}feature_{feature}_{save_name}_boxplot.png')\n",
    "\n",
    "    #Bar graph of the percentage of total activations\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.set_title(f'Weighted Percentage of {title_text}: feature={feature}')\n",
    "    max_text = [model.tokenizer.decode([t]).replace(\"\\n\", \"\\\\n\").replace(\" \", \"_\") for t in max_tokens]\n",
    "    # Set x-axis label\n",
    "    ax.set_xlabel('Token')\n",
    "    plt.xticks(rotation=30)\n",
    "\n",
    "    # Set y-axis label\n",
    "    ax.set_ylabel(f'Weighted Percentage of Total {y_label}')\n",
    "    ax.bar(max_text[::-1], [t/total_sums*100 for t in max_token_sums[::-1]])\n",
    "    plt.savefig(f'{save_location}feature_{feature}_{save_name}_bar.png')\n",
    "# get_token_statistics(feature, dictionary_activations[:, feature], dataset, max_seq_length, save_location = \"features/\", num_unique_tokens=10)\n",
    "get_token_statistics(feature, logit_diffs, dataset, max_seq_length, save_location = \"features/\", setting=\"output\", num_unique_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(logit_diffs < -0.01).nonzero()[:, 0], feature_activation.nonzero()[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, full_token_list_ablated, _, full_activations_ablated = get_feature_datapoints(uniform_indices, logit_diffs, model.tokenizer, max_seq_length, dataset)\n",
    "display_tokens(full_token_list_ablated, full_activations, model.tokenizer, logit_diffs = full_activations_ablated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_and_activations_to_html(full_token_list_ablated, full_activations, model.tokenizer, logit_diffs = full_activations_ablated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<div style=\"display: inline-block;\"><span style=\"background-color:rgba(245,245,245,1);color:rgb(0,0,0)\">About</span><div style=\"display: block; height: 10px; background-color:rgba(245,245,245,1); text-align: center;\"></div></div><div style=\"display: inline-block;\"><span style=\"background-color:rgba(245,245,245,1);color:rgb(0,0,0)\">\\\\n</span><div style=\"display: block; height: 10px; background-color:rgba(245,245,245,1); text-align: center;\"></div></div><div style=\"display: inline-block;\"><span style=\"background-color:rgba(245,245,245,1);color:rgb(0,0,0)\">\\\\n</span><div style=\"display: block; height: 10px; background-color:rgba(245,245,245,1); text-align: center;\"></div></div><div style=\"display: inline-block;\"><span style=\"background-color:rgba(245,245,245,1);color:rgb(0,0,0)\">Te</span><div style=\"display: block; height: 10px; background-color:rgba(245,245,245,1); text-align: center;\"></div></div><div style=\"display: inline-block;\"><span style=\"background-color:rgba(245,245,245,1);color:rgb(0,0,0)\">ams</span><div style=\"display: block; height: 10px; background-color:rgba(245,245,245,1); text-align: center;\"></div></div><div style=\"display: inline-block;\"><span style=\"background-color:rgba(245,245,245,1);color:rgb(0,0,0)\">&nbspare</span><div style=\"display: block; height: 10px; background-color:rgba(245,245,245,1); text-align: center;\"></div></div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def ablate_context_one_token_at_a_time(model, dataset, cache_name, autoencoder, feature, batch_size=32):\n",
    "all_changed_activations = []\n",
    "for token_ind, token_l in enumerate(token_list):\n",
    "# for token_ind, token_l in enumerate(full_token_list):\n",
    "    seq_size = len(token_l)\n",
    "    original_activation = partial_activations[token_ind][-1]\n",
    "    # Run through the model for each seq length\n",
    "    if(seq_size==1):\n",
    "        continue # Size 1 sequences don't have any context to ablate\n",
    "    # changed_activations = torch.zeros(seq_size).cpu() + original_activation\n",
    "    changed_activations = torch.zeros(seq_size).cpu() \n",
    "    for i in range(seq_size-1):\n",
    "        # ablated_tokens = token_l[:i+1] + token_l[i+1:]\n",
    "        ablated_tokens = token_l\n",
    "        ablated_tokens = torch.tensor(ablated_tokens).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(ablated_tokens.to(device))\n",
    "            neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "            dictionary_activations = autoencoder.encode(neuron_activations)\n",
    "\n",
    "            changed_activations[i] += dictionary_activations[-1,feature].item()\n",
    "    changed_activations -= original_activation\n",
    "    all_changed_activations.append(changed_activations.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tokens(token_list, all_changed_activations, model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_changed_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tokens(token_list, partial_activations, model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablate_feature_direction_display(full_text, autoencoder, model, layer, features=feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
