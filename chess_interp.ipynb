{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/logan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# Download the model\n",
    "device = \"cuda:0\"\n",
    "model_name=\"BlueSunflower/Pythia-70M-chess\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_seq_length=30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model_id = [\"jbrinkma/Pythia-70M-chess_sp51_r4_gpt_neox.layers.1\", \"jbrinkma/Pythia-70M-chess_sp51_r4_gpt_neox.layers.2.mlp\"]\n",
    "filename = \"sae.pt\"\n",
    "autoencoders = []\n",
    "for model_id in ae_model_id:\n",
    "    ae_download_location = hf_hub_download(repo_id=model_id, filename=filename)\n",
    "    autoencoder = torch.load(ae_download_location)\n",
    "    autoencoder.to_device(device)\n",
    "    autoencoders.append(autoencoder)\n",
    "cache_names = [\"_\".join(model_id.split(\"_\")[-2:]) for model_id in ae_model_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def download_data(): \n",
    "\n",
    "    # setup dir\n",
    "    data_dir = \"./data\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    file_path = os.path.join(data_dir, \"data_stockfish_262k.tar.gz\")\n",
    "    if not Path(file_path).is_file():\n",
    "\n",
    "        # load tar.gz file\n",
    "        r = requests.get(\"https://huggingface.co/datasets/BlueSunflower/chess_games_base/resolve/main/data_stockfish_262k.tar.gz\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "        # unpack tar.gz file\n",
    "        file = tarfile.open(file_path) \n",
    "        file.extractall(data_dir) \n",
    "        file.close() \n",
    "\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-a6a5482aabd33742/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-a6a5482aabd33742/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-58fdf3e322afcc8e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-a6a5482aabd33742/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-172a37628a616d5d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-a6a5482aabd33742/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1aed0094d3128987.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "local_path = \"data/test_stockfish_5000.json\"\n",
    "dataset = load_dataset(\"json\", data_files=local_path, split=\"train\").map(\n",
    "    lambda x: tokenizer(x['moves']),\n",
    "    batched=True\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > max_seq_length\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:max_seq_length]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Based method\n",
    "from einops import rearrange\n",
    "from baukit import TraceDict\n",
    "# Grab a datapoint\n",
    "dp = dataset[0][\"input_ids\"]\n",
    "dp = torch.tensor([dp]).to(device)\n",
    "# Define multiple interventions\n",
    "inner_activation_list = []\n",
    "def replace_with_autoencoder(values, layer_name):\n",
    "    # check if instance tuple\n",
    "    if(isinstance(values, tuple)):\n",
    "        changed_values = values[0]\n",
    "    else:\n",
    "        changed_values = values\n",
    "    # return values\n",
    "    batch, seq, dim = changed_values.shape\n",
    "    layer_index = cache_names.index(layer_name)\n",
    "    autoencoder = autoencoders[layer_index]\n",
    "    autoencoder_input = rearrange(changed_values, \"b s n -> (b s) n\")\n",
    "    codes = autoencoder.encode(autoencoder_input)\n",
    "    inner_activation_list.append(codes)\n",
    "    reconstructed = autoencoder.decode(codes)\n",
    "    changed_values =  rearrange(reconstructed, \"(b s) n -> b s n\", b=batch, s=seq)\n",
    "\n",
    "    if(isinstance(values, tuple)):\n",
    "        values = (changed_values, values[1])\n",
    "    else:\n",
    "        values = changed_values\n",
    "    return values\n",
    "\n",
    "# with TraceDict(model, cache_names) as ret:\n",
    "with TraceDict(model, cache_names, edit_output=replace_with_autoencoder) as ret:\n",
    "    logits = model(dp).logits.to(\"cpu\")\n",
    "    # internal_activations = ret[cache_names[0]].output\n",
    "    # # check if instance tuple\n",
    "    # if(isinstance(internal_activations, tuple)):\n",
    "    #     internal_activations = internal_activations[0]\n",
    "internal_output_N = inner_activation_list[1]\n",
    "internal_output_N_minus_1 = inner_activation_list[0]\n",
    "grad_N_to_N_minus_1 = torch.autograd.grad(internal_output_N, internal_output_N_minus_1, grad_outputs=torch.ones_like(internal_output_N))\n",
    "# grad_N_to_N_minus_1 = torch.autograd.grad(internal_output_N, internal_output_N_minus_1, retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpha_utils_interp import *\n",
    "import os\n",
    "# make features/ dir if not exist\n",
    "save_path = \"features/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "num_feature_datapoints = 10\n",
    "ae_index = 0\n",
    "cache_name = cache_names[ae_index]\n",
    "autoencoder = autoencoders[ae_index]\n",
    "dictionary_activations, tokens_for_each_datapoint = get_dictionary_activations(model, dataset, cache_name, max_seq_length, autoencoder, batch_size=32)\n",
    "sparsity = dictionary_activations[:80].count_nonzero(dim=1).float().mean()\n",
    "print(f\"Sparsity: {sparsity}\")\n",
    "max_values = dictionary_activations.max(dim=0)\n",
    "\n",
    "feature_ind = max_values.values.topk(20).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_setting = \"input_only\"\n",
    "model_type = \"causal\"\n",
    "features = feature_ind.tolist()\n",
    "num_features = 10\n",
    "for feature in features:\n",
    "    # Check if feature is dead (<10 activations)\n",
    "    dead_threshold = 10\n",
    "    # if(dictionary_activations[:, current_feature].count_nonzero() < dead_threshold):\n",
    "    if(dictionary_activations[:, feature].count_nonzero() < dead_threshold):s\n",
    "        print(f\"Feature {feature} is dead\")\n",
    "        continue\n",
    "    uniform_indices = get_feature_indices(feature, dictionary_activations, k=num_feature_datapoints, setting=\"uniform\")\n",
    "    text_list, full_text, token_list, full_token_list, partial_activations, full_activations = get_feature_datapoints(uniform_indices, dictionary_activations[:, feature], tokenizer, max_seq_length, dataset)\n",
    "    # get_token_statistics(feature, dictionary_activations[:, feature], dataset, tokenizer, max_seq_length, tokens_for_each_datapoint, save_location = save_path, num_unique_tokens=10)\n",
    "    if(input_setting == \"input_only\"):\n",
    "        # Calculate logit diffs on this feature for the full_token_list\n",
    "        logit_diffs = ablate_feature_direction(model, full_token_list, cache_name, max_seq_length, autoencoder, feature = feature, batch_size=32, setting=\"sentences\", model_type=model_type)\n",
    "        # save_token_display(full_token_list, full_activations, tokenizer, path =f\"{save_path}uniform_{feature}.png\", logit_diffs = logit_diffs, model_type=model_type)\n",
    "        save_token_display(full_token_list, full_activations, tokenizer, path =f\"{save_path}uniform_{feature}.png\", logit_diffs = logit_diffs, model_type=model_type, show=True)\n",
    "        all_changed_activations = ablate_context_one_token_at_a_time(model, token_list, cache_name, autoencoder, feature, max_ablation_length=30)\n",
    "        save_token_display(token_list, all_changed_activations, tokenizer, path =f\"{save_path}ablate_context_{feature}.png\", model_type=model_type, show=True)\n",
    "    else:\n",
    "        logit_diffs = ablate_feature_direction(model, dataset, cache_name, max_seq_length, autoencoder, feature = feature, batch_size=32, setting=\"dataset\")\n",
    "        _, _, _, full_token_list_ablated, _, full_activations_ablated = get_feature_datapoints(uniform_indices, logit_diffs, tokenizer, max_seq_length, dataset)\n",
    "        get_token_statistics(feature, logit_diffs, dataset, tokenizer, max_seq_length, tokens_for_each_datapoint, save_location = save_path, setting=\"output\", num_unique_tokens=10)\n",
    "        save_token_display(full_token_list_ablated, full_activations, tokenizer, path =f\"{save_path}uniform_{feature}.png\", logit_diffs = full_activations_ablated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find related features\n",
    "### Gradient based method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlational -> Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
