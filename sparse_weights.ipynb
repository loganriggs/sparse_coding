{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/logan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# Download the model\n",
    "device = \"cuda:0\"\n",
    "model_name=\"EleutherAI/Pythia-70M-deduped\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Sparse AE's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoders import *\n",
    "# ae_model_id = [\"jbrinkma/Pythia-70M-chess_sp51_r4_gpt_neox.layers.1\", \"jbrinkma/Pythia-70M-chess_sp51_r4_gpt_neox.layers.2.mlp\"]\n",
    "model_id = \"jbrinkma/Pythia-70M-deduped-SAEs\"\n",
    "filename = [\"Pythia-70M-deduped-1.pt\", \"Pythia-70M-deduped-mlp-2.pt\"]\n",
    "autoencoders = []\n",
    "for filen in filename:\n",
    "    ae_download_location = hf_hub_download(repo_id=model_id, filename=filen)\n",
    "    autoencoder = torch.load(ae_download_location)\n",
    "    autoencoder.to_device(device)\n",
    "    autoencoders.append(autoencoder)\n",
    "cache_names = [\"gpt_neox.layers.1\", \"gpt_neox.layers.2.mlp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-7888eac6661e5b23.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a67af88cba1e56b6.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-5872d54a9717e874.arrow\n"
     ]
    }
   ],
   "source": [
    "max_seq_length=30 # max length of per data point\n",
    "from datasets import load_dataset\n",
    "dataset_name = \"NeelNanda/pile-10k\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\").map(\n",
    "    lambda x: tokenizer(x['text']),\n",
    "    batched=True\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > max_seq_length\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:max_seq_length]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab Intermediate Layer Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/310 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from baukit import TraceDict\n",
    "# with TraceDict(model, cache_names) as ret:\n",
    "\n",
    "num_features, d_model = autoencoder.encoder.shape\n",
    "datapoints = dataset.num_rows\n",
    "batch_size = 32\n",
    "with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "    dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "    for i, batch in enumerate(tqdm(dl)):\n",
    "        batch = batch.to(device)\n",
    "        # Get LLM intermediate activations\n",
    "        with TraceDict(model, cache_names) as ret:\n",
    "            _ = model(batch)\n",
    "        # Get SAE intermediate codes\n",
    "        for ae_ind, cache_name in enumerate(cache_names):\n",
    "            autoencoder = autoencoders[ae_ind]\n",
    "            internal_activations = ret[cache_name].output\n",
    "            # check if instance tuple ie a layer output\n",
    "            if(isinstance(internal_activations, tuple)):\n",
    "                internal_activations = internal_activations[0]\n",
    "\n",
    "            batched_neuron_activations = rearrange(internal_activations, \"b s n -> (b s) n\" )\n",
    "            batched_dictionary_activations = autoencoder.encode(batched_neuron_activations)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get MLP\n",
    "model.gpt_neox.layers[2].mlp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
