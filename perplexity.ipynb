{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_replacement(value, hook, autoencoder):\n",
    "    # Rearrange to fit autoencoder\n",
    "    int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "    # Run through the autoencoder\n",
    "    reconstruction = autoencoder.predict(int_val)\n",
    "    batch, seq_len, hidden_size = value.shape\n",
    "    reconstruction = rearrange(reconstruction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "    return reconstruction\n",
    "\n",
    "def reconstruction_acquirement(value, hook, autoencoder, sum_mse):\n",
    "    # Rearrange to fit autoencoder\n",
    "    int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "    # Run through the autoencoder\n",
    "    reconstruction = autoencoder.predict(int_val)\n",
    "    batch, seq_len, hidden_size = value.shape\n",
    "    reconstruction = rearrange(reconstruction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "    sum_mse.add_((reconstruction - value).pow(2).sum())\n",
    "\n",
    "def reconstruction_no_outliers(value, hook, autoencoder, outliers):\n",
    "    # Rearrange to fit autoencoder\n",
    "    int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "    # Run through the autoencoder\n",
    "    reconstruction = autoencoder.predict(int_val)\n",
    "    batch, seq_len, hidden_size = value.shape\n",
    "    reconstruction = rearrange(reconstruction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "    # Replace outliers with original values\n",
    "    reconstruction[:, :, outliers] = value[:, :, outliers]\n",
    "    return reconstruction\n",
    "\n",
    "def find_outliers(dataset, cache_name, model, num_outlier_dims=2):\n",
    "    device = model.cfg.device\n",
    "    batch_size = 32\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        # Just take the first batch\n",
    "        first_batch = next(iter(dl))\n",
    "        _, cache = model.run_with_cache(first_batch.to(device))\n",
    "        layer_activations = rearrange(cache[cache_name], \"b s h -> (b s) h\").cpu()\n",
    "        outlier_dims = layer_activations.abs().max(0).values.topk(num_outlier_dims).indices\n",
    "    return outlier_dims\n",
    "\n",
    "def get_original_perplexity(model, dataset):\n",
    "    total_loss = 0\n",
    "    batch_size = 32\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(dl):\n",
    "            loss = model(batch.to(device), return_type=\"loss\")\n",
    "            total_loss += loss.item()\n",
    "        # Average\n",
    "        avg_neg_log_likelihood = total_loss / len(dl)\n",
    "\n",
    "        # Convert to tensor\n",
    "        avg_neg_log_likelihood = torch.tensor(avg_neg_log_likelihood, device=device)\n",
    "\n",
    "        # Exponentiate to compute perplexity\n",
    "        perplexity = torch.exp(avg_neg_log_likelihood)\n",
    "        print(f\"Perplexity for original model: {perplexity.item():.2f}\")\n",
    "    return perplexity.item()\n",
    "\n",
    "def get_perplexity(autoencoder, model, dataset, forward_hook):\n",
    "    device = model.cfg.device\n",
    "    model = model.eval()\n",
    "    batch_size = 32\n",
    "    # autoencoder.to_device(device)\n",
    "    total_loss = 0\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(dl):\n",
    "            loss = model.run_with_hooks(\n",
    "                batch.to(device), \n",
    "                return_type=\"loss\",\n",
    "                fwd_hooks=[forward_hook]\n",
    "                )\n",
    "            total_loss += loss.item()\n",
    "    # Average\n",
    "    avg_neg_log_likelihood = total_loss / len(dl)\n",
    "\n",
    "    # Convert to tensor\n",
    "    avg_neg_log_likelihood = torch.tensor(avg_neg_log_likelihood, device=device)\n",
    "\n",
    "    # Exponentiate to compute perplexity\n",
    "    perplexity = torch.exp(avg_neg_log_likelihood)\n",
    "    print(f\"Perplexity {perplexity.item():.2f}\")\n",
    "    return perplexity.item()\n",
    "\n",
    "def get_MSE(autoencoder, model, dataset, cache_name, device):\n",
    "    sum_mse = torch.empty((1,), device=device)\n",
    "    forward_hook = (cache_name, partial(reconstruction_acquirement, autoencoder=autoencoder, sum_mse=sum_mse))\n",
    "    device = model.cfg.device\n",
    "    model = model.eval()\n",
    "    batch_size = 32\n",
    "    # autoencoder.to_device(device)\n",
    "    # total_loss = 0\n",
    "    total_rows = 0\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(dl):\n",
    "            _ = model.run_with_hooks(\n",
    "                batch.to(device), \n",
    "                return_type=None,\n",
    "                fwd_hooks=[forward_hook]\n",
    "                )\n",
    "            # total_loss += loss.item()\n",
    "            total_rows += batch.shape[0]\n",
    "    # Average\n",
    "    mse = sum_mse / total_rows\n",
    "    return mse.item()\n",
    "\n",
    "def get_MSE_baukit(autoencoder, model, dataset, cache_name, device, max_length = 1_000_000):\n",
    "    sum_mse = 0\n",
    "    # forward_hook = (cache_name, partial(reconstruction_acquirement, autoencoder=autoencoder, sum_mse=sum_mse))\n",
    "    # device = model.cfg.device\n",
    "    model = model.eval()\n",
    "    batch_size = 32\n",
    "    # autoencoder.to_device(device)\n",
    "    # total_loss = 0\n",
    "    total_rows = 0\n",
    "    with torch.no_grad():\n",
    "        dl = dataset #DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        # for i, batch in enumerate(dl):\n",
    "        #     _ = model.run_with_hooks(\n",
    "        #         batch.to(device), \n",
    "        #         return_type=None,\n",
    "        #         fwd_hooks=[forward_hook]\n",
    "        #         )\n",
    "        #     # total_loss += loss.item()\n",
    "        #     total_rows += batch.shape[0]\n",
    "        mses = []\n",
    "        for i, b in enumerate(dl):\n",
    "            batch = b[\"input_ids\"].to(device)\n",
    "            # batch = batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                with Trace(model, cache_name) as ret:\n",
    "                    _ = model(batch)\n",
    "                    representation = ret.output\n",
    "                    if(isinstance(representation, tuple)):\n",
    "                        representation = representation[0]\n",
    "                    \n",
    "                    layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "\n",
    "                reconstructed = autoencoder.decode(autoencoder.encode(layer_activations))\n",
    "                one_mse = (layer_activations - reconstructed).pow(2).mean().detach().item()\n",
    "                # print(\"layer vs reconst shapes: \", layer_activations.shape, reconstructed.shape)\n",
    "                # print(\"layer_activations norm: \", layer_activations[0].norm())\n",
    "                # print(\"reconstructed norm: \", reconstructed[0].norm())\n",
    "                mses.append(one_mse)\n",
    "                # print(f\"one_mse is {one_mse.item()}\")\n",
    "                sum_mse += one_mse\n",
    "                total_rows += 1\n",
    "            \n",
    "            num_tokens = layer_activations.shape[0] * i\n",
    "            if num_tokens > max_length:\n",
    "                break\n",
    "    # Average\n",
    "    mse = sum_mse / total_rows\n",
    "    return mses #float(mse)\n",
    "\n",
    "def get_l2ratio(autoencoder, model, dataset, cache_name, device, filter=None):\n",
    "    res_stream = []\n",
    "    reconst = []\n",
    "    # exp_var = ExplainedVariance()\n",
    "    \n",
    "    nfeatures = autoencoder.encoder.shape[0]\n",
    "    \n",
    "    if filter is None:\n",
    "        filter = torch.ones((nfeatures,), device=device)\n",
    "    \n",
    "    if max(filter) > 1:\n",
    "        # filter is currently [id_1, id_2, id_3, ...]\n",
    "        # must change filter to binary form [0,1,0,0,0,1,0,1,1...]\n",
    "        new_filter = torch.zeros((nfeatures,), device=device)\n",
    "        for f in filter:\n",
    "            new_filter[f] = 1\n",
    "        filter = new_filter\n",
    "    \n",
    "    def reconstruction_acquirement(value, hook):\n",
    "        # Rearrange to fit autoencoder\n",
    "        # print(value.shape)\n",
    "        int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "        # Run through the autoencoder\n",
    "        f = autoencoder.encode(int_val) * filter\n",
    "        reconstruction = autoencoder.decode(f)\n",
    "        batch, seq_len, hidden_size = value.shape\n",
    "        reconstruction = rearrange(reconstruction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "        # exp_var.update(reconstruction, value)\n",
    "        res_stream.append(value)\n",
    "        reconst.append(reconstruction)\n",
    "        return reconstruction\n",
    "\n",
    "    forward_hook = (cache_name, (reconstruction_acquirement))\n",
    "    device = model.cfg.device\n",
    "    model = model.eval()\n",
    "    batch_size = 32\n",
    "    # autoencoder.to_device(device)\n",
    "    total_loss = 0\n",
    "    total_rows = 0\n",
    "    \n",
    "    sum_l2_ratio = 0\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(dl):\n",
    "            model.run_with_hooks(\n",
    "                batch.to(device), \n",
    "                return_type=None,\n",
    "                fwd_hooks=[forward_hook]\n",
    "                )\n",
    "\n",
    "            sum_l2_ratio += (reconst[-1].norm(p=2, dim=-1)/res_stream[-1].norm(p=2, dim=-1)).mean()\n",
    "            res_stream = []\n",
    "            reconst = []\n",
    "            \n",
    "    l2_ratio = sum_l2_ratio/len(dl)\n",
    "    return l2_ratio\n",
    "\n",
    "def get_perplexity_and_mse(autoencoder, model, dataset, cache_name, device, filter=None):\n",
    "    sum_mse = torch.zeros((1,), device=device)\n",
    "    sparsity = torch.zeros((1,), device=device)\n",
    "    res_stream = []\n",
    "    reconst = []\n",
    "    # exp_var = ExplainedVariance()\n",
    "    \n",
    "    nfeatures = autoencoder.encoder.shape[0]\n",
    "    print(\"nfeatures: \", nfeatures)\n",
    "    \n",
    "    if filter is None:\n",
    "        filter = torch.ones((nfeatures,), device=device)\n",
    "    \n",
    "    if max(filter) > 1:\n",
    "        # filter is currently [id_1, id_2, id_3, ...]\n",
    "        # must change filter to binary form [0,1,0,0,0,1,0,1,1...]\n",
    "        new_filter = torch.zeros((nfeatures,), device=device)\n",
    "        for f in filter:\n",
    "            new_filter[f] = 1\n",
    "        filter = new_filter\n",
    "    \n",
    "    def reconstruction_replacement_acquirement(value, hook):\n",
    "        # Rearrange to fit autoencoder\n",
    "        # print(value.shape)\n",
    "        int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "        # Run through the autoencoder\n",
    "        f = autoencoder.encode(int_val) * filter\n",
    "        reconstruction = autoencoder.decode(f)\n",
    "        batch, seq_len, hidden_size = value.shape\n",
    "        reconstruction = rearrange(reconstruction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "        sum_mse.add_((reconstruction - value).pow(2).sum())\n",
    "        sparsity.add_((f > 0).sum())\n",
    "        \n",
    "        # exp_var.update(reconstruction, value)\n",
    "        res_stream.append(value)\n",
    "        reconst.append(reconstruction)\n",
    "        \n",
    "        return reconstruction\n",
    "\n",
    "    forward_hook = (cache_name, (reconstruction_replacement_acquirement))\n",
    "    device = model.cfg.device\n",
    "    model = model.eval()\n",
    "    batch_size = 32\n",
    "    # autoencoder.to_device(device)\n",
    "    total_loss = 0\n",
    "    total_rows = 0\n",
    "    sum_l2_ratio = 0\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(dl):\n",
    "            loss = model.run_with_hooks(\n",
    "                batch.to(device), \n",
    "                return_type=\"loss\",\n",
    "                fwd_hooks=[forward_hook]\n",
    "                )\n",
    "            total_loss += loss.item() * batch.shape[0]\n",
    "            total_rows += batch.shape[0]\n",
    "            sum_l2_ratio += (reconst[-1].norm(p=2, dim=-1)/res_stream[-1].norm(p=2, dim=-1)).mean()\n",
    "            res_stream = []\n",
    "            reconst = []\n",
    "    # Average\n",
    "    mse = sum_mse / total_rows / 256 / 512\n",
    "    sparsity = sparsity / total_rows / 256\n",
    "    \n",
    "    l2_ratio = sum_l2_ratio/len(dl)\n",
    "    \n",
    "    avg_neg_log_likelihood = total_loss / total_rows\n",
    "\n",
    "    # Convert to tensor\n",
    "    avg_neg_log_likelihood = torch.tensor(avg_neg_log_likelihood, device=device)\n",
    "\n",
    "    # Exponentiate to compute perplexity\n",
    "    perplexity = torch.exp(avg_neg_log_likelihood)\n",
    "    print(f\"MSE {mse.item():.4f}\")\n",
    "    print(f\"Perplexity {perplexity.item():.2f}\")\n",
    "    return mse.item(), sparsity.item(), perplexity.item(), l2_ratio.item() #, exp_var.compute()\n",
    "\n",
    "def download_dataset(dataset_name, max_length=256, num_datapoints=None):\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    if(num_datapoints):\n",
    "        split_text = f\"train[:{num_datapoints}]\"\n",
    "    else:\n",
    "        split_text = \"train\"\n",
    "    dataset = load_dataset(dataset_name, split=split_text).map(\n",
    "        lambda x: model.tokenizer(x['text']),\n",
    "        batched=True,\n",
    "    ).filter(\n",
    "        lambda x: len(x['input_ids']) > max_length\n",
    "    ).map(\n",
    "        lambda x: {'input_ids': x['input_ids'][:max_length]}\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = autoencoders[4][0][0]\n",
    "autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"1\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "# Your activation name will be different. In the next cells, we will show you how to find it.\n",
    "with torch.no_grad():\n",
    "    with Trace(bau_model, bau_cache_names[4]) as ret:\n",
    "        _ = bau_model(tokens)\n",
    "        representation = ret.output\n",
    "        # check if instance tuple\n",
    "        if(isinstance(representation, tuple)):\n",
    "            representation = representation[0]\n",
    "        activation_size = representation.shape[-1]\n",
    "print(f\"Activation size: {activation_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"1\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "outp = bau_model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change these settings to load the correct autoencoder\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "\n",
    "from baukit import Trace, TraceDict\n",
    "import matplotlib.style as style\n",
    "\n",
    "\n",
    "# layer = 2\n",
    "layers = [5]\n",
    "l1_alphas = [2e-3]\n",
    "setting = \"residual\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "cache_names = []\n",
    "\n",
    "for layer in layers:\n",
    "    if setting == \"residual\":\n",
    "        cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    elif setting == \"mlp\":\n",
    "        cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    cache_names.append(cache_name)\n",
    "    \n",
    "bau_cache_names = []\n",
    "for layer in layers:\n",
    "    if setting == \"residual\":\n",
    "        bau_cache_name = f\"gpt_neox.layers.{layer}\"\n",
    "    elif setting == \"mlp\":\n",
    "        bau_cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    bau_cache_names.append(bau_cache_name)\n",
    "        \n",
    "# gpt2_autoencoders = torch.load(\"/mnt/ssd-cluster/gpt2small/tied_residual_l2_r8/_31/learned_dicts.pt\")\n",
    "# pythia_autoencoders = torch.load(\"/mnt/ssd-cluster/longrun2408/tied_residual_l2_r6/_31/learned_dicts.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modes = [\"sae\", \"baseline\", \"scale\", \"dnorotation\", \"dfree\", \"enorotation\", \"efree\", \"norotation\"]\n",
    "modes = [\"sae\",] #, \"baseline\", \"dfree\"]\n",
    "# modes = [\"dfree\", \"sae\", \"baseline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load autoencoders\n",
    "\n",
    "# modes = [\"sae\", \"dfree\"]\n",
    "autoencoders = []\n",
    "for layer in layers:\n",
    "    l1_variants = []\n",
    "    \n",
    "    for l1 in ((l1_alphas)):  \n",
    "        mode_tsaes = []\n",
    "        for mode in modes:\n",
    "            if mode == \"sae\":\n",
    "                mode_tsae = torch.load(f\"trained_models/checkpoint5/base_sae_70m_32_{layer}_{l1}.pt\")\n",
    "                mode_tsaes.append(mode_tsae)\n",
    "            # elif mode == \"baseline\":\n",
    "            #     mode_tsae = torch.load(f\"trained_models/base_retrain_70m/base_sae_70m_{layer}_{l1}.pt\")\n",
    "            #     mode_tsaes.append(mode_tsae)\n",
    "            else:\n",
    "                mode_tsae = torch.load(f\"trained_models/base_autoTED_70m/base_autoTED_70m_32_{mode}_{layer}_{l1}.pt\")\n",
    "                mode_tsaes.append(mode_tsae)\n",
    "        \n",
    "        l1_variants.append(mode_tsaes)\n",
    "    autoencoders.append(l1_variants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weak_features = torch.load(\"weak_features.p\").to(device)\n",
    "# transition_features = torch.load(\"transition_features.p\").to(device)\n",
    "# vip_features = torch.load(\"vip_features.p\").to(device)\n",
    "# non_weak_features = torch.logical_or(transition_features, vip_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import argparse\n",
    "from utils import dotdict\n",
    "from activation_dataset import setup_token_data\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cfg = dotdict()\n",
    "# models: \"EleutherAI/pythia-6.9b\", \"lomahony/eleuther-pythia6.9b-hh-sft\", \"usvsnsp/pythia-6.9b-ppo\", \"Dahoas/gptj-rm-static\", \"reciprocate/dahoas-gptj-rm-static\"\n",
    "# cfg.model_name=\"lomahony/eleuther-pythia6.9b-hh-sft\"\n",
    "# \"EleutherAI/pythia-70m\", \"lomahony/pythia-70m-helpful-sft\", \"lomahony/eleuther-pythia70m-hh-sft\"\n",
    "cfg.model_name=\"EleutherAI/pythia-70m\"\n",
    "cfg.layers=[5] #[0,1,2,3,4,5]\n",
    "cfg.setting=\"residual\"\n",
    "# cfg.tensor_name=\"gpt_neox.layers.{layer}\" or \"transformer.h.{layer}\"\n",
    "cfg.tensor_name=\"gpt_neox.layers.{layer}\"\n",
    "original_l1_alpha = 8e-4\n",
    "# cfg.l1_alpha=original_l1_alpha\n",
    "# cfg.l1_alphas=[0, 1e-5, 2e-5, 4e-5, 8e-5, 1e-4, 2e-4, 4e-4, 8e-4, 1e-3, 2e-3, 4e-3, 8e-3]\n",
    "cfg.l1_alphas=[2e-3]\n",
    "cfg.sparsity=None\n",
    "cfg.num_epochs=10\n",
    "cfg.model_batch_size= 8 * 8\n",
    "cfg.sub_batch_size=8 * 8\n",
    "cfg.lr=1e-3\n",
    "cfg.kl=False\n",
    "cfg.reconstruction=False\n",
    "# cfg.dataset_name=\"NeelNanda/pile-10k\"\n",
    "cfg.dataset_name=\"Elriggs/openwebtext-100k\"\n",
    "cfg.device=\"cuda:0\"\n",
    "cfg.ratio = 32\n",
    "cfg.seed = 0\n",
    "cfg.max_length = 256\n",
    "# cfg.device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get dataset\n",
    "# model_name = \"EleutherAI/pythia-70m\"\n",
    "# model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "# from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, GPTJForSequenceClassification\n",
    "# bau_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# bau_model = bau_model.to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "\n",
    "# # dataset = download_dataset(\"Elriggs/openwebtext-100k\", max_length=256, num_datapoints=100) # num tokens = max_length * num_datapoints\n",
    "# # print(len(dataset))\n",
    "\n",
    "# cfg.max_length = 256\n",
    "# dataset = setup_token_data(cfg, tokenizer, bau_model, seed=cfg.seed)\n",
    "# max_length = 100_000\n",
    "\n",
    "# # Get top-N outlier dims\n",
    "# # outlier_dims = find_outliers(dataset, cache_name, model, num_outlier_dims=2)\n",
    "# # Get original model perplexity \n",
    "\n",
    "# # original_perplexity = get_original_perplexity(model, dataset)\n",
    "# model = _\n",
    "\n",
    "# # Get perplexity for all autoencoders\n",
    "\n",
    "# # num_autoencoders = len(autoencoders)\n",
    "# filters = [None,]\n",
    "# # all_perplexities2 = np.zeros((len(layers),len(l1_alphas),len(modes), len(filters)))\n",
    "# all_mses2 = np.zeros((len(layers),len(l1_alphas),len(modes), len(filters)))\n",
    "# # all_sparsities2 = np.zeros((len(layers),len(l1_alphas),len(modes), len(filters)))\n",
    "# # all_l2_ratios = np.zeros((len(layers),len(l1_alphas),len(modes), len(filters)))\n",
    "\n",
    "# done_so_far = 0\n",
    "# for layer in range(len(layers)):    \n",
    "#     cache_name = bau_cache_names[layer]\n",
    "#     for l1 in range(len(l1_alphas)):\n",
    "#         for m_id in range(len(modes)):\n",
    "#             for filter_id in range(len(filters)):\n",
    "#                 filter = filters[filter_id]\n",
    "            \n",
    "#                 autoencoder = autoencoders[layer][l1][m_id]#.to(device)\n",
    "#                 # m, s, p, l2_r = get_perplexity_and_mse(autoencoder, model, dataset, cache_name, device, filter=filter)\n",
    "#                 dataset = setup_token_data(cfg, tokenizer, bau_model, seed=cfg.seed)\n",
    "#                 m = get_MSE_baukit(autoencoder, bau_model, dataset, bau_cache_name, device, max_length=max_length)\n",
    "#                 # m = get_MSE(autoencoder, model, dataset, cache_name, device)\n",
    "#                 print(\"max m: \", max(m))\n",
    "#                 print(\"mean m: \", sum(m)/len(m))\n",
    "#                 print(\"min m: \", min(m))\n",
    "                \n",
    "#                 all_mses2[layer][l1][m_id][filter_id] = sum(m)/len(m)\n",
    "#                 done_so_far += 1\n",
    "#                 print(\"Done so far: \", done_so_far)\n",
    "#                 # all_perplexities2[layer][l1][m_id][filter_id] = p\n",
    "#                 # all_sparsities2[layer][l1][m_id][filter_id] = s\n",
    "#                 # all_l2_ratios[layer][l1][m_id][filter_id] = l2_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_since_activation = torch.zeros((len(cfg.layers), len(cfg.l1_alphas), autoencoders[0][0][0].encoder.shape[0]))\n",
    "# total_activations = torch.zeros(autoencoder.encoder.shape[0])\n",
    "max_num_tokens = 1_000_000\n",
    "save_every = 30_000\n",
    "num_saved_so_far = 0\n",
    "# Freeze model parameters \n",
    "\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, GPTJForSequenceClassification\n",
    "\n",
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model = model.to(device)\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "model.to(cfg.device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "\n",
    "tensor_names = [cfg.tensor_name.format(layer=layer) for layer in cfg.layers]\n",
    "token_loader = setup_token_data(cfg, tokenizer, model, seed=cfg.seed)\n",
    "\n",
    "# last_encoder = autoencoder.encoder.clone().detach()\n",
    "i=0\n",
    "filters = [None]\n",
    "all_mses2 = np.zeros((len(layers),len(l1_alphas),len(modes), len(filters)))\n",
    "all_perplexities2 = np.zeros((len(layers),len(l1_alphas),len(modes), len(filters)))\n",
    "all_sparsities2 = np.zeros((len(layers),len(l1_alphas),len(modes), len(filters)))\n",
    "all_l2_ratios = np.zeros((len(layers),len(l1_alphas),len(modes), len(filters)))\n",
    "\n",
    "for i_inside, batch in enumerate(tqdm(token_loader,total=min(int(max_num_tokens/(cfg.max_length*cfg.model_batch_size)), len(token_loader)))):\n",
    "    i+=1\n",
    "    tokens = batch[\"input_ids\"].to(cfg.device)\n",
    "        \n",
    "    for layer in range(len(cfg.layers)):\n",
    "        for l1 in range(len(cfg.l1_alphas)):\n",
    "            for m_id in range(len(modes)):\n",
    "                mode = modes[m_id]\n",
    "                \n",
    "                l1_alpha = cfg.l1_alphas[l1]\n",
    "                                                    \n",
    "                with torch.no_grad():\n",
    "                \n",
    "                    autoencoder = autoencoders[layer][l1][m_id]\n",
    "                    \n",
    "                    def edit_out(activations):\n",
    "                        # print(\"edit act inp len: \", len(activations))\n",
    "                        # print(\"edit act vals: \", activations)\n",
    "                        # print(\"act[0] shape: \", activations[0].shape)\n",
    "                        # print(\"act[1] len: \", len(activations[1]))\n",
    "                        acts = rearrange(activations[0], \"b seq d_model -> (b seq) d_model\")\n",
    "                        reconstructed = autoencoder.decode(autoencoder.encode(acts))\n",
    "                        reconstructed = reconstructed.reshape(activations[0].shape)\n",
    "                        # print(\"reconstructed shape: \", reconstructed.shape)\n",
    "                        return reconstructed, activations[1]\n",
    "\n",
    "                    # with Trace(model, tensor_names[layer], edit_output=edit_out) as ret:\n",
    "                    with Trace(model, tensor_names[layer]) as ret:\n",
    "                        outp = model(tokens)\n",
    "                        representation = ret.output[0]\n",
    "                        acts = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "                    \n",
    "                    c = autoencoder.encode(acts)\n",
    "                    x_hat = autoencoder.decode(c)\n",
    "                    \n",
    "                    reconstruction_loss = (x_hat - acts).pow(2).mean()\n",
    "                    print(\"recon\", reconstruction_loss)\n",
    "                    \n",
    "                    all_mses2[layer,l1,m_id,0] += reconstruction_loss.detach().clone().item()\n",
    "                    \n",
    "                    l1_loss = torch.norm(c, 1, dim=-1).mean()\n",
    "                    total_loss = reconstruction_loss + l1_alpha*l1_loss\n",
    "\n",
    "                    # time_since_activation[layer, l1, :] += 1\n",
    "                    # time_since_activation[layer, l1, :] = time_since_activation[layer, l1, :] * (c.sum(dim=0).cpu()==0).detach()\n",
    "                    \n",
    "                    sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "                    \n",
    "                    rearr_out = rearrange(outp[0], \"b seq d_model -> (b seq) d_model\")\n",
    "                    rearr_tokens = rearrange(tokens, \"b seq -> (b seq)\")\n",
    "                    ce = torch.nn.functional.cross_entropy(rearr_out, rearr_tokens)\n",
    "                    all_perplexities2[layer,l1,m_id,0] += ce\n",
    "                    all_sparsities2[layer][l1][m_id][0] += sparsity\n",
    "                    all_l2_ratios[layer][l1][m_id][0] += (x_hat.norm(dim=-1)/acts.norm(dim=-1)).mean()\n",
    "                \n",
    "\n",
    "    inside_tokens = i_inside*cfg.max_length*cfg.model_batch_size\n",
    "    if inside_tokens > max_num_tokens:\n",
    "        break\n",
    "    \n",
    "all_mses2 = all_mses2/i\n",
    "all_perplexities2[layer,l1,m_id,0] = np.exp(all_perplexities2[layer,l1,m_id,0]/i)\n",
    "all_sparsities2 = all_sparsities2/i\n",
    "all_l2_ratios = all_l2_ratios/i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_perplexities2[layer,l1,m_id,0] = np.exp(all_perplexities2[layer,l1,m_id,0]/i)\n",
    "# all_sparsities2 = all_sparsities2/i\n",
    "# all_l2_ratios = all_l2_ratios/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(all_mses2, \"all_mses.pt\")\n",
    "# torch.save(all_perplexities2, \"all_perplexities.pt\")\n",
    "# torch.save(all_sparsities2, \"all_sparsities.pt\")\n",
    "# torch.save(all_l2_ratios, \"all_l2_ratios.pt\")\n",
    "# torch.save(original_perplexity, \"original_perplexity.pt\")\n",
    "\n",
    "# torch.save(all_mses2, \"all_mses_32.pt\")\n",
    "# torch.save(all_perplexities2, \"all_perplexities_32.pt\")\n",
    "# torch.save(all_sparsities2, \"all_sparsities_32.pt\")\n",
    "# torch.save(all_l2_ratios, \"all_l2_ratios_32.pt\")\n",
    "# torch.save(original_perplexity, \"original_perplexity_32.pt\")\n",
    "\n",
    "# all_mses2 = torch.load(\"all_mses.pt\")\n",
    "# all_perplexities2 = torch.load(\"all_perplexities.pt\")\n",
    "# all_sparsities2 = torch.load(\"all_sparsities.pt\")\n",
    "# all_l2_ratios = torch.load(\"all_l2_ratios.pt\")\n",
    "original_perplexity = 39.63\n",
    "\n",
    "# original_perplexity = torch.load(\"original_perplexity.pt\")\n",
    "\n",
    "\n",
    "# all_mses2 = torch.load(\"all_mses_32.pt\")\n",
    "# all_perplexities2 = torch.load(\"all_perplexities_32.pt\")\n",
    "# all_sparsities2 = torch.load(\"all_sparsities_32.pt\")\n",
    "# all_l2_ratios = torch.load(\"all_l2_ratios_32.pt\")\n",
    "# original_perplexity = torch.load(\"original_perplexity_32.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mses2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mses2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmodes = len(modes)-1\n",
    "nice_name_dict = {\n",
    "    \"sae\": \"Original\",\n",
    "    \"baseline\": \"Baseline\",\n",
    "    \"dnorotation\": \"Unrotated Decoder\", \n",
    "    \"dfree\": \"Decoder\", \n",
    "    \"scale\": \"Scale\", \n",
    "    \"enorotation\": \"Unrotated Encoder\", \n",
    "    \"efree\": \"Encoder\", \n",
    "    \"norotation\": \"Unrotated All\"\n",
    "}\n",
    "nice_modes = [nice_name_dict[m] for m in modes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Style and Colors\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for st in style.available:\n",
    "#     style.use(st)\n",
    "#     colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] \n",
    "#     # print('\\n'.join(color for color in colors))  \n",
    "#     print(f\"{len(colors)}: {st}\")\n",
    "    \n",
    "style.use(\"seaborn-v0_8\")\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] \n",
    "\n",
    "# colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] \n",
    "# mode_colors = colors + []\n",
    "\n",
    "# style.use(\"default\")\n",
    "# sns.set_style(style=None, rc = {})\n",
    "palette = sns.color_palette(palette=\"Set3\", n_colors=8, desat=None, as_cmap=False)\n",
    "display(palette)\n",
    "\n",
    "\n",
    "palette2 = sns.color_palette(palette=None, n_colors=6, desat=None, as_cmap=False) \n",
    "\n",
    "palette3 = sns.color_palette(\"RdPu\", 10)\n",
    "\n",
    "palette = [palette[5]] + palette2 + [palette3[4]]\n",
    "# sns.set_palette(palette)\n",
    "display(palette2)\n",
    "display(palette3)\n",
    "style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shade(rgb, factor):\n",
    "    r,g,b = rgb\n",
    "    return (factor*r, factor*g, factor*b)\n",
    "\n",
    "def multi_shade(rgb, n=4):\n",
    "    factors = 0.9 - 0.8 * np.arange(n)/n\n",
    "    norm = max(rgb)\n",
    "    factors = factors/norm\n",
    "    return [shade(rgb, f) for f in factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot all_model_perplexities - all_model_perplexities2\n",
    "\n",
    "# plt.figure(dpi=200) \n",
    "\n",
    "filter_ids = [1,2,3,4]\n",
    "figsize= (14,10.5)\n",
    "\n",
    "plt.rcParams['font.size'] = 16\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(filter_ids), figsize=figsize, dpi=200)\n",
    "\n",
    "ylim = (-0.02, 0.36)\n",
    "\n",
    "for i, mode_id in enumerate(filter_ids):\n",
    "    ax = axes[i]\n",
    "    filtered_perps = all_perplexities2[:,0,mode_id]\n",
    "    sae_perp = all_perplexities2[:,0,0]\n",
    "    recovered_perps = (sae_perp - filtered_perps)/(sae_perp - original_perplexity)\n",
    "\n",
    "    scale_color = 1.3 - layer/len(layers)\n",
    "    xs = np.arange(len(layers))/(2*len(layers)) - (len(layers)-1)/(4*len(layers))\n",
    "    ax.bar(xs,recovered_perps[:,0], width=0.5/len(layers), color=multi_shade(palette[mode_id], len(layers)))\n",
    "    ax.set_title(f\"{nice_modes[mode_id]}\")\n",
    "    ax.set_xticks(xs, [f\"{l+1}\" for l in layers], rotation=0)\n",
    "    # ax.set_ylim(ylim)\n",
    "    \n",
    "axes[0].set_ylabel(\"Fraction of Perplexity Recovered\", fontsize=24)\n",
    "# plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# for layer in layers:\n",
    "#     filtered_perps = all_perplexities2[layer,0,filter_ids]\n",
    "#     sae_perp = all_perplexities2[layer][0][0]\n",
    "#     recovered_perps = (sae_perp - filtered_perps)/(sae_perp - original_perplexity)\n",
    "#     # print(recovered_perps.shape)\n",
    "\n",
    "#     xs = np.arange(len(filter_ids)) + layer/(2*len(layers)) - (len(layers)-1)/(4*len(layers))\n",
    "#     scale_color = 1.3 - layer/len(layers)\n",
    "#     plt.bar(xs,recovered_perps[:,0], width=1/2/len(layers), color=[shade(palette[f], scale_color) for f in filter_ids])\n",
    "    \n",
    "    \n",
    "# xs = np.arange(len(filter_ids))\n",
    "\n",
    "# plt.legend([f\"Layer {l+1}\" for l in layers])\n",
    "# plt.title(\"Fraction of Perplexity Recovered\")\n",
    "# # plt.xlabel(\"Autoencoder Mode\")\n",
    "# plt.ylabel(\"Perplexity Recovery\")\n",
    "# plt.xticks(xs, [nice_modes[f] for f in filter_ids], rotation=20)\n",
    "# set ylimit to 10\n",
    "# plt.ylim(0, 1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=1200) \n",
    "\n",
    "filter_ids = [1,2,3,4,5,6,7]\n",
    "plt.rcParams['font.size'] = 12\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(filter_ids), figsize=(14,10.5), dpi=200)\n",
    "# axes[0]\n",
    "\n",
    "ylim = (-0.02, 1.01)\n",
    "\n",
    "for i, mode_id in enumerate(filter_ids):\n",
    "    ax = axes[i]\n",
    "    filtered_perps = all_perplexities2[:,0,mode_id]\n",
    "    sae_perp = all_perplexities2[:,0,0]\n",
    "    recovered_perps = (sae_perp - filtered_perps)/(sae_perp - original_perplexity)\n",
    "\n",
    "    scale_color = 1.3 - layer/len(layers)\n",
    "    xs = np.arange(len(layers))/(2*len(layers)) - (len(layers)-1)/(4*len(layers))\n",
    "    ax.bar(xs,recovered_perps[:,0], width=0.5/len(layers), color=multi_shade(palette[mode_id], len(layers)))\n",
    "    ax.set_title(f\"{nice_modes[mode_id]}\")\n",
    "    ax.set_xticks(xs, [f\"{l+1}\" for l in layers], rotation=0)\n",
    "    # ax.set_ylim(ylim)\n",
    "    \n",
    "axes[0].set_ylabel(\"Fraction of Perplexity Recovered\", fontsize=24)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot all_model_perplexities - all_model_perplexities2\n",
    "\n",
    "plt.figure(dpi=1200) \n",
    "\n",
    "filter_ids = [1,2,3,4,5,6,7]\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(filter_ids), figsize=(14,10.5), dpi=200)\n",
    "\n",
    "ylim = (-0.03, 1.01)\n",
    "\n",
    "for i, mode_id in enumerate(filter_ids):\n",
    "    ax = axes[i]\n",
    "    filtered_perps = all_mses2[:,0,mode_id]\n",
    "    sae_perp = all_mses2[:,0,0]\n",
    "    recovered_perps = (sae_perp - filtered_perps)/(sae_perp)\n",
    "\n",
    "    scale_color = 1.3 - layer/len(layers)\n",
    "    xs = np.arange(len(layers))/(2*len(layers)) - (len(layers)-1)/(4*len(layers))\n",
    "    ax.bar(xs,recovered_perps[:,0], width=0.5/len(layers), color=multi_shade(palette[mode_id], len(layers)))\n",
    "    ax.set_title(f\"{nice_modes[mode_id]}\")\n",
    "    ax.set_xticks(xs, [f\"{l+1}\" for l in layers], rotation=0)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "axes[0].set_ylabel(\"Fraction of MSE Reduced\", fontsize=24)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot all_model_perplexities - all_model_perplexities2\n",
    "\n",
    "plt.figure(dpi=1200) \n",
    "\n",
    "filter_ids = [1,2,3,4]\n",
    "plt.rcParams['font.size'] = 16\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(filter_ids), figsize=(14,10.5), dpi=1200)\n",
    "\n",
    "ylim = (-0.01, 0.15)\n",
    "\n",
    "for i, mode_id in enumerate(filter_ids):\n",
    "    ax = axes[i]\n",
    "    filtered_perps = all_mses2[:,0,mode_id]\n",
    "    sae_perp = all_mses2[:,0,0]\n",
    "    recovered_perps = (sae_perp - filtered_perps)/(sae_perp)\n",
    "\n",
    "    scale_color = 1.3 - layer/len(layers)\n",
    "    xs = np.arange(len(layers))/(2*len(layers)) - (len(layers)-1)/(4*len(layers))\n",
    "    ax.bar(xs,recovered_perps[:,0], width=0.5/len(layers), color=multi_shade(palette[mode_id], len(layers)))\n",
    "    ax.set_title(f\"{nice_modes[mode_id]}\")\n",
    "    ax.set_xticks(xs, [f\"{l+1}\" for l in layers], rotation=0)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "axes[0].set_ylabel(\"Fraction of MSE Reduced\", fontsize=24)\n",
    "# plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sparsities2[:,0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot all_model_perplexities - all_model_perplexities2\n",
    "\n",
    "plt.figure(dpi=1200) \n",
    "\n",
    "filter_ids = [0,1,2,3,4,5,6,7]\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(filter_ids), figsize=(14,10.5), dpi=1200)\n",
    "\n",
    "ylim = (-0.03, 1500)\n",
    "\n",
    "for i, mode_id in enumerate(filter_ids):\n",
    "    ax = axes[i]\n",
    "    recovered_perps = all_sparsities2[:,0,mode_id]\n",
    "\n",
    "    scale_color = 1.3 - layer/len(layers)\n",
    "    xs = np.arange(len(layers))/(2*len(layers)) - (len(layers)-1)/(4*len(layers))\n",
    "    ax.bar(xs,recovered_perps[:,0], width=0.5/len(layers), color=multi_shade(palette[mode_id], len(layers)))\n",
    "    ax.set_title(f\"{nice_modes[mode_id]}\")\n",
    "    ax.set_xticks(xs, [f\"{l+1}\" for l in layers], rotation=0)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "axes[0].set_ylabel(\"Average L0\", fontsize=24)\n",
    "plt.tight_layout()\n",
    "# plt.title(\"Sparsity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot all_model_perplexities - all_model_perplexities2\n",
    "\n",
    "plt.figure(dpi=1200) \n",
    "\n",
    "filter_ids = [0,1,2,3,4]\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(filter_ids), figsize=(14,10.5), dpi=1200)\n",
    "\n",
    "ylim = (-0.03, 1.01)\n",
    "\n",
    "for i, mode_id in enumerate(filter_ids):\n",
    "    ax = axes[i]\n",
    "    recovered_perps = all_l2_ratios[:,0,mode_id]\n",
    "\n",
    "    xs = np.arange(len(layers))/(2*len(layers)) - (len(layers)-1)/(4*len(layers))\n",
    "    ax.bar(xs,recovered_perps[:,0], width=0.5/len(layers), color=multi_shade(palette[mode_id], len(layers)))\n",
    "    ax.set_title(f\"{nice_modes[mode_id]}\")\n",
    "    ax.set_xticks(xs, [f\"{l+1}\" for l in layers], rotation=0)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "axes[0].set_ylabel(\"L2 Ratio\", fontsize=24)\n",
    "# plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot all_model_perplexities - all_model_perplexities2\n",
    "\n",
    "filter_ids = [0,]\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(filter_ids), figsize=(8,6), dpi=1200)\n",
    "\n",
    "ylim = (-0.03, 1.01)\n",
    "\n",
    "for i, mode_id in enumerate(filter_ids):\n",
    "    if len(filter_ids)==1:\n",
    "        ax = axes\n",
    "    else:\n",
    "        ax = axes[i]\n",
    "    if i==0:\n",
    "        ax.set_ylabel(\"L2 Ratio\", fontsize=16)\n",
    "    recovered_perps = all_l2_ratios[:,0,mode_id]\n",
    "\n",
    "    xs = np.arange(len(layers))/(2*len(layers)) - (len(layers)-1)/(4*len(layers))\n",
    "    ax.bar(xs,recovered_perps[:,0], width=0.5/len(layers), color=multi_shade(palette[mode_id], len(layers)))\n",
    "    # ax.set_title(f\"{nice_modes[mode_id]}\")\n",
    "    ax.set_xticks(xs, [f\"{l+1}\" for l in layers], rotation=0)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "# plt.tight_layout()\n",
    "ax.set_title(f\"SAE Reconstruction L2 Ratio\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot all_model_perplexities - all_model_perplexities2\n",
    "\n",
    "filter_ids = [0,1,2,3,4,5,6,7]\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(filter_ids), figsize=(14,10.5), dpi=1200)\n",
    "\n",
    "ylim = (-0.03, 1.1)\n",
    "\n",
    "for i, mode_id in enumerate(filter_ids):\n",
    "    if len(filter_ids)==1:\n",
    "        ax = axes\n",
    "    else:\n",
    "        ax = axes[i]\n",
    "    if i==0:\n",
    "        ax.set_ylabel(\"L2 Ratio\", fontsize=24)\n",
    "    recovered_perps = all_l2_ratios[:,0,mode_id]\n",
    "\n",
    "    xs = np.arange(len(layers))/(2*len(layers)) - (len(layers)-1)/(4*len(layers))\n",
    "    ax.bar(xs,recovered_perps[:,0], width=0.5/len(layers), color=multi_shade(palette[mode_id], len(layers)))\n",
    "    ax.set_title(f\"{nice_modes[mode_id]}\")\n",
    "    ax.set_xticks(xs, [f\"{l+1}\" for l in layers], rotation=0)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "plt.tight_layout()\n",
    "# ax.set_title(f\"Original SAE\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_mses2, \"all_mses.pt\")\n",
    "torch.save(all_perplexities2, \"all_perplexities.pt\")\n",
    "torch.save(all_sparsities2, \"all_sparsities.pt\")\n",
    "torch.save(all_l2_ratios, \"all_l2_ratios.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mses2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_perplexities2[:,0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sparsities2[:,0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mses2[:,0,:,0]/512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_l2_ratios[:,0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1,2,3,4,5]:\n",
    "    avg_mse_change = 1 - (all_mses2[:,0,[i],0] / all_mses2[:,0,[0],0]).mean()\n",
    "    print(avg_mse_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1,2,3,4,5]:\n",
    "    avg_perp_change = (all_perplexities2[:,0,[0],0] - all_perplexities2[:,0,[i],0])/(all_perplexities2[:,0,[0],0] - original_perplexity)\n",
    "    print(avg_perp_change.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot all_model_perplexities - all_model_perplexities2\n",
    "import matplotlib.pyplot as plt\n",
    "model_perplexity_diff = np.array(all_model_perplexities) - np.array(all_model_perplexities2)\n",
    "plt.plot(model_perplexity_diff.T)\n",
    "plt.legend(models)\n",
    "plt.title(\"Perplexity difference between reconstruction-top-2-outliers and reconstructed model\")\n",
    "plt.xlabel(\"Autoencoder\")\n",
    "plt.ylabel(\"Perplexity difference\")\n",
    "# set ylimit to 10\n",
    "plt.ylim(0, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the pythia perplexities & gpts\n",
    "plt.plot(all_model_perplexities2[0], label=\"GPT2-top-2-outliers\", color=\"orange\")\n",
    "plt.plot(all_model_perplexities[0], label=\"GPT2\", linestyle=\"--\", color=\"orange\")\n",
    "plt.plot(all_model_perplexities2[1], label=\"Pythia-top-2-outliers\", color=\"blue\")\n",
    "plt.plot(all_model_perplexities[1], label=\"Pythia\", linestyle=\"--\", color=\"blue\")\n",
    "plt.legend()\n",
    "plt.title(\"Perplexity of GPT2 and Pythia with and without top-2-outliers\")\n",
    "plt.xlabel(\"Autoencoder\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.ylim(20, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perplexity_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_perplexities # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_perplexities # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_perplexities # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_perplexities# 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_dims = find_outliers(dataset, cache_name, model, num_outlier_dims=2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change these settings to load the correct autoencoder\n",
    "layer = 2\n",
    "setting = \"residual\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "# filename = \"outputs/20230720-192301-EleutherAI/pythia-70m-deduped-2/minirun9/autoencoders.pkl\"\n",
    "filename = \"outputs/20230720-192301-EleutherAI/pythia-70m-deduped-2/minirun5/autoencoders.pkl\"\n",
    "# filename = \"outputs/20230721-161919-EleutherAI/pythia-70m-deduped-2/minirun9/autoencoders.pkl\"\n",
    "# filename = \"outputs/20230721-161919-EleutherAI/pythia-70m-deduped-2/minirun3/autoencoders.pkl\"\n",
    "# filename = \"/home/mchorse/logan_sparse_coding/sparse_coding/ae2.pkl\"\n",
    "\n",
    "\n",
    "if setting == \"residual\":\n",
    "    cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    neurons = model.cfg.d_model\n",
    "elif setting == \"mlp\":\n",
    "    cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "    neurons = model.cfg.d_mlp\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# Load the pickle file\n",
    "with open(filename, 'rb') as file:\n",
    "    autoencoders = pickle.load(file)\n",
    "\n",
    "# Index for l1 value, usually only 1 value is available\n",
    "l1_index = -1\n",
    "dictionaries = [autoencoder.decoder.weight.data.T for autoencoder in autoencoders[l1_index]]\n",
    "for d in dictionaries:\n",
    "    print(d.shape)\n",
    "print(\"len of autoencoders: \", len(autoencoders))\n",
    "\n",
    "dict_index = 0\n",
    "smaller_dict, larger_dict = dictionaries[dict_index], dictionaries[dict_index+1]\n",
    "smaller_auto_encoder, larger_auto_encoder = autoencoders[l1_index][dict_index], autoencoders[l1_index][dict_index+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from torch import nn\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "# autoencoder_filename = \"/home/mchorse/logan/sparse_coding/output_sweep_tied_mlpout_l1_r8/_9/learned_dicts.pt\"\n",
    "# autoencoder_filename = \"/home/mchorse/logan/sparse_coding/output_sweep_tied_mlpout_l2_r4/_19/learned_dicts.pt\"\n",
    "# autoencoder_filename = \"/home/mchorse/sparse_coding_hoagy/tiedlong_tied_residual_l5_r4/_10/learned_dicts.pt\"\n",
    "# autoencoder_filename = \"/home/mchorse/logan/sparse_coding/output_hoagy_dense_sweep_tied_residual_l4_r6/_99/learned_dicts.pt\"\n",
    "# autoencoder_filename = \"/home/mchorse/logan/sparse_coding/output_hoagy_dense_sweep_tied_residual_l4_r6/_0/learned_dicts.pt\"\n",
    "layer = 4\n",
    "\n",
    "autoencoder_filename = \"/home/mchorse/logan/sparse_coding/output_hoagy_dense_sweep_tied_residual_l4_r6/_0/learned_dicts.pt\"\n",
    "auto_num = 0 # Selects which specific autoencoder to use\n",
    "all_autoencoders = torch.load(autoencoder_filename)\n",
    "num_dictionaries = len(all_autoencoders)\n",
    "print(f\"Loaded {num_dictionaries} autoencoders\")\n",
    "autoencoder, hyperparams = all_autoencoders[auto_num]\n",
    "l1_alpha = hyperparams['l1_alpha']\n",
    "# autoencoder2, hyperparams2 = all_autoencoders[auto_num+1]\n",
    "smaller_dict = autoencoder.get_learned_dict()\n",
    "# larger_dict = autoencoder2.get_learned_dict()\n",
    "\n",
    "#Change these settings to load the correct autoencoder\n",
    "setting = \"residual\"\n",
    "# setting = \"attention\"\n",
    "# setting = \"mlp\"\n",
    "# setting = \"mlp_out\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "# model_name = \"EleutherAI/pythia-160m\"\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "if setting == \"residual\":\n",
    "    cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    neurons = model.cfg.d_model\n",
    "elif setting == \"mlp\":\n",
    "    cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "    neurons = model.cfg.d_mlp\n",
    "elif setting == \"attention\":\n",
    "    cache_name = f\"blocks.{layer}.hook_attn_out\"\n",
    "    neurons = model.cfg.d_model\n",
    "elif setting == \"mlp_out\":\n",
    "    cache_name = f\"blocks.{layer}.hook_mlp_out\"\n",
    "    neurons = model.cfg.d_model\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "layer = 2\n",
    "setting = \"residual\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "if setting == \"residual\":\n",
    "    cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    neurons = model.cfg.d_model\n",
    "elif setting == \"mlp\":\n",
    "    cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "    neurons = model.cfg.d_mlp\n",
    "else:\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCS\n",
    "Max cosine similarity between one dictionary & another one. If they learned the same feature, then they'll have high cosine similarity. \n",
    "\n",
    "If two dictionaries learned it, it's probably a real feature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model activations & Dictionary Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downnload dataset\n",
    "dataset_name = \"NeelNanda/pile-10k\"\n",
    "token_amount= 256\n",
    "dataset = load_dataset(dataset_name, split=\"train\").map(\n",
    "    lambda x: model.tokenizer(x['text']),\n",
    "    batched=True,\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > token_amount\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:token_amount]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def mlp_ablation_hook(value, hook, autoencoder):\n",
    "    # Rearrange to fit autoencoder\n",
    "    int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "    # Run through the autoencoder\n",
    "    reconstruction = autoencoder.predict(int_val)\n",
    "    batch, seq_len, hidden_size = value.shape\n",
    "    reconstruction = rearrange(reconstruction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "    return reconstruction\n",
    "n_chunks = 12\n",
    "chunk_size = 2\n",
    "l1_values = [1e-3, 9e-4, 8e-4, 7e-4]\n",
    "all_perplexities = np.zeros(4, (n_chunks//chunk_size))\n",
    "for chunk_trained in range(0,12,2): \n",
    "    autoencoder_filename = f\"/home/mchorse/logan/sparse_coding/output_hoagy_dense_sweep_tied_residual_l4_r6/_{chunk_trained}/learned_dicts.pt\"\n",
    "    all_autoencoders = torch.load(autoencoder_filename)\n",
    "    model = model.eval()\n",
    "    batch_size = 128\n",
    "    num_dictionaries = len(all_autoencoders)\n",
    "    l1_perplexity = np.zeros(num_dictionaries + 1)\n",
    "    all_l1s = np.zeros(num_dictionaries + 1)\n",
    "    for l1_index in range(len(all_autoencoders)):\n",
    "        # print(f\"L1 index: {l1_index}\")\n",
    "        autoencoder, hyperparams = all_autoencoders[l1_index]\n",
    "        autoencoder.to_device(device)\n",
    "        print(hyperparams)\n",
    "        # for dict_index in range(num_dictionaries + 1):\n",
    "        #     if(dict_index != num_dictionaries):\n",
    "        #         autoencoder, hyperparams = all_autoencoders[dict_index]\n",
    "        #         autoencoder.to_device(device)\n",
    "        #         print(hyperparams)\n",
    "        #     else:\n",
    "        #         continue\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "            dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "            for i, batch in enumerate(dl):\n",
    "                # Get model predictions\n",
    "                if(l1_index == num_dictionaries): # Full model\n",
    "                    loss = model(batch.to(device), return_type=\"loss\")\n",
    "                else:\n",
    "                    loss = model.run_with_hooks(batch.to(device), \n",
    "                        return_type=\"loss\",\n",
    "                        fwd_hooks=[(\n",
    "                            cache_name, \n",
    "                            partial(mlp_ablation_hook, autoencoder=autoencoder),\n",
    "                            )]\n",
    "                        )\n",
    "                total_loss += loss.item()\n",
    "        # Average\n",
    "        avg_neg_log_likelihood = total_loss / len(dl)\n",
    "\n",
    "        # Convert to tensor\n",
    "        avg_neg_log_likelihood = torch.tensor(avg_neg_log_likelihood, device=device)\n",
    "\n",
    "        # Exponentiate to compute perplexity\n",
    "        perplexity = torch.exp(avg_neg_log_likelihood)\n",
    "        if(l1_index == num_dictionaries):\n",
    "            print(\"Perplexity for full model: \", perplexity.item())\n",
    "            original_perplexity = perplexity.item()\n",
    "        else:\n",
    "            # print(f\"Perplexity for dict_size={dictionaries[dict_index].shape[0]}: {perplexity.item():.2f}\")\n",
    "            print(f\"{perplexity.item():.2f}\")\n",
    "        # l1_perplexity[l1_index] = perplexity.item()\n",
    "        all_perplexities[chunk_trained, l1_index] = perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_autoencoders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def mlp_ablation_hook(value, hook, autoencoder):\n",
    "    # Rearrange to fit autoencoder\n",
    "    int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "    # Run through the autoencoder\n",
    "    reconstruction = autoencoder.predict(int_val)\n",
    "    batch, seq_len, hidden_size = value.shape\n",
    "    reconstruction = rearrange(reconstruction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "    return reconstruction\n",
    "\n",
    "model = model.eval()\n",
    "batch_size = 128\n",
    "# for l1_index in range(len(autoencoders)):\n",
    "#     print(f\"L1 index: {l1_index}\")\n",
    "#     all_autoencoders = autoencoders[l1_index]\n",
    "all_autoencoders = all_autoencoders[3:]\n",
    "num_dictionaries = len(all_autoencoders)\n",
    "all_perplexities = np.zeros(num_dictionaries + 1)\n",
    "all_l1s = np.zeros(num_dictionaries + 1)\n",
    "for dict_index in range(num_dictionaries + 1):\n",
    "    if(dict_index != num_dictionaries):\n",
    "        autoencoder, hyperparams = all_autoencoders[dict_index]\n",
    "        autoencoder.to_device(device)\n",
    "        print(hyperparams)\n",
    "    else:\n",
    "        continue\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(dl):\n",
    "            # Get model predictions\n",
    "            if(dict_index == num_dictionaries): # Full model\n",
    "                loss = model(batch.to(device), return_type=\"loss\")\n",
    "            else:\n",
    "                loss = model.run_with_hooks(batch.to(device), \n",
    "                    return_type=\"loss\",\n",
    "                    fwd_hooks=[(\n",
    "                        cache_name, \n",
    "                        partial(mlp_ablation_hook, autoencoder=autoencoder),\n",
    "                        )]\n",
    "                    )\n",
    "            total_loss += loss.item()\n",
    "    # Average\n",
    "    avg_neg_log_likelihood = total_loss / len(dl)\n",
    "\n",
    "    # Convert to tensor\n",
    "    avg_neg_log_likelihood = torch.tensor(avg_neg_log_likelihood, device=device)\n",
    "\n",
    "    # Exponentiate to compute perplexity\n",
    "    perplexity = torch.exp(avg_neg_log_likelihood)\n",
    "    if(dict_index == num_dictionaries):\n",
    "        print(\"Perplexity for full model: \", perplexity.item())\n",
    "        original_perplexity = perplexity.item()\n",
    "    else:\n",
    "        # print(f\"Perplexity for dict_size={dictionaries[dict_index].shape[0]}: {perplexity.item():.2f}\")\n",
    "        print(f\"{perplexity.item():.2f}\")\n",
    "    all_perplexities[dict_index] = perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run_9                                   Run_5 (~converged on reconstruction loss. This is check connection between recon & perplexity and MMCS & perplexity)      \n",
    "Perplexity for full model:  25.12\n",
    "L1 index: 0 (3e-3)                                Sparsity\n",
    "Perplexity for dict_size=2048: 169.76   176.08    24\n",
    "Perplexity for dict_size=4096: 138.02   171.45    26\n",
    "Perplexity for dict_size=8192: 127.86   124.92    26\n",
    "L1 index: 1 (1e-3)\n",
    "Perplexity for dict_size=2048: 59.14    59.96    73\n",
    "Perplexity for dict_size=4096: 53.14    55.28    72\n",
    "Perplexity for dict_size=8192: 51.90    51.16    71\n",
    "L1 index: 2 (8e-4)\n",
    "Perplexity for dict_size=2048: 48.80    48.25    96\n",
    "Perplexity for dict_size=4096: 47.09    45.67    95\n",
    "Perplexity for dict_size=8192: 55.75    44.87    93\n",
    "\n",
    "L1 index: 3 (6e-4)                      Run_3 (converged on reconstruction loss)\n",
    "Perplexity for dict_size=2048: 41.49    39.21    127\n",
    "Perplexity for dict_size=4096: 42.53    38.97    127\n",
    "Perplexity for dict_size=8192: 39.87    39.46    124\n",
    "L1 index: 4 (4e-4)\n",
    "Perplexity for dict_size=2048: 30.51    31.06    245\n",
    "Perplexity for dict_size=4096: 49.06    32.59    197 #NOTE: LOGAN DO A DIFF DICT HERE YOU LOSER\n",
    "Perplexity for dict_size=8192: 33.24    32.81    181\n",
    "L1 index: 5 (2e-4)\n",
    "Perplexity for dict_size=2048: 26.19    26.35    387\n",
    "Perplexity for dict_size=4096: 26.23    26.27    407\n",
    "Perplexity for dict_size=8192: 26.27    27.28    377\n",
    "'''\n",
    "\n",
    "'''\n",
    "L1 index   Perplexity  Layer  n_chunks   Size\n",
    "1e-3            38.8     4      100       3k\n",
    "1e-3            38.8     4      80        3k\n",
    "1e-3            38.8     4      60        3k\n",
    "1e-3            38.3     4      40        3k\n",
    "1e-3            38.82    4      20        3k\n",
    "1e-3            50.61    4      1         3k\n",
    "1e-3            49       4      80        2k\n",
    "7e-4            38       4      80        2k\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run_3 (~converged on reconstruction loss. This is check connection between recon & perplexity and MMCS & perplexity)\n",
    "Run_9\n",
    "Perplexity for full model:  25.12\n",
    "L1 index: 0 (3e-3)\n",
    "L1 index: 1 (1e-3)\n",
    "L1 index: 2 (8e-4)\n",
    "L1 index: 3 (6e-4)\n",
    "Perplexity for dict_size=2048: 41.49\n",
    "Perplexity for dict_size=4096: 42.53\n",
    "Perplexity for dict_size=8192: 39.87\n",
    "L1 index: 4 (4e-4)\n",
    "Perplexity for dict_size=2048: 30.51\n",
    "Perplexity for dict_size=4096: 49.06\n",
    "Perplexity for dict_size=8192: 33.24\n",
    "L1 index: 5 (2e-4)\n",
    "Perplexity for dict_size=2048: 26.19\n",
    "Perplexity for dict_size=4096: 26.23\n",
    "Perplexity for dict_size=8192: 26.27\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import numpy as np \n",
    "from torch import nn\n",
    "import pickle\n",
    "\n",
    "def mlp_ablation_hook(value, hook, autoencoder):\n",
    "    # Rearrange to fit autoencoder\n",
    "    int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "    # Run through the autoencoder\n",
    "    reconstruction = autoencoder.predict(int_val)\n",
    "    batch, seq_len, hidden_size = value.shape\n",
    "    reconstruction = rearrange(reconstruction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "    return reconstruction\n",
    "\n",
    "all_autoencoders = torch.load(\"/home/mchorse/sparse_coding_aidan_new/output_4_rd_deep/_7/learned_dicts.pt\")\n",
    "num_dictionaries = len(all_autoencoders)\n",
    "all_perplexities = np.zeros(num_dictionaries + 1)\n",
    "all_l1s = np.zeros(num_dictionaries + 1)\n",
    "model = model.eval()\n",
    "batch_size = 32\n",
    "for dict_index in range(num_dictionaries + 1):\n",
    "    autoencoder, hyperparams = all_autoencoders[dict_index]\n",
    "    l1_alpha = hyperparams['l1_alpha']\n",
    "    autoencoder.to_device(device)\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(dl):\n",
    "            # Get model predictions\n",
    "            if(dict_index == num_dictionaries): # Full model\n",
    "                loss = model(batch.to(device), return_type=\"loss\")\n",
    "            else:\n",
    "                loss = model.run_with_hooks(batch.to(device), \n",
    "                    return_type=\"loss\",\n",
    "                    fwd_hooks=[(\n",
    "                        cache_name, \n",
    "                        partial(mlp_ablation_hook, autoencoder=autoencoder),\n",
    "                        )]\n",
    "                    )\n",
    "            total_loss += loss.item()\n",
    "    # Average\n",
    "    avg_neg_log_likelihood = total_loss / len(dl)\n",
    "\n",
    "    # Convert to tensor\n",
    "    avg_neg_log_likelihood = torch.tensor(avg_neg_log_likelihood, device=device)\n",
    "\n",
    "    # Exponentiate to compute perplexity\n",
    "    perplexity = torch.exp(avg_neg_log_likelihood)\n",
    "    if(dict_index == num_dictionaries):\n",
    "        print(\"Perplexity for full model: \", perplexity.item())\n",
    "    else:\n",
    "        print(f\"Perplexity for l1={l1_alpha:.2E}: {perplexity.item():.2f}\")\n",
    "    all_perplexities[dict_index] = perplexity.item()\n",
    "    all_l1s[dict_index] = l1_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "# Load model\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "model = model.eval()\n",
    "\n",
    "# Load autoencoders\n",
    "all_autoencoders = torch.load(\"/home/mchorse/sparse_coding_aidan_new/output_4_rd_deep/_7/learned_dicts.pt\")\n",
    "num_dictionaries = len(all_autoencoders)\n",
    "\n",
    "# Set layer & cache name\n",
    "layer = 2\n",
    "setting = \"residual\"\n",
    "if setting == \"residual\":\n",
    "    cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    neurons = model.cfg.d_model\n",
    "elif setting == \"mlp\":\n",
    "    cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "    neurons = model.cfg.d_mlp\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Download dataset\n",
    "batch_size = 32\n",
    "dataset_name = \"NeelNanda/pile-10k\"\n",
    "token_amount= 256\n",
    "dataset = load_dataset(dataset_name, split=\"train\").map(\n",
    "    lambda x: model.tokenizer(x['text']),\n",
    "    batched=True,\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > token_amount\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:token_amount]}\n",
    ")\n",
    "\n",
    "# Define function to replace activations with reconstruction\n",
    "def replace_with_reconstruction(value, hook, autoencoder):\n",
    "    # Rearrange to fit autoencoder\n",
    "    int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "    # Run through the autoencoder\n",
    "    reconstruction = autoencoder.predict(int_val)\n",
    "    batch, seq_len, hidden_size = value.shape\n",
    "    reconstruction = rearrange(reconstruction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "    return reconstruction\n",
    "\n",
    "with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "    dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "    # Calculate Original Perplexity ie no intervention/no dictionary\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(dl):\n",
    "        loss = model(batch.to(device), return_type=\"loss\")\n",
    "        total_loss += loss.item()\n",
    "    # Average\n",
    "    avg_neg_log_likelihood = total_loss / len(dl)\n",
    "    # Convert to tensor\n",
    "    avg_neg_log_likelihood = torch.tensor(avg_neg_log_likelihood, device=device)\n",
    "    # Exponentiate to compute perplexity\n",
    "    original_perplexity = torch.exp(avg_neg_log_likelihood)\n",
    "    print(\"Perplexity for original model: \", original_perplexity.item())\n",
    "\n",
    "    # Compute perplexity for each dictionary\n",
    "    all_perplexities = np.zeros(num_dictionaries)\n",
    "    all_l1s = np.zeros(num_dictionaries)\n",
    "    # Calculate Perplexity for each dictionary\n",
    "    for dict_index in range(num_dictionaries):\n",
    "        autoencoder, hyperparams = all_autoencoders[dict_index]\n",
    "        l1_alpha = hyperparams['l1_alpha']\n",
    "        autoencoder.to_device(device)\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(dl):\n",
    "            # Perplexity with reconstructed activations\n",
    "            loss = model.run_with_hooks(batch.to(device), \n",
    "                return_type=\"loss\",\n",
    "                fwd_hooks=[(\n",
    "                    cache_name, # intermediate activation that we intervene on\n",
    "                    partial(replace_with_reconstruction, autoencoder=autoencoder), # function to apply to cache_name\n",
    "                    )]\n",
    "                )\n",
    "            total_loss += loss.item()\n",
    "        # Average\n",
    "        avg_neg_log_likelihood = total_loss / len(dl)\n",
    "        # Convert to tensor\n",
    "        avg_neg_log_likelihood = torch.tensor(avg_neg_log_likelihood, device=device)\n",
    "        # Exponentiate to compute perplexity\n",
    "        perplexity = torch.exp(avg_neg_log_likelihood)\n",
    "        print(f\"Perplexity for l1={l1_alpha:.2E}: {perplexity.item():.2f}\")\n",
    "        all_perplexities[dict_index] = perplexity.item()\n",
    "        all_l1s[dict_index] = l1_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot perplexities against their l1s\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(all_l1s, all_perplexities)\n",
    "plt.xlabel('l1')\n",
    "plt.ylabel('perplexity')\n",
    "plt.yscale('log')\n",
    "plt.title('Pythia-70m Residual Layer 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_perplexities[:32].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot perplexities against their l1s\n",
    "N = 15\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(all_l1s[:N], all_perplexities[:N])\n",
    "plt.xlabel('l1')\n",
    "plt.ylabel('perplexity')\n",
    "# plt.yscale('log')\n",
    "plt.title('Pythia-70m Residual Layer 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_l1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_perplexities.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "outlier_dimensions = torch.tensor([216, 117, 437, 341, 215,  58, 477, 113, 265,  41, 145, 298, 61, 359, 355, 290])\n",
    "outlier_dimensions = torch.tensor([111, 156,  71,  47, 369, 473, 396, 197,  48, 258])\n",
    "perplexities_individual = torch.zeros(len(outlier_dimensions))\n",
    "l1_index = 1\n",
    "num_dictionaries = len(autoencoders[l1_index])\n",
    "# for dict_index in range(num_dictionaries):\n",
    "for dict_index in range(len(outlier_dimensions)):\n",
    "    def ablate_residual_dimension(value, hook):\n",
    "        # Rearrange to fit autoencoder\n",
    "        value[:, :, outlier_dimensions[dict_index:dict_index+1]] = 0.0\n",
    "        return value\n",
    "    model = model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    batch_size = 128\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(dl):\n",
    "            # Get model predictions\n",
    "            loss = model.run_with_hooks(batch.to(device), \n",
    "                return_type=\"loss\",\n",
    "                fwd_hooks=[(\n",
    "                    cache_name, \n",
    "                    ablate_residual_dimension\n",
    "                    )]\n",
    "                )\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Average\n",
    "    avg_neg_log_likelihood = total_loss / len(dl)\n",
    "\n",
    "    # Convert to tensor\n",
    "    avg_neg_log_likelihood = torch.tensor(avg_neg_log_likelihood, device=device)\n",
    "\n",
    "    # Exponentiate to compute perplexity\n",
    "    perplexity = torch.exp(avg_neg_log_likelihood)\n",
    "    print(f\"Perplexity for Outlier_dimension={dict_index}: {perplexity.item():.2f}\")\n",
    "    perplexities_individual[dict_index] = perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot perplexities: y-axis perplexity, x-axis: number of outlier dimensions ablated\n",
    "# Shift perplexities_individual to the right by 1 & assign it's first value to perplexities[0]\n",
    "perplexities_new = torch.zeros(len(outlier_dimensions)+1)\n",
    "perplexities_new[1:] = perplexities_individual\n",
    "perplexities_new[0] = perplexities[0]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(perplexities.cpu().numpy(), label=\"Cumulative Ablation\")\n",
    "plt.plot(perplexities_new[:-1].cpu().numpy(), label=\"Individual Ablation\")\n",
    "plt.xlabel(\"Number of outlier dimensions ablated\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "# neurons = model.W_in.shape[-1]\n",
    "neurons = model.cfg.d_model\n",
    "datapoints = dataset.num_rows\n",
    "batch_size = 64\n",
    "neuron_activations = torch.zeros((datapoints*token_amount, neurons))\n",
    "dictionary_activations = torch.zeros((datapoints*token_amount, smaller_dict_features))\n",
    "smaller_auto_encoder = smaller_auto_encoder.to(device)\n",
    "\n",
    "with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "    dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "    for i, batch in enumerate(tqdm(dl)):\n",
    "        _, cache = model.run_with_cache(batch.to(device))\n",
    "        batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "        neuron_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_neuron_activations.cpu()\n",
    "        reconstruction, batched_dictionary_activations = smaller_auto_encoder(batched_neuron_activations)\n",
    "        dictionary_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_dictionary_activations.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_activations[:10000,:].count_nonzero(axis=1).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Activation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.activations import text_neuron_activations\n",
    "# Get the activations for the best dict features\n",
    "def get_feature_datapoints(feature_index, dictionary_activations, dataset, k=10, setting=\"max\"):\n",
    "    best_feature_activations = dictionary_activations[:, feature_index]\n",
    "    # Sort the features by activation, get the indices\n",
    "    if setting==\"max\":\n",
    "        found_indices = torch.argsort(best_feature_activations, descending=True)[:k]\n",
    "    elif setting==\"uniform\":\n",
    "        min_value = torch.min(best_feature_activations)\n",
    "        max_value = torch.max(best_feature_activations)\n",
    "\n",
    "        # Define the number of bins\n",
    "        num_bins = k\n",
    "\n",
    "        # Calculate the bin boundaries as linear interpolation between min and max\n",
    "        bin_boundaries = torch.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Assign each activation to its respective bin\n",
    "        bins = torch.bucketize(best_feature_activations, bin_boundaries)\n",
    "\n",
    "        # Initialize a list to store the sampled indices\n",
    "        sampled_indices = []\n",
    "\n",
    "        # Sample from each bin\n",
    "        for bin_idx in torch.unique(bins):\n",
    "            # Get the indices corresponding to the current bin\n",
    "            bin_indices = torch.nonzero(bins == bin_idx, as_tuple=False).squeeze(dim=1)\n",
    "            \n",
    "            # Randomly sample from the current bin\n",
    "            sampled_indices.extend(np.random.choice(bin_indices, size=1, replace=False))\n",
    "\n",
    "        # Convert the sampled indices to a PyTorch tensor & reverse order\n",
    "        found_indices = torch.tensor(sampled_indices).long().flip(dims=[0])\n",
    "    else: # random\n",
    "        # get nonzero indices\n",
    "        nonzero_indices = torch.nonzero(best_feature_activations)[:, 0]\n",
    "        # shuffle\n",
    "        shuffled_indices = nonzero_indices[torch.randperm(nonzero_indices.shape[0])]\n",
    "        found_indices = shuffled_indices[:k]\n",
    "    datapoint_indices =[np.unravel_index(i, (datapoints, token_amount)) for i in found_indices]\n",
    "    text_list = []\n",
    "    full_text = []\n",
    "    token_list = []\n",
    "    full_token_list = []\n",
    "    for md, s_ind in datapoint_indices:\n",
    "        md = int(md)\n",
    "        s_ind = int(s_ind)\n",
    "        full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "        full_text.append(model.tokenizer.decode(full_tok))\n",
    "        tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "        text = model.tokenizer.decode(tok)\n",
    "        text_list.append(text)\n",
    "        token_list.append(tok)\n",
    "        full_token_list.append(full_tok)\n",
    "    return text_list, full_text, token_list, full_token_list\n",
    "\n",
    "def get_neuron_activation(token, feature, model, setting=\"dictionary_basis\"):\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(token.to(model.cfg.device))\n",
    "        neuron_act_batch = cache[cache_name]\n",
    "        if setting==\"dictionary_basis\":\n",
    "            _, act = smaller_auto_encoder(neuron_act_batch)\n",
    "            return act[0, :, feature].tolist()\n",
    "        else: # neuron/residual basis\n",
    "            return neuron_act_batch[0, :, feature].tolist()\n",
    "\n",
    "def ablate_text(text, feature, model, setting=\"plot\"):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    display_text_list = []\n",
    "    activation_list = []\n",
    "    for t in text:\n",
    "        # Convert text into tokens\n",
    "        if isinstance(t, str): # If the text is a list of tokens\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            tokens = model.to_tokens(t, prepend_bos=False)\n",
    "        else: # t equals tokens\n",
    "            tokens = t\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "        seq_size = tokens.shape[1]\n",
    "        if(seq_size == 1): # If the text is a single token, we can't ablate it\n",
    "            continue\n",
    "        original = get_neuron_activation(tokens, feature, model)[-1]\n",
    "        changed_activations = torch.zeros(seq_size, device=device).cpu()\n",
    "        for i in range(seq_size):\n",
    "            # Remove the i'th token from the input\n",
    "            ablated_tokens = torch.cat((tokens[:,:i], tokens[:,i+1:]), dim=1)\n",
    "            changed_activations[i] += get_neuron_activation(ablated_tokens, feature, model)[-1]\n",
    "        changed_activations -= original\n",
    "        display_text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "        activation_list += changed_activations.tolist() + [0.0]\n",
    "    activation_list = torch.tensor(activation_list).reshape(-1,1,1)\n",
    "    if setting == \"plot\":\n",
    "        return text_neuron_activations(tokens=display_text_list, activations=activation_list)\n",
    "    else:\n",
    "        return display_text_list, activation_list\n",
    "def visualize_text(text, feature, model, setting=\"dictionary_basis\", max_activation = None):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    if isinstance(feature, int):\n",
    "        feature = [feature]\n",
    "    display_text_list = []\n",
    "    act_list = []\n",
    "    for t in text:\n",
    "        if isinstance(t, str): # If the text is a list of tokens\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            token = model.to_tokens(t, prepend_bos=False)\n",
    "        else: # t are tokens\n",
    "            token = t\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "        for f in feature:\n",
    "            display_text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            act_list += get_neuron_activation(token, f, model, setting) + [0.0]\n",
    "    act_list = torch.tensor(act_list).reshape(-1,1,1)\n",
    "    if(max_activation is not None):\n",
    "        act_list = torch.clamp(act_list, max=max_activation)\n",
    "    return text_neuron_activations(tokens=display_text_list, activations=act_list)\n",
    "# Ablate the feature direction of the tokens\n",
    "# token_list is a list of tokens, convert to tensor of shape (batch_size, seq_len)\n",
    "from einops import rearrange\n",
    "def ablate_feature_direction(tokens, feature, model, autoencoder):\n",
    "    def mlp_ablation_hook(value, hook):\n",
    "        # Rearrange to fit autoencoder\n",
    "        int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "\n",
    "        # Run through the autoencoder\n",
    "        _, act = autoencoder(int_val)\n",
    "        feature_to_ablate = feature # TODO: bring this out of the function\n",
    "\n",
    "        # Subtract value with feature direction*act_of_feature\n",
    "        feature_direction = torch.outer(act[:, feature_to_ablate].squeeze(), autoencoder.decoder.weight[:, feature_to_ablate].squeeze())\n",
    "        batch, seq_len, hidden_size = value.shape\n",
    "        feature_direction = rearrange(feature_direction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "        value -= feature_direction\n",
    "        return value\n",
    "\n",
    "    return model.run_with_hooks(tokens, \n",
    "        fwd_hooks=[(\n",
    "            cache_name, \n",
    "            mlp_ablation_hook\n",
    "            )]\n",
    "        )\n",
    "def add_feature_direction(tokens, feature, model, autoencoder, scalar=1.0):\n",
    "    def residual_add_hook(value, hook):\n",
    "        feature_direction = autoencoder.decoder.weight[:, feature].squeeze()\n",
    "        value += scalar*feature_direction\n",
    "        return value\n",
    "\n",
    "    return model.run_with_hooks(tokens, \n",
    "        fwd_hooks=[(\n",
    "            cache_name,\n",
    "            residual_add_hook\n",
    "            )]\n",
    "        )\n",
    "def ablate_feature_direction_display(text, features=None, setting=\"true_tokens\", verbose=False):\n",
    "\n",
    "    if features==None:\n",
    "        features = torch.tensor([best_feature])\n",
    "    if isinstance(features, int):\n",
    "        features = torch.tensor([features])\n",
    "    if isinstance(features, list):\n",
    "        features = torch.tensor(features)\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    text_list = []\n",
    "    logit_list = []\n",
    "    for t in text:\n",
    "        tokens = model.to_tokens(t, prepend_bos=False)\n",
    "        with torch.no_grad():\n",
    "            original_logits = model(tokens).log_softmax(-1).cpu()\n",
    "            ablated_logits = ablate_feature_direction(tokens, features, model, smaller_auto_encoder).log_softmax(-1).cpu()\n",
    "        diff_logits = ablated_logits  - original_logits# ablated > original -> negative diff\n",
    "        tokens = tokens.cpu()\n",
    "        if setting == \"true_tokens\":\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            gather_tokens = rearrange(tokens[:,1:], \"b s -> b s 1\") # TODO: verify this is correct\n",
    "            # Gather the logits for the true tokens\n",
    "            diff = rearrange(diff_logits[:, :-1].gather(-1,gather_tokens), \"b s n -> (b s n)\")\n",
    "        elif setting == \"max\":\n",
    "            # Negate the diff_logits to see which tokens have the largest effect on the neuron\n",
    "            val, ind = (-1*diff_logits).max(-1)\n",
    "            diff = rearrange(val[:, :-1], \"b s -> (b s)\")\n",
    "            diff*= -1 # Negate the values gathered\n",
    "            split_text = model.to_str_tokens(ind, prepend_bos=False)\n",
    "            gather_tokens = rearrange(ind[:,1:], \"1 s -> 1 s 1\")\n",
    "        split_text = split_text[1:] # Remove the first token since we're not predicting it\n",
    "        if(verbose):\n",
    "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            orig = rearrange(original_logits[:, :-1].gather(-1, gather_tokens), \"b s n -> (b s n)\")\n",
    "            ablated = rearrange(ablated_logits[:, :-1].gather(-1, gather_tokens), \"b s n -> (b s n)\")\n",
    "            logit_list += orig.tolist() + [0.0]\n",
    "            logit_list += ablated.tolist() + [0.0]\n",
    "        text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "        logit_list += diff.tolist() + [0.0]\n",
    "    logit_list = torch.tensor(logit_list).reshape(-1,1,1)\n",
    "    if verbose:\n",
    "        print(f\"Max & Min logit-diff: {logit_list.max().item():.2f} & {logit_list.min().item():.2f}\")\n",
    "    return text_neuron_activations(tokens=text_list, activations=logit_list)\n",
    "def generate_text(input_text, num_tokens, model, autoencoder, feature, temperature=0.7, setting=\"add\", scalar=1.0):\n",
    "    # Convert input text to tokens\n",
    "    input_ids = model.tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    for _ in range(num_tokens):\n",
    "        # Generate logits\n",
    "        with torch.no_grad():\n",
    "            if(setting==\"add\"):\n",
    "                logits = add_feature_direction(input_ids, feature, model, autoencoder, scalar=scalar)\n",
    "            else:\n",
    "                logits = model(input_ids)\n",
    "\n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Sample from the distribution\n",
    "        probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "        predicted_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Append predicted token to input_ids\n",
    "        input_ids = torch.cat((input_ids, predicted_token), dim=-1)\n",
    "\n",
    "    # Decode the tokens to text\n",
    "    output_text = model.tokenizer.decode(input_ids[0])\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# Logit Lens\n",
    "def logit_lens(model, best_feature, smaller_dict, layer):\n",
    "    with torch.no_grad():\n",
    "        # There are never-used tokens, which have high norm. We want to ignore these.\n",
    "        bad_ind = (model.W_U.norm(dim=0) > 20)\n",
    "        feature_direction = smaller_dict[best_feature].to(device)\n",
    "        # feature_direction = torch.matmul(feature_direction, model.W_out[layer]) # if MLP\n",
    "        logits = torch.matmul(feature_direction, model.W_U).cpu()\n",
    "    # Don't include bad indices\n",
    "    logits[bad_ind] = -1000\n",
    "    topk_values, topk_indices = torch.topk(logits, 20)\n",
    "    top_text = model.to_str_tokens(topk_indices)\n",
    "    print(f\"{top_text}\")\n",
    "    print(topk_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text\n",
    "You can use the functions below to find interesting features to then add here to \"feature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \" for\"\n",
    "temp = 0.7\n",
    "tokens_to_generate = 20\n",
    "feature = 10 \n",
    "scalar = 100.0\n",
    "# Using the function:\n",
    "print(\"Normal:\\n\" + generate_text(sentence, tokens_to_generate, model, smaller_auto_encoder, feature=feature, temperature=temp, scalar=scalar, setting=\"normal\"))\n",
    "print(\"Add:\\n\" + generate_text(sentence, tokens_to_generate, model, smaller_auto_encoder, feature=feature, temperature=temp, scalar=scalar, setting=\"add\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Search\n",
    "Type in a sentence & see which features activate \n",
    "Note: Some features may be outliers, which will typically show up as high activations for the first word & first period or \\n (or high positive bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \" I do like a\"\n",
    "split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "token = model.to_tokens(t, prepend_bos=False)\n",
    "_, cache = model.run_with_cache(token.to(model.cfg.device))\n",
    "neuron_act_batch = cache[cache_name]\n",
    "_, act = smaller_auto_encoder(neuron_act_batch)\n",
    "v, i = act[0, -1, :].topk(10)\n",
    "\n",
    "print(\"Activations:\",[round(val,2) for val in v.tolist()])\n",
    "print(\"Feature_ids\", i.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Interp\n",
    "Investigate the example sentences the activate this feature.\n",
    "\n",
    "Max: show max activating (tokens,contexts)\n",
    "\n",
    "Uniform: Show range of activations from each bin (e.g. sample an example from 1-2, 2-3, etc). \n",
    "[Note: if a feature is monosemantic, then the full range of activations should be that feature, not just max-activating ones]\n",
    "\n",
    "Full_text: shows the full text example\n",
    "\n",
    "Text_list: shows up to the most activating example (try w/ max activating on a couple of examples to see)\n",
    "\n",
    "ablate_text: remove the context one token at a time, and show the decrease/increase in activation of that feature\n",
    "\n",
    "ablate_feature_direction: removes feature direction from model's activation mid-inference, showing the logit diff in the output for every token.\n",
    "\n",
    "logit_lens: show the logit lens for that feature. If matches ablate_feature_direction, then the computation path is through the residual stream, else, it's through future layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 5\n",
    "# best_feature = int(max_indices[N])\n",
    "best_feature = 1461\n",
    "print(\"bias:\", smaller_auto_encoder.encoder_bias.detach().cpu().numpy()[best_feature])\n",
    "print(f\"Feature index: {best_feature}\")\n",
    "print(f\"MCS: {max_cosine_similarities[best_feature]}\")\n",
    "text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, dictionary_activations, dataset, setting=\"uniform\")\n",
    "# text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, dictionary_activations, dataset, setting=\"max\")\n",
    "visualize_text(full_text, best_feature, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablate_text(text_list, best_feature, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablate_feature_direction_display(full_text, best_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens(model,best_feature, smaller_dict, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_text = [\n",
    "    \" Your text here\",\n",
    "]\n",
    "visualize_text(custom_text, best_feature, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Centric Viewpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through datapoints & see if the features that activate on them make sense.\n",
    "d_point = 0\n",
    "# text = tokens_dataset[d_point]\n",
    "data_ind, sequence_pos = np.unravel_index(d_point, (datapoints, token_amount))\n",
    "feature_val, feature_ind = dictionary_activations[d_point].topk(10)\n",
    "data_ind = int(data_ind)\n",
    "sequence_pos = int(sequence_pos)\n",
    "full_tok = torch.tensor(dataset[data_ind][\"input_ids\"])\n",
    "full_text = []\n",
    "full_text.append(model.tokenizer.decode(full_tok))\n",
    "visualize_text(full_text, feature_ind, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the neuron/residual basis\n",
    "When we look at the weights of a feature, we are seeing the literal dimensions from the residual stream/neurons being read from the feature. \n",
    "\n",
    "Here I'm visualizing the weight values for the residual stream. If there are outliers, then it's mainly reading from that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check features non-zero weights in decoder\n",
    "# Plot a histogram of the weights\n",
    "max_activation = dictionary_activations[:, best_feature].max()\n",
    "weights = smaller_dict[best_feature]\n",
    "plt.hist(weights, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(weights*max_activation).topk(20), (weights*max_activation).topk(20, largest=False).values, (weights*max_activation > 0.2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepend/Append tokens\n",
    "We can iterate over all tokens to check which ones activate a feature a lot to more rigorously test a hypothesis on what a feature means.\n",
    "\n",
    "Note: I'm literately running the model through all 50k tokens prepended to the text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_all_tokens_and_get_feature_activation(model, minimal_activating_example, feature, setting=\"prepend\"):\n",
    "    tokens = model.to_tokens(minimal_activating_example, prepend_bos=False)\n",
    "\n",
    "    # Run through every number up to vocab size\n",
    "    vocab_size = model.cfg.d_vocab\n",
    "    batch_size = 256*2 # Define your desired batch size\n",
    "\n",
    "    dollar_feature_activations = torch.zeros(vocab_size)\n",
    "    for start in range(0, vocab_size, batch_size):\n",
    "        end = min(start + batch_size, vocab_size)\n",
    "\n",
    "        token_prep = torch.arange(start, end).to(device)\n",
    "        token_prep = token_prep.unsqueeze(1)  # Add a dimension for concatenation\n",
    "\n",
    "        # 1. Prepend to the tokens\n",
    "        if setting == \"prepend\":\n",
    "            tokens_catted = torch.cat((token_prep, tokens.repeat(end - start, 1)), dim=1).long()\n",
    "        elif setting == \"append\":\n",
    "            tokens_catted = torch.cat((tokens.repeat(end - start, 1), token_prep), dim=1).long()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown setting: {setting}\")\n",
    "\n",
    "        # 2. Run through the model\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(tokens_catted.to(device))\n",
    "            neuron_act_batch = cache[cache_name]\n",
    "            _, act = smaller_auto_encoder(neuron_act_batch)\n",
    "\n",
    "        # 3. Get the feature\n",
    "        dollar_feature_activations[start:end] = act[:, -1, feature].cpu().squeeze()\n",
    "\n",
    "    k = 20\n",
    "    k_increasing_val, k_increasing_ind = dollar_feature_activations.topk(k)\n",
    "    k_decreasing_val, k_decreasing_ind = dollar_feature_activations.topk(k, largest=False)\n",
    "    if(setting == \"prepend\"):\n",
    "        print(f\"[token]{minimal_activating_example}\")\n",
    "    elif(setting == \"append\"):\n",
    "        print(f\"{minimal_activating_example}[token]\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown setting: {setting}\")\n",
    "    # Print indices converted to tokens\n",
    "    print(f\"Top-{k} increasing: {model.to_str_tokens(k_increasing_ind)}\")\n",
    "    # Print values\n",
    "    print(f\"Top-{k} increasing: {[f'{val:.2f}' for val in k_increasing_val]}\")\n",
    "    print(f\"Top-{k} decreasing: {model.to_str_tokens(k_decreasing_ind)}\")\n",
    "    print(f\"Top-{k} decreasing: {[f'{val:.2f}' for val in k_decreasing_val]}\")\n",
    "    print(f\"Number of 0 activations: {torch.sum(dollar_feature_activations == 0)}\")\n",
    "    if(setting == \"prepend\"):\n",
    "        best_text = \"\".join(model.to_str_tokens(dollar_feature_activations.argmax()) + [minimal_activating_example])\n",
    "    else:\n",
    "        best_text = \"\".join([minimal_activating_example] + model.to_str_tokens(dollar_feature_activations.argmax()))\n",
    "    return best_text\n",
    "\n",
    "best_text = \"\"\n",
    "for x in range(3):\n",
    "    # best_text = prepend_all_tokens_and_get_feature_activation(model, best_text, best_feature, setting=\"prepend\")\n",
    "    best_text = prepend_all_tokens_and_get_feature_activation(model, best_text, best_feature, setting=\"append\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" for all $\", best_feature, setting=\"prepend\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" tree\", best_feature, setting=\"prepend\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" tree\", best_feature, setting=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_text = \"\"\n",
    "for x in range(3):\n",
    "    best_text = prepend_all_tokens_and_get_feature_activation(model, best_text, best_feature, setting=\"prepend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepend_all_tokens_and_get_feature_activation(model, \" for all $\", best_feature, setting=\"prepend\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \"The\", best_feature, setting=\"append\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" tree\", best_feature, setting=\"append\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
