{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import argparse\n",
    "from utils import dotdict\n",
    "from activation_dataset import setup_token_data\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cfg = dotdict()\n",
    "# models: \"EleutherAI/pythia-6.9b\", \"usvsnsp/pythia-6.9b-ppo\", \"lomahony/eleuther-pythia6.9b-hh-sft\", \"reciprocate/dahoas-gptj-rm-static\"\n",
    "# \"EleutherAI/pythia-70m\", \"lomahony/eleuther-pythia70m-hh-sft\", \"BlueSunflower/Pythia-70M-chess\"\n",
    "cfg.model_name=\"EleutherAI/pythia-70m\"\n",
    "cfg.target_name=\"BlueSunflower/Pythia-70M-chess\"\n",
    "cfg.layers=[0,1,2,3,4,5]\n",
    "cfg.setting=\"residual\"\n",
    "# cfg.tensor_name=\"gpt_neox.layers.{layer}\"\n",
    "cfg.tensor_name=\"gpt_neox.layers.{layer}\" # \"gpt_neox.layers.{layer}\" (pythia), \"transformer.h.{layer}\" (rm)\n",
    "cfg.target_tensor_name=\"gpt_neox.layers.{layer}\"\n",
    "original_l1_alpha = 8e-4\n",
    "cfg.l1_alpha=original_l1_alpha\n",
    "cfg.l1_alphas=[2e-3]\n",
    "# cfg.l1_alphas=[0, 1e-5, 2e-5, 4e-5, 8e-5, 1e-4, 2e-4, 4e-4, 8e-4, 1e-3, 2e-3, 4e-3, 8e-3]\n",
    "cfg.sparsity=None\n",
    "cfg.num_epochs=10\n",
    "cfg.model_batch_size=8 * 8\n",
    "cfg.lr=1e-3\n",
    "cfg.kl=False\n",
    "cfg.reconstruction=False\n",
    "# cfg.dataset_name=\"NeelNanda/pile-10k\", \"BlueSunflower/ChessGames\", \"Elriggs/openwebtext-100k\"\n",
    "cfg.dataset_name=\"BlueSunflower/ChessGames\"\n",
    "cfg.device=\"cuda:0\"\n",
    "cfg.ratio = 4\n",
    "cfg.seed = 0\n",
    "cfg.max_length = 256\n",
    "# cfg.device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = [cfg.tensor_name.format(layer=layer) for layer in cfg.layers]\n",
    "target_tensor_names = [cfg.target_tensor_name.format(layer=layer) for layer in cfg.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.model_name)\n",
    "model = model.to(cfg.device)\n",
    "target_model = AutoModelForCausalLM.from_pretrained(cfg.target_name).to(cfg.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# TODO iteratively grab dataset?\n",
    "token_loader = setup_token_data(cfg, tokenizer, model, seed=cfg.seed)\n",
    "num_tokens = cfg.max_length*cfg.model_batch_size*len(token_loader)\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1 datapoint on model to get the activation size\n",
    "from baukit import Trace, TraceDict\n",
    "\n",
    "text = \"1\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "# Your activation name will be different. In the next cells, we will show you how to find it.\n",
    "with torch.no_grad():\n",
    "    with Trace(model, tensor_names[0]) as ret:\n",
    "        _ = model(tokens)\n",
    "        representation = ret.output\n",
    "        # check if instance tuple\n",
    "        if(isinstance(representation, tuple)):\n",
    "            representation = representation[0]\n",
    "        activation_size = representation.shape[-1]\n",
    "print(f\"Activation size: {activation_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target sparsity to 10% of activation_size if not set\n",
    "if cfg.sparsity is None:\n",
    "    cfg.sparsity = int(activation_size*0.05)\n",
    "    print(f\"Target sparsity: {cfg.sparsity}\")\n",
    "\n",
    "target_lower_sparsity = cfg.sparsity * 0.9\n",
    "target_upper_sparsity = cfg.sparsity * 1.1\n",
    "adjustment_factor = 0.1  # You can set this to whatever you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def autoencoder_name_from_llm_name(llm_name):\n",
    "#     if llm_name == \"EleutherAI/pythia-6.9b\":\n",
    "#         return \"base_sae_6b\"\n",
    "#     if llm_name == \"EleutherAI/pythia-70m\":\n",
    "#         return \"base_sae_70m\"\n",
    "#     if llm_name == \"lomahony/eleuther-pythia6.9b-hh-sft\":\n",
    "#         return \"sft_sae_6b\"\n",
    "#     if llm_name == \"lomahony/eleuther-pythia70m-hh-sft\":\n",
    "#         return \"sft_sae_70m\"\n",
    "#     if llm_name == \"usvsnsp/pythia-6.9b-ppo\":\n",
    "#         return \"ppo_sae_6b\"\n",
    "#     if llm_name == \"reciprocate/dahoas-gptj-rm-static\":\n",
    "#         return \"rm_sae_gptj\"\n",
    "#     return \"Error\"\n",
    "\n",
    "def autoencoder_name_from_llm_name(llm_name, layer, l1_alpha):\n",
    "    # model_save_name = llm_name.split(\"/\")[-1]\n",
    "    return f\"base_sae_70m_{layer}_{l1_alpha}\"\n",
    "    # return f\"{model_save_name}_{layer}_{l1_alpha}\"\n",
    "    \n",
    "def autoTED_name_from_llm_name(llm_name, layer, l1_alpha):\n",
    "    # if llm_name == \"EleutherAI/pythia-6.9b\":\n",
    "    #     return \"base_autoTED_6b\"\n",
    "    if llm_name == \"EleutherAI/pythia-70m\":\n",
    "        return f\"base_autoTED_70m/base_autoTED_70m_{layer}_{l1_alpha}\"\n",
    "    # if llm_name == \"lomahony/eleuther-pythia6.9b-hh-sft\":\n",
    "    #     return \"sft_autoTED_6b\"\n",
    "    # if llm_name == \"lomahony/eleuther-pythia70m-hh-sft\":\n",
    "    #     return \"sft_autoTED_70m\"\n",
    "    # if llm_name == \"usvsnsp/pythia-6.9b-ppo\":\n",
    "    #     return \"ppo_autoTED_6b\"\n",
    "    # if llm_name == \"reciprocate/dahoas-gptj-rm-static\":\n",
    "    #     return \"rm_autoTED_gptj\"\n",
    "    return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load autoencoder\n",
    "from autoencoders.learned_dict import TiedSAE, UntiedSAE, AnthropicSAE, TransferSAE\n",
    "from torch import nn\n",
    "\n",
    "model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "print(f\"Loading autoencoder from model {model_save_name}\")\n",
    "\n",
    "\n",
    "\n",
    "autoencoders = []\n",
    "for layer in cfg.layers:\n",
    "    l1_variants = []\n",
    "    \n",
    "    for l1 in cfg.l1_alphas:\n",
    "        save_name = autoTED_name_from_llm_name(cfg.model_name, layer, l1)\n",
    "        autoencoder = torch.load(f\"trained_models/{save_name}.pt\")\n",
    "        autoencoder.to_device(cfg.device)\n",
    "        l1_variants.append(autoencoder)\n",
    "        \n",
    "    autoencoders.append(l1_variants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize New transfer autoencoder\n",
    "from autoencoders.learned_dict import TiedSAE, UntiedSAE, AnthropicSAE, TransferSAE\n",
    "from torch import nn\n",
    "\n",
    "modes = [\"scale\", \"rotation\", \"bias\", \"free\"]\n",
    "transfer_autoencoders = []\n",
    "optimizers = []\n",
    "for layer in cfg.layers:\n",
    "    l1_variants = []\n",
    "    l1_optimizers = []\n",
    "    \n",
    "    for l1 in range(len(cfg.l1_alphas)):\n",
    "        autoencoder = autoencoders[layer][l1]\n",
    "        \n",
    "        mode_tsaes = []\n",
    "        mode_opts = []\n",
    "        for mode in modes:\n",
    "            mode_tsae = TransferSAE(\n",
    "                # n_feats = n_dict_components, \n",
    "                # activation_size=activation_size,\n",
    "                autoencoder,\n",
    "                decoder=autoencoder.get_learned_dict().detach().clone(),\n",
    "                decoder_bias=autoencoder.shift_bias.detach().clone(),\n",
    "                scale=autoencoder.scale.detach().clone(),\n",
    "                mode=\"free\",\n",
    "            )\n",
    "            mode_tsae.set_grad()\n",
    "            mode_tsaes.append(mode_tsae)\n",
    "            mode_opts.append(\n",
    "                torch.optim.Adam(mode_tsae.parameters(), lr=cfg.lr)\n",
    "            )\n",
    "        \n",
    "        l1_variants.append(mode_tsaes)\n",
    "        l1_optimizers.append(mode_opts)\n",
    "    transfer_autoencoders.append(l1_variants)\n",
    "    optimizers.append(l1_optimizers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb setup\n",
    "secrets = json.load(open(\"secrets.json\"))\n",
    "wandb.login(key=secrets[\"wandb_key\"])\n",
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "wandb_run_name = f\"{cfg.model_name}_transfer_{start_time[4:]}_{cfg.sparsity}\"  # trim year\n",
    "print(f\"wandb_run_name: {wandb_run_name}\")\n",
    "wandb.init(project=\"sparse coding\", config=dict(cfg), name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(autoencoder, base_activation, target_activation, transfer_autoencoders, optimizers, step_number, dead_features, log_every=100, log_prefix=\"\", indiv_prefixes = None):\n",
    "    i = step_number\n",
    "    c = autoencoder.encode(base_activation.to(cfg.device))\n",
    "    x_hat = autoencoder.decode(c)\n",
    "    \n",
    "    autoencoder_loss = (x_hat - target_activation.to(cfg.device)).pow(2).mean()\n",
    "    dead_features += c.sum(dim=0).cpu()\n",
    "    \n",
    "    wandb_log = {}\n",
    "    \n",
    "    if not isinstance(transfer_autoencoders, list):\n",
    "        transfer_autoencoders = [transfer_autoencoders]\n",
    "        optimizers = [optimizers]\n",
    "        \n",
    "    if not isinstance(indiv_prefixes, list):\n",
    "        indiv_prefixes = [indiv_prefixes]\n",
    "    \n",
    "    for tsae, optimizer, indiv_prefix in zip(transfer_autoencoders, optimizers, indiv_prefixes):\n",
    "        x_hat = tsae.decode(c)\n",
    "        \n",
    "        reconstruction_loss = (x_hat - target_activation.to(cfg.device)).pow(2).mean()\n",
    "        total_loss = reconstruction_loss # NO L1 LOSS\n",
    "\n",
    "        if (i % log_every == 0): # Check here so first check is model w/o change\n",
    "            # self_similarity = torch.cosine_similarity(tsae.decoder, last_decoders[mode], dim=-1).mean().cpu().item()\n",
    "            # last_decoders[mode] = tsae.decoder.clone().detach()\n",
    "            num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "            with torch.no_grad():\n",
    "                sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            # print(f\"{log_prefix}Reconstruction Loss: {reconstruction_loss:.2f} | Tokens: {num_tokens_so_far}\") # | Self Similarity: {self_similarity:.2f}\")\n",
    "            wandb_log.update({\n",
    "                f'{log_prefix}{indiv_prefix}Reconstruction Loss': reconstruction_loss.item(),\n",
    "                # f'{log_prefix}{mode} Self Similarity': self_similarity\n",
    "            })\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (i % log_every == 0):\n",
    "        with torch.no_grad():\n",
    "            sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            # Count number of dead_features are zero\n",
    "            num_dead_features = (dead_features == 0).sum().item()\n",
    "        # print(f\"{log_prefix}Sparsity: {sparsity:.1f} | Dead Features: {num_dead_features} | Reconstruction Loss: {autoencoder_loss:.2f} | Tokens: {num_tokens_so_far}\")\n",
    "        # dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "        wandb_log.update({\n",
    "                f'{log_prefix}SAE Sparsity': sparsity,\n",
    "                f'{log_prefix}Dead Features': num_dead_features,\n",
    "                f'{log_prefix}SAE Reconstruction Loss': autoencoder_loss.item(),\n",
    "                f'{log_prefix}Tokens': num_tokens_so_far,\n",
    "            })\n",
    "        \n",
    "        # wandb.log(wandb_log)\n",
    "    \n",
    "    return wandb_log, dead_features #, last_decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transfer autoencoder\n",
    "token_loader = setup_token_data(cfg, tokenizer, model, seed=cfg.seed)\n",
    "dead_features = [[torch.zeros(autoencoder.encoder.shape[0])\n",
    "                         for l1 in cfg.l1_alphas]\n",
    "                         for layer in range(len(tensor_names))]\n",
    "# auto_dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "\n",
    "max_num_tokens = 100_000_000\n",
    "log_every=100\n",
    "# Freeze model parameters \n",
    "model = model.to(cfg.device)\n",
    "model.eval()\n",
    "target_model = target_model.to(cfg.device)\n",
    "target_model.eval()\n",
    "\n",
    "\n",
    "# last_decoders = dict([(modes[i],transfer_autoencoders[i].decoder.clone().detach()) for i in range(len(transfer_autoencoders))])\n",
    "model_on_gpu = True\n",
    "\n",
    "saved_inputs = []\n",
    "i = 0 # counts all optimization steps\n",
    "num_saved_so_far = 0\n",
    "print(\"starting loop\")\n",
    "# for (base_activation, target_activation) in tqdm(generate_activations(model, token_loader, cfg, model_on_gpu=model_on_gpu, num_batches=500), \n",
    "#                                                  total=int(max_num_tokens/(cfg.max_length*cfg.model_batch_size))):\n",
    "for i, batch in enumerate(tqdm(token_loader,total=int(max_num_tokens/(cfg.max_length*cfg.model_batch_size)))):\n",
    "    tokens = batch[\"input_ids\"].to(cfg.device)\n",
    "    with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model       \n",
    "        base_activations = []\n",
    "        with TraceDict(model, tensor_names) as ret:\n",
    "            _ = model(tokens)\n",
    "            s = ret[tensor_names[0]]\n",
    "            for layer_name in tensor_names:\n",
    "                representation = ret[layer_name].output[0]\n",
    "                flattened_rep = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "                base_activations.append(flattened_rep)\n",
    "                \n",
    "        target_activations = []\n",
    "        with TraceDict(target_model, tensor_names) as ret:\n",
    "            _ = target_model(tokens)\n",
    "            for layer_name in tensor_names:\n",
    "                representation = ret[layer_name].output[0]\n",
    "                flattened_rep = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "                target_activations.append(flattened_rep)\n",
    "    \n",
    "    wandb_log = {}\n",
    "    for layer in range(len(cfg.layers)):\n",
    "        for l1_id in range(len(cfg.l1_alphas)):\n",
    "            indiv_prefixes = [f\"{layer} {cfg.l1_alphas[l1_id]} {mode} \" for mode in modes]\n",
    "            wandb_logging, dead_features[layer][l1_id] = training_step(autoencoders[layer][l1_id], base_activations[layer], target_activations[layer], \n",
    "                                                                        transfer_autoencoders[layer][l1_id], optimizers[layer][l1_id], i, \n",
    "                                                                        dead_features[layer][l1_id], log_every, log_prefix=f\"{layer} {cfg.l1_alphas[l1_id]} \",\n",
    "                                                                        indiv_prefixes=indiv_prefixes)\n",
    "            wandb_log.update(wandb_logging)\n",
    "    \n",
    "    if len(wandb_log) > 0:\n",
    "        wandb.log(wandb_log)\n",
    "        pass\n",
    "        \n",
    "    i+=1\n",
    "    \n",
    "    if ((i+2) % 2000==0): # save periodically but before big changes\n",
    "        for layer in range(len(cfg.layers)):\n",
    "            for l1_id in range(len(cfg.l1_alphas)):\n",
    "                for mode_id in range(len(modes)):\n",
    "                    mode = modes[mode_id]\n",
    "                    l1_alpha = cfg.l1_alphas[l1_id]\n",
    "                    model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "                    save_name = f\"transfer_base_chess_70m_{mode}_{layer}_{l1_alpha}_ckpt{num_saved_so_far}\" \n",
    "\n",
    "                    # Make directory trained_models if it doesn't exist\n",
    "                    import os\n",
    "                    if not os.path.exists(\"trained_models\"):\n",
    "                        os.makedirs(\"trained_models\")\n",
    "                    # Save model\n",
    "                    torch.save(transfer_autoencoders[layer][l1_id][mode_id], f\"trained_models/{save_name}.pt\")\n",
    "                    # torch.save(dead_features[layer][l1_id], f\"trained_models/dead_features_{model_save_name}_{layer}_{l1_alpha}.pt\")\n",
    "        \n",
    "        num_saved_so_far += 1\n",
    "                \n",
    "    \n",
    "    num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "    if(num_tokens_so_far > max_num_tokens):\n",
    "        print(f\"Reached max number of tokens: {max_num_tokens}\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model at end\n",
    "    \n",
    "for layer in range(len(cfg.layers)):\n",
    "    for l1_id in range(len(cfg.l1_alphas)):\n",
    "        for mode_id in range(len(modes)):\n",
    "            mode = modes[mode_id]\n",
    "            l1_alpha = cfg.l1_alphas[l1_id]\n",
    "            model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "            save_name = f\"transfer_base_chess_70m_{mode}_{layer}_{l1_alpha}\" \n",
    "\n",
    "            # Make directory trained_models if it doesn't exist\n",
    "            import os\n",
    "            if not os.path.exists(\"trained_models\"):\n",
    "                os.makedirs(\"trained_models\")\n",
    "            # Save model\n",
    "            torch.save(transfer_autoencoders[layer][l1_id][mode_id], f\"trained_models/{save_name}.pt\")\n",
    "            # torch.save(dead_features[layer][l1_id], f\"trained_models/dead_features_{model_save_name}_{layer}_{l1_alpha}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
