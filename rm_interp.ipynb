{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/logan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00, 10.31s/it]\n",
      "Some weights of the model checkpoint at reciprocate/dahoas-gptj-rm-static were not used when initializing GPTJForSequenceClassification: ['transformer.h.22.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.7.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.27.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.0.attn.bias', 'transformer.h.12.attn.bias', 'transformer.h.16.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.18.attn.bias', 'transformer.h.10.attn.bias', 'transformer.h.23.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.6.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.2.attn.bias', 'transformer.h.17.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.9.attn.bias', 'transformer.h.15.attn.bias', 'transformer.h.26.attn.bias', 'transformer.h.14.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"reciprocate/dahoas-gptj-rm-static\"\n",
    "# rm = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "rm = AutoModelForSequenceClassification.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# turn off gradients\n",
    "for param in rm.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3036]], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run some example text through the model\n",
    "text = \"I like to eat ice cream.\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "reward = rm(input_ids).logits\n",
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtyping import TensorType\n",
    "from torch import nn\n",
    "class TiedSAE(nn.Module):\n",
    "    def __init__(self, activation_size, n_dict_components):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Parameter(torch.empty((n_dict_components, activation_size)))\n",
    "        nn.init.xavier_uniform_(self.encoder)\n",
    "        self.encoder_bias = nn.Parameter(torch.zeros((n_dict_components,)))\n",
    "\n",
    "    def get_learned_dict(self):\n",
    "        norms = torch.norm(self.encoder, 2, dim=-1)\n",
    "        return self.encoder / torch.clamp(norms, 1e-8)[:, None]\n",
    "\n",
    "    def encode(self, batch):\n",
    "        c = torch.einsum(\"nd,bd->bn\", self.encoder, batch)\n",
    "        c = c + self.encoder_bias\n",
    "        c = torch.clamp(c, min=0.0)\n",
    "        return c\n",
    "\n",
    "    def decode(self, code: TensorType[\"_batch_size\", \"_n_dict_components\"]) -> TensorType[\"_batch_size\", \"_activation_size\"]:\n",
    "        learned_dict = self.get_learned_dict()\n",
    "        x_hat = torch.einsum(\"nd,bn->bd\", learned_dict, code)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, batch: TensorType[\"_batch_size\", \"_activation_size\"]) -> TensorType[\"_batch_size\", \"_activation_size\"]:\n",
    "        c = self.encode(batch)\n",
    "        x_hat = self.decode(c)\n",
    "        return x_hat, c\n",
    "\n",
    "    def n_dict_components(self):\n",
    "        return self.get_learned_dict().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autoencoders import *\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "layer = 15\n",
    "rm_sae_repo_id = \"Elriggs/dahoas-gptj-rm-sae\"\n",
    "rm_sae_filename = f\"dahoas-gptj-rm-static_r4_transformer.h.{layer}.pt\"\n",
    "ae_download_location = hf_hub_download(repo_id=rm_sae_repo_id, filename=rm_sae_filename)\n",
    "output_cache_name = f\"transformer.h.{layer}\"\n",
    "autoencoder = torch.load(ae_download_location).to(device)\n",
    "for param in autoencoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset openwebtext-10k (/root/.cache/huggingface/datasets/stas___openwebtext-10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/stas___openwebtext-10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b/cache-f990913222bd2a7b_*_of_00008.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 0.33M\n"
     ]
    }
   ],
   "source": [
    "from activation_dataset import chunk_and_tokenize\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Download the dataset\n",
    "# This formats it, so every datapoint is max_length tokens long\n",
    "dataset_name=\"stas/openwebtext-10k\"\n",
    "max_seq_length=32\n",
    "dataset = load_dataset(dataset_name, split=\"train[:300]\")\n",
    "dataset, _ = chunk_and_tokenize(dataset, tokenizer, max_length=max_seq_length)\n",
    "max_tokens = dataset.num_rows*max_seq_length\n",
    "print(f\"Number of tokens: {max_tokens/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get reward & Counterfactual Reward (when ablating each feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:03<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "from baukit import Trace\n",
    "from functools import partial\n",
    "\n",
    "#TODO: This needs to change to ablate a specific feature. So subtract the feature out of the activation\n",
    "def less_than_rank_1_ablate(value, layer_name, autoencoder, feature):\n",
    "    if(isinstance(value, tuple)):\n",
    "        second_value = value[1]\n",
    "        internal_activation = value[0]\n",
    "    else:\n",
    "        internal_activation = value\n",
    "    # Only ablate the feature direction up to the negative bias\n",
    "    # ie Only subtract when it activates above that negative bias.\n",
    "\n",
    "    # Rearrange to fit autoencoder\n",
    "    int_val = rearrange(internal_activation, 'b s h -> (b s) h')\n",
    "    # Run through the autoencoder\n",
    "    act = autoencoder.encode(int_val)\n",
    "    dictionary_for_this_autoencoder = autoencoder.get_learned_dict()\n",
    "    feature_direction = torch.outer(act[:, feature].squeeze(), dictionary_for_this_autoencoder[feature].squeeze())\n",
    "    batch, seq_len, hidden_size = internal_activation.shape\n",
    "    feature_direction = rearrange(feature_direction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "    internal_activation -= feature_direction\n",
    "    if(isinstance(value, tuple)):\n",
    "        return_value = (internal_activation, second_value)\n",
    "    else:\n",
    "        return_value = internal_activation\n",
    "    return return_value\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "batch_size = 32\n",
    "num_datapoints = dataset.num_rows\n",
    "num_features, d_model = autoencoder.encoder.shape\n",
    "original_reward = torch.zeros(num_datapoints)\n",
    "ablated_reward = torch.zeros((num_datapoints, num_features))\n",
    "diff_reward = torch.zeros((num_datapoints, num_features))\n",
    "with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "    dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "    for i, batch in enumerate(tqdm(dl)):\n",
    "        # Get original reward\n",
    "        batch = batch.to(device)\n",
    "        logit1 = rm(batch).logits[:, 0]\n",
    "        original_reward[i*batch_size:(i+1)*batch_size] = rm(batch).logits[:, 0].cpu()\n",
    "        # Get ablated reward\n",
    "        intervention_function = partial(less_than_rank_1_ablate,  autoencoder=autoencoder, feature=0)\n",
    "        with Trace(rm, output_cache_name, edit_output=intervention_function) as _:\n",
    "            ablated_reward[i*batch_size:(i+1)*batch_size, 0] = rm(batch).logits[:, 0].cpu()\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_feature(activations, layer_name, autoencoder, feature_to_ablate):\n",
    "        # return activations\n",
    "    # Check if tuple ie residual layer output as opposed to e.g. mlp output\n",
    "    if isinstance(activations, tuple):\n",
    "        temp_activations = activations[1]\n",
    "        to_edit_activations = activations[0]\n",
    "    else:\n",
    "        to_edit_activations = activations\n",
    "\n",
    "    b, s, n = to_edit_activations.shape\n",
    "    mlp_flattened = rearrange(to_edit_activations, \"b s n -> (b s) n\")\n",
    "    reconstruction_flattened, _ = autoencoder(mlp_flattened)\n",
    "    reconstruction = rearrange(reconstruction_flattened, \"(b s) n -> b s n\", b=b, s=s)\n",
    "\n",
    "    if isinstance(activations, tuple):\n",
    "        reconstruction = tuple([reconstruction, temp_activations])\n",
    "    return reconstruction\n",
    "\n",
    "intervention_function = partial(less_than_rank_1_ablate,  autoencoder=autoencoder, feature = 0)\n",
    "\n",
    "with Trace(rm, output_cache_name, edit_output=intervention_function) as _:\n",
    "    rm(batch).logits[:, 0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def less_than_rank_1_ablate(value, layer_name, autoencoder, feature):\n",
    "    if(isinstance(value, tuple)):\n",
    "        second_value = value[1]\n",
    "        internal_activation = value[0]\n",
    "    else:\n",
    "        internal_activation = value\n",
    "    # Only ablate the feature direction up to the negative bias\n",
    "    # ie Only subtract when it activates above that negative bias.\n",
    "\n",
    "    # Rearrange to fit autoencoder\n",
    "    int_val = rearrange(internal_activation, 'b s h -> (b s) h')\n",
    "    # Run through the autoencoder\n",
    "    act = autoencoder.encode(int_val)\n",
    "    dictionary_for_this_autoencoder = autoencoder.get_learned_dict()\n",
    "    feature_direction = torch.outer(act[:, feature].squeeze(), dictionary_for_this_autoencoder[feature].squeeze())\n",
    "    batch, seq_len, hidden_size = internal_activation.shape\n",
    "    feature_direction = rearrange(feature_direction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "    internal_activation -= feature_direction\n",
    "    if(isinstance(value, tuple)):\n",
    "        return_value = (internal_activation, second_value)\n",
    "    else:\n",
    "        return_value = internal_activation\n",
    "    return return_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
