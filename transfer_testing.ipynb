{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import argparse\n",
    "from utils import dotdict\n",
    "from activation_dataset import setup_token_data\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cfg = dotdict()\n",
    "# models: \"EleutherAI/pythia-6.9b\", \"usvsnsp/pythia-6.9b-ppo\", \"lomahony/eleuther-pythia6.9b-hh-sft\", \"reciprocate/dahoas-gptj-rm-static\"\n",
    "# \"EleutherAI/pythia-70m\", \"lomahony/eleuther-pythia70m-hh-sft\"\n",
    "cfg.target_name=\"EleutherAI/pythia-70m\"\n",
    "cfg.model_name=\"lomahony/eleuther-pythia70m-hh-sft\"\n",
    "cfg.layers=[4]\n",
    "cfg.setting=\"residual\"\n",
    "# cfg.tensor_name=\"gpt_neox.layers.{layer}\" or \"transformer.h.{layer}\"\n",
    "cfg.tensor_name=\"gpt_neox.layers.{layer}\"\n",
    "cfg.target_tensor_name=\"gpt_neox.layers.{layer}\"\n",
    "original_l1_alpha = 8e-4\n",
    "cfg.l1_alpha=original_l1_alpha\n",
    "cfg.sparsity=None\n",
    "cfg.num_epochs=10\n",
    "cfg.model_batch_size=8\n",
    "cfg.lr=1e-3\n",
    "cfg.kl=False\n",
    "cfg.reconstruction=False\n",
    "# cfg.dataset_name=\"NeelNanda/pile-10k\"\n",
    "cfg.dataset_name=\"Elriggs/openwebtext-100k\"\n",
    "cfg.device=\"cuda:0\"\n",
    "cfg.ratio = 4\n",
    "cfg.seed = 1\n",
    "# cfg.device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = [cfg.tensor_name.format(layer=layer) for layer in cfg.layers]\n",
    "target_tensor_names = [cfg.target_tensor_name.format(layer=layer) for layer in cfg.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.model_name)\n",
    "model = model.to(cfg.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "cfg.max_length = 256\n",
    "token_loader = setup_token_data(cfg, tokenizer, model, seed=cfg.seed, split=\"train\")\n",
    "num_tokens = cfg.max_length*cfg.model_batch_size*len(token_loader)\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1 datapoint on model to get the activation size\n",
    "from baukit import Trace\n",
    "\n",
    "text = \"1\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "# Your activation name will be different. In the next cells, we will show you how to find it.\n",
    "with torch.no_grad():\n",
    "    with Trace(model, tensor_names[0]) as ret:\n",
    "        _ = model(tokens)\n",
    "        representation = ret.output\n",
    "        # check if instance tuple\n",
    "        if(isinstance(representation, tuple)):\n",
    "            representation = representation[0]\n",
    "        activation_size = representation.shape[-1]\n",
    "print(f\"Activation size: {activation_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target sparsity to 10% of activation_size if not set\n",
    "if cfg.sparsity is None:\n",
    "    cfg.sparsity = int(activation_size*0.05)\n",
    "    print(f\"Target sparsity: {cfg.sparsity}\")\n",
    "\n",
    "target_lower_sparsity = cfg.sparsity * 0.9\n",
    "target_upper_sparsity = cfg.sparsity * 1.1\n",
    "adjustment_factor = 0.1  # You can set this to whatever you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base and target autoencoders\n",
    "from autoencoders.learned_dict import TiedSAE, UntiedSAE, AnthropicSAE, TransferSAE\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "target_model = AutoModelForCausalLM.from_pretrained(cfg.target_name).cpu()\n",
    "\n",
    "save_name = f\"sft_autoTED_70m\"  # trim year\n",
    "autoencoder = torch.load(f\"trained_models/{save_name}.pt\")\n",
    "print(f\"autoencoder loaded from f{save_name}\")\n",
    "autoencoder.to_device(cfg.device)\n",
    "\n",
    "save_name = f\"base_autoTED_70m\" \n",
    "target_autoencoder = torch.load(f\"trained_models/{save_name}.pt\")\n",
    "print(f\"target_autoencoder loaded from f{save_name}\")\n",
    "target_autoencoder.to_device(cfg.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize New transfer autoencoder\n",
    "from autoencoders.learned_dict import TiedSAE, UntiedSAE, AnthropicSAE, TransferSAE\n",
    "from torch import nn\n",
    "\n",
    "modes = [\"scale\", \"rotation\", \"bias\", \"free\"]\n",
    "transfer_autoencoders = []\n",
    "for mode in modes:\n",
    "    # mode_tsae = TransferSAE(\n",
    "    #     # n_feats = n_dict_components, \n",
    "    #     # activation_size=activation_size,\n",
    "    #     autoencoder,\n",
    "    #     decoder=autoencoder.get_learned_dict().detach().clone(),\n",
    "    #     decoder_bias=autoencoder.shift_bias.detach().clone(),\n",
    "    #     mode=mode,\n",
    "    # )\n",
    "    mode_tsae = torch.load(f\"/root/sparse_coding/trained_models/eleuther-pythia70m-hh-sft_pythia-70m_{mode}_r4_gpt_neox.layers.4.pt\") #trained_models/transfer_base_sft_6b_{mode}_6.pt\n",
    "    mode_tsae.to_device(cfg.device)\n",
    "    transfer_autoencoders.append(mode_tsae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb setup\n",
    "secrets = json.load(open(\"secrets.json\"))\n",
    "wandb.login(key=secrets[\"wandb_key\"])\n",
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "wandb_run_name = f\"testing_{cfg.model_name}_{cfg.target_name}_{start_time[4:]}\"  # trim year\n",
    "print(f\"wandb_run_name: {wandb_run_name}\")\n",
    "wandb.init(project=\"sparse coding\", config=dict(cfg), name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_activations(model, inputs, layer_name):\n",
    "    acts = []\n",
    "    for tokens in inputs:\n",
    "        with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model\n",
    "            with Trace(model, layer_name) as ret:\n",
    "                _ = model(tokens)\n",
    "                representation = ret.output\n",
    "                if(isinstance(representation, tuple)):\n",
    "                    representation = representation[0]\n",
    "        layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "        acts.append(layer_activations.cpu())\n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_activations(model, target_model, token_loader, cfg, model_on_gpu=True, num_batches=500):\n",
    "    saved_inputs = []\n",
    "    for k, (batch) in enumerate(token_loader):\n",
    "        saved_inputs.append(batch[\"input_ids\"].to(cfg.device))\n",
    "        \n",
    "        if (k+1)%num_batches==0:\n",
    "            # compute base and target model activations\n",
    "            if model_on_gpu:\n",
    "                base_activations = compute_activations(model, saved_inputs, layer_name=tensor_names[0])\n",
    "                model = model.cpu()\n",
    "                target_model = target_model.to(cfg.device)\n",
    "            target_activations = compute_activations(target_model, saved_inputs, layer_name=target_tensor_names[0])\n",
    "            if not model_on_gpu:\n",
    "                target_model = target_model.cpu()\n",
    "                model = model.to(cfg.device)\n",
    "                base_activations = compute_activations(model, saved_inputs, layer_name=tensor_names[0])\n",
    "            model_on_gpu = not model_on_gpu\n",
    "            \n",
    "            for base_activation, target_activation in zip(base_activations, target_activations):\n",
    "                yield base_activation, target_activation\n",
    "\n",
    "            # wipe saved inputs\n",
    "            saved_inputs = []\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing transfer autoencoders\n",
    "token_loader = setup_token_data(cfg, tokenizer, model, seed=cfg.seed, split=\"train\")\n",
    "dead_features = torch.zeros(autoencoder.encoder.shape[0]) # dead features of base SAE on base\n",
    "target_dead_features = torch.zeros(autoencoder.encoder.shape[0]) # dead features of target SAE on base\n",
    "sft_dead_features = torch.zeros(autoencoder.encoder.shape[0]) # dead features of target SAE on target\n",
    "\n",
    "max_num_tokens = 10_000_000\n",
    "log_every=100\n",
    "# Freeze model parameters \n",
    "target_model = target_model.cpu()\n",
    "target_model.eval()\n",
    "model = model.to(cfg.device)\n",
    "model.eval()\n",
    "\n",
    "target_model.requires_grad_(False)\n",
    "model.requires_grad_(False)\n",
    "\n",
    "last_decoders = dict([(modes[i],transfer_autoencoders[i].decoder.clone().detach()) for i in range(len(transfer_autoencoders))])\n",
    "model_on_gpu = True\n",
    "\n",
    "saved_inputs = []\n",
    "i = 0 # counts all optimization steps\n",
    "num_saved_so_far = 0\n",
    "print(\"starting loop\")\n",
    "\n",
    "auto_total_loss = 0\n",
    "auto_base_loss = 0\n",
    "auto_sft_loss = 0\n",
    "\n",
    "target_base_loss = 0\n",
    "target_sft_loss = 0\n",
    "\n",
    "target_total_loss = 0\n",
    "total_losses = dict((mode,0) for mode in modes)\n",
    "\n",
    "for (base_activation, target_activation) in tqdm(generate_activations(model, target_model, token_loader, cfg, model_on_gpu=model_on_gpu, num_batches=500),\n",
    "                                                 total=int(max_num_tokens/(cfg.max_length*cfg.model_batch_size))):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        c = autoencoder.encode(base_activation.to(cfg.device))\n",
    "        x_hat = autoencoder.decode(c)\n",
    "        autoencoder_loss = (x_hat - target_activation.to(cfg.device)).pow(2).mean()\n",
    "        auto_total_loss += autoencoder_loss\n",
    "        auto_base_loss += (x_hat - base_activation.to(cfg.device)).pow(2).mean()\n",
    "        c_sft = autoencoder.encode(target_activation.to(cfg.device))\n",
    "        auto_sft_loss += (autoencoder.decode(c_sft) - target_activation.to(cfg.device)).pow(2).mean()\n",
    "        dead_features += c.sum(dim=0).cpu()\n",
    "        \n",
    "        \n",
    "        target_c = target_autoencoder.encode(base_activation.to(cfg.device))\n",
    "        target_x_hat = target_autoencoder.decode(c)\n",
    "        target_autoencoder_loss = (target_x_hat - target_activation.to(cfg.device)).pow(2).mean()\n",
    "        target_total_loss += target_autoencoder_loss\n",
    "        target_base_loss += (target_x_hat - base_activation.to(cfg.device)).pow(2).mean()\n",
    "        target_c_sft = target_autoencoder.encode(target_activation.to(cfg.device))\n",
    "        target_sft_loss += (target_autoencoder.decode(target_c_sft) - target_activation.to(cfg.device)).pow(2).mean()\n",
    "        target_dead_features += target_c.sum(dim=0).cpu()\n",
    "        sft_dead_features += target_c_sft.sum(dim=0).cpu()\n",
    "    \n",
    "    wandb_log = {}\n",
    "    \n",
    "    for tsae, mode in zip(transfer_autoencoders, modes):\n",
    "        with torch.no_grad():\n",
    "            x_hat = tsae.decode(c)\n",
    "        \n",
    "        reconstruction_loss = (x_hat - target_activation.to(cfg.device)).pow(2).mean()\n",
    "        total_loss = reconstruction_loss # NO L1 LOSS\n",
    "        total_losses[mode] += total_loss\n",
    "\n",
    "        if (i % log_every == 0): # Check here so first check is model w/o change\n",
    "            self_similarity = torch.cosine_similarity(tsae.decoder, last_decoders[mode], dim=-1).mean().cpu().item()\n",
    "            last_decoders[mode] = tsae.decoder.clone().detach()\n",
    "            num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "            with torch.no_grad():\n",
    "                sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            print(f\"Reconstruction Loss: {reconstruction_loss:.2f} | Tokens: {num_tokens_so_far} | Self Similarity: {self_similarity:.2f}\")\n",
    "            wandb_log.update({\n",
    "                f'{mode} Reconstruction Loss': reconstruction_loss.item(),\n",
    "                f'{mode} Self Similarity': self_similarity\n",
    "            })\n",
    "\n",
    "    if (i % log_every == 0):\n",
    "        with torch.no_grad():\n",
    "            sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            num_dead_features = (dead_features == 0).sum().item()\n",
    "            \n",
    "            target_sparsity = (target_c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            target_num_dead_features = (target_dead_features == 0).sum().item()\n",
    "            \n",
    "        print(f\"Sparsity: {sparsity:.1f} | Dead Features: {num_dead_features} | Reconstruction Loss: {autoencoder_loss:.2f} | Tokens: {num_tokens_so_far}\")\n",
    "        \n",
    "        wandb_log.update({  # Base SAE log\n",
    "                f'SAE Sparsity': sparsity,\n",
    "                f'Dead Features': num_dead_features,\n",
    "                f'SAE Reconstruction Loss': autoencoder_loss.item(),\n",
    "                f'Tokens': num_tokens_so_far,\n",
    "            })\n",
    "        \n",
    "        wandb_log.update({  # Target SAE log\n",
    "                f'Target SAE Sparsity': target_sparsity,\n",
    "                f'Target Dead Features': target_num_dead_features,\n",
    "                f'Target SAE Reconstruction Loss': target_autoencoder_loss.item(),\n",
    "            })\n",
    "        \n",
    "        # Non transfer statistics (only base, or only sft)\n",
    "        with torch.no_grad():\n",
    "            sft_sparsity = (c_sft != 0).float().mean(dim=0).sum().cpu().item()            \n",
    "            target_sft_sparsity = (target_c_sft != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            num_sft_dead_features = (sft_dead_features == 0).sum().item()\n",
    "            \n",
    "        wandb_log.update({  # Base only and Target only losses\n",
    "                f'Sparsity on Target': sft_sparsity,\n",
    "                f'Target Sparsity on Target': target_sft_sparsity,\n",
    "                f'Target Dead Features': num_sft_dead_features,\n",
    "            })\n",
    "        wandb.log(wandb_log)\n",
    "    i+=1\n",
    "    \n",
    "                \n",
    "    \n",
    "    num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "    if(num_tokens_so_far > max_num_tokens):\n",
    "        print(f\"Reached max number of tokens: {max_num_tokens}\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log total average loss and finish wandb\n",
    "wandb_log = {\n",
    "    'SAE Average Loss': auto_total_loss/i,\n",
    "    'Target SAE Average Loss': target_total_loss/i,\n",
    "    \n",
    "    'SAE Average Loss on Base': auto_base_loss/i,\n",
    "    'Target SAE Average Loss on Base': target_base_loss/i,\n",
    "    \n",
    "    'SAE Average Loss on Target': auto_sft_loss/i,\n",
    "    'Target SAE Average Loss on Target': target_sft_loss/i,\n",
    "    }\n",
    "for mode in modes:\n",
    "    wandb_log.update({  # Target SAE log\n",
    "                    f'{mode} Average Loss': total_losses[mode]/i,\n",
    "                })\n",
    "    \n",
    "wandb.log(wandb_log)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Prints the nicely formatted dictionary\n",
    "pprint.pprint(wandb_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_and_target_losses = [\n",
    "    auto_total_loss,\n",
    "    auto_base_loss,\n",
    "    auto_sft_loss,\n",
    "    target_base_loss,\n",
    "    target_sft_loss,\n",
    "    target_total_loss\n",
    "]\n",
    "\n",
    "print([x/i for x in auto_and_target_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dead features\n",
    "import os\n",
    "if not os.path.exists(\"trained_models\"):\n",
    "    os.makedirs(\"trained_models\")\n",
    "# Save model\n",
    "# torch.save(dead_features, f\"trained_models/base_dead_features_70m.pt\")\n",
    "torch.save(target_dead_features, f\"trained_models/sft_on_base_dead_features_70m.pt\")\n",
    "# torch.save(sft_dead_features, f\"trained_models/sft_dead_features_70m.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
