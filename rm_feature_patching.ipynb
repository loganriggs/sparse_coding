{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'd like to load in the model & all sae's (I think) & the top-datapoints, then find the diff-features, patch them for all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"reciprocate/dahoas-gptj-rm-static\"\n",
    "# rm = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "rm = AutoModelForSequenceClassification.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# turn off gradients\n",
    "for param in rm.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtyping import TensorType\n",
    "from torch import nn\n",
    "class TiedSAE(nn.Module):\n",
    "    def __init__(self, activation_size, n_dict_components):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Parameter(torch.empty((n_dict_components, activation_size)))\n",
    "        nn.init.xavier_uniform_(self.encoder)\n",
    "        self.encoder_bias = nn.Parameter(torch.zeros((n_dict_components,)))\n",
    "\n",
    "    def get_learned_dict(self):\n",
    "        norms = torch.norm(self.encoder, 2, dim=-1)\n",
    "        return self.encoder / torch.clamp(norms, 1e-8)[:, None]\n",
    "\n",
    "    def encode(self, batch):\n",
    "        c = torch.einsum(\"nd,bd->bn\", self.encoder, batch)\n",
    "        c = c + self.encoder_bias\n",
    "        c = torch.clamp(c, min=0.0)\n",
    "        return c\n",
    "\n",
    "    def decode(self, code: TensorType[\"_batch_size\", \"_n_dict_components\"]) -> TensorType[\"_batch_size\", \"_activation_size\"]:\n",
    "        learned_dict = self.get_learned_dict()\n",
    "        x_hat = torch.einsum(\"nd,bn->bd\", learned_dict, code)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, batch: TensorType[\"_batch_size\", \"_activation_size\"]) -> TensorType[\"_batch_size\", \"_activation_size\"]:\n",
    "        c = self.encode(batch)\n",
    "        x_hat = self.decode(c)\n",
    "        return x_hat, c\n",
    "\n",
    "    def n_dict_components(self):\n",
    "        return self.get_learned_dict().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoders import *\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# ae_model_id = [\"jbrinkma/Pythia-70M-chess_sp51_r4_gpt_neox.layers.1\", \"jbrinkma/Pythia-70M-chess_sp51_r4_gpt_neox.layers.2.mlp\"]\n",
    "model_id = \"jbrinkma/Pythia-70M-deduped-SAEs\"\n",
    "autoencoders = []\n",
    "layers = rm.config.num_hidden_layers\n",
    "cache_names = [(f\"gpt_neox.layers.{i}\", f\"gpt_neox.layers.{i+1}.mlp\") for i in range(layers-1)]\n",
    "num_layers = len(cache_names)\n",
    "cache_names = [item for sublist in cache_names for item in sublist]\n",
    "filenames = [(f\"Pythia-70M-deduped-{i}.pt\", f\"Pythia-70M-deduped-mlp-{i+1}.pt\") for i in range(layers-1)]\n",
    "filenames = [item for sublist in filenames for item in sublist]\n",
    "for filen in filenames:\n",
    "    ae_download_location = hf_hub_download(repo_id=model_id, filename=filen)\n",
    "    autoencoder = torch.load(ae_download_location)\n",
    "    # Freeze autoencoder weights\n",
    "    for param in autoencoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    autoencoders.append(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# Load in original dataset\n",
    "hh = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the long texts\n",
    "index_file_name = \"rm_save_files/index_small_enough.pt\"\n",
    "dataset_size = hh.num_rows\n",
    "if os.path.exists(index_file_name):\n",
    "    index_small_enough = torch.load(index_file_name)\n",
    "else:\n",
    "    index_small_enough = torch.ones(dataset_size, dtype=torch.bool)\n",
    "    threshold = 870 # 99% of chosen data\n",
    "    for ind, text in enumerate(tqdm(hh)):\n",
    "        chosen_text = text[\"chosen\"]\n",
    "        rejected_text = text[\"rejected\"]\n",
    "        #convert to tokens\n",
    "        length_chosen = len(tokenizer(chosen_text)[\"input_ids\"])\n",
    "        length_rejected = len(tokenizer(rejected_text)[\"input_ids\"])\n",
    "        if length_chosen > threshold or length_rejected > threshold:\n",
    "            index_small_enough[ind] = False\n",
    "    # Save the indices\n",
    "    torch.save(index_small_enough, \"rm_save_files/index_small_enough.pt\")\n",
    "hh = hh.select(index_small_enough.nonzero()[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top-k datapoints\n",
    "k = 1000\n",
    "rm_dir = \"rm_save_files\"\n",
    "reward_diffs = torch.load(rm_dir + \"/chosen_rejected_reward_diffs.pt\")\n",
    "_, top_indices = torch.topk(reward_diffs.abs(), k)\n",
    "batch_size = 16\n",
    "hh_dl = DataLoader(hh, batch_size=batch_size, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
