{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import argparse\n",
    "from utils import dotdict\n",
    "from activation_dataset import setup_token_data\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cfg = dotdict()\n",
    "# models: \"EleutherAI/pythia-6.9b\", \"lomahony/eleuther-pythia6.9b-hh-sft\", \"usvsnsp/pythia-6.9b-ppo\", \"Dahoas/gptj-rm-static\", \"reciprocate/dahoas-gptj-rm-static\"\n",
    "# cfg.model_name=\"lomahony/eleuther-pythia6.9b-hh-sft\"\n",
    "cfg.model_name=\"EleutherAI/pythia-6.9b\"\n",
    "cfg.layers=[10]\n",
    "cfg.setting=\"residual\"\n",
    "# cfg.tensor_name=\"gpt_neox.layers.{layer}\" or \"transformer.h.{layer}\"\n",
    "cfg.tensor_name=\"gpt_neox.layers.{layer}\"\n",
    "original_l1_alpha = 8e-4\n",
    "cfg.l1_alpha=original_l1_alpha\n",
    "cfg.sparsity=None\n",
    "cfg.num_epochs=10\n",
    "cfg.model_batch_size=8\n",
    "cfg.lr=1e-3\n",
    "cfg.kl=False\n",
    "cfg.reconstruction=False\n",
    "# cfg.dataset_name=\"NeelNanda/pile-10k\"\n",
    "cfg.dataset_name=\"Elriggs/openwebtext-100k\"\n",
    "cfg.device=\"cuda:0\"\n",
    "cfg.ratio = 4\n",
    "cfg.seed = 0\n",
    "# cfg.device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = [cfg.tensor_name.format(layer=layer) for layer in cfg.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf12790d0fc14718856d920a779d8045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8040eccb7517421d993fc54af58993fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/25.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e60b979888c4c898cb3bf852aad3b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f51ace6df8e4d2d9ba872ba64a49cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in the model\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, GPTJForSequenceClassification\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.model_name)\n",
    "model = model.to(cfg.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# TODO iteratively grab dataset?\n",
    "cfg.max_length = 256\n",
    "token_loader = setup_token_data(cfg, tokenizer, model, seed=cfg.seed)\n",
    "num_tokens = cfg.max_length*cfg.model_batch_size*len(token_loader)\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1 datapoint on model to get the activation size\n",
    "from baukit import Trace\n",
    "\n",
    "text = \"1\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "# Your activation name will be different. In the next cells, we will show you how to find it.\n",
    "with torch.no_grad():\n",
    "    with Trace(model, tensor_names[0]) as ret:\n",
    "        _ = model(tokens)\n",
    "        representation = ret.output\n",
    "        # check if instance tuple\n",
    "        if(isinstance(representation, tuple)):\n",
    "            representation = representation[0]\n",
    "        activation_size = representation.shape[-1]\n",
    "print(f\"Activation size: {activation_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize New autoencoder\n",
    "from autoencoders.learned_dict import TiedSAE, UntiedSAE, AnthropicSAE\n",
    "from torch import nn\n",
    "params = dict()\n",
    "n_dict_components = activation_size*cfg.ratio\n",
    "params[\"encoder\"] = torch.empty((n_dict_components, activation_size), device=cfg.device)\n",
    "nn.init.xavier_uniform_(params[\"encoder\"])\n",
    "\n",
    "params[\"decoder\"] = torch.empty((n_dict_components, activation_size), device=cfg.device)\n",
    "nn.init.xavier_uniform_(params[\"decoder\"])\n",
    "\n",
    "params[\"encoder_bias\"] = torch.empty((n_dict_components,), device=cfg.device)\n",
    "nn.init.zeros_(params[\"encoder_bias\"])\n",
    "\n",
    "params[\"shift_bias\"] = torch.empty((activation_size,), device=cfg.device)\n",
    "nn.init.zeros_(params[\"shift_bias\"])\n",
    "\n",
    "autoencoder = AnthropicSAE(  # TiedSAE, UntiedSAE, AnthropicSAE\n",
    "    # n_feats = n_dict_components, \n",
    "    # activation_size=activation_size,\n",
    "    encoder=params[\"encoder\"],\n",
    "    encoder_bias=params[\"encoder_bias\"],\n",
    "    decoder=params[\"decoder\"],\n",
    "    shift_bias=params[\"shift_bias\"],\n",
    ")\n",
    "autoencoder.to_device(cfg.device)\n",
    "autoencoder.set_grad()\n",
    "# autoencoder.encoder.requires_grad = True\n",
    "# autoencoder.encoder_bias.requires_grad = True\n",
    "# autoencoder.decoder.requires_grad = True\n",
    "# autoencoder.shift_bias.requires_grad = True\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        autoencoder.encoder, \n",
    "        autoencoder.encoder_bias,\n",
    "        autoencoder.decoder,\n",
    "        autoencoder.shift_bias,\n",
    "    ], lr=cfg.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target sparsity to 10% of activation_size if not set\n",
    "if cfg.sparsity is None:\n",
    "    cfg.sparsity = int(activation_size*0.05)\n",
    "    print(f\"Target sparsity: {cfg.sparsity}\")\n",
    "\n",
    "target_lower_sparsity = cfg.sparsity * 0.9\n",
    "target_upper_sparsity = cfg.sparsity * 1.1\n",
    "adjustment_factor = 0.1  # You can set this to whatever you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_bias = autoencoder.encoder_bias.clone().detach()\n",
    "# Wandb setup\n",
    "secrets = json.load(open(\"secrets.json\"))\n",
    "wandb.login(key=secrets[\"wandb_key\"])\n",
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "wandb_run_name = f\"{cfg.model_name}_{start_time[4:]}_{cfg.sparsity}\"  # trim year\n",
    "print(f\"wandb_run_name: {wandb_run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"sparse coding\", config=dict(cfg), name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_since_activation = torch.zeros(autoencoder.encoder.shape[0])\n",
    "total_activations = torch.zeros(autoencoder.encoder.shape[0])\n",
    "max_num_tokens = 100_000_000\n",
    "save_every = 10_000\n",
    "num_saved_so_far = 0\n",
    "# Freeze model parameters \n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "model.to(cfg.device)\n",
    "last_encoder = autoencoder.encoder.clone().detach()\n",
    "for i, batch in enumerate(tqdm(token_loader,total=int(max_num_tokens/(cfg.max_length*cfg.model_batch_size)))):\n",
    "    tokens = batch[\"input_ids\"].to(cfg.device)\n",
    "    with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model\n",
    "        with Trace(model, tensor_names[0]) as ret:\n",
    "            _ = model(tokens)\n",
    "            representation = ret.output\n",
    "            if(isinstance(representation, tuple)):\n",
    "                representation = representation[0]\n",
    "    layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "    # activation_saver.save_batch(layer_activations.clone().cpu().detach())\n",
    "\n",
    "    c = autoencoder.encode(layer_activations)\n",
    "    x_hat = autoencoder.decode(c)\n",
    "    \n",
    "    reconstruction_loss = (x_hat - layer_activations).pow(2).mean()\n",
    "    l1_loss = torch.norm(c, 1, dim=-1).mean()\n",
    "    total_loss = reconstruction_loss + cfg.l1_alpha*l1_loss\n",
    "\n",
    "    time_since_activation += 1\n",
    "    time_since_activation = time_since_activation * (c.sum(dim=0).cpu()==0)\n",
    "    total_activations += c.sum(dim=0).cpu()\n",
    "    if ((i) % 100 == 0): # Check here so first check is model w/o change\n",
    "        # self_similarity = torch.cosine_similarity(c, last_encoder, dim=-1).mean().cpu().item()\n",
    "        # Above is wrong, should be similarity between encoder and last encoder\n",
    "        self_similarity = torch.cosine_similarity(autoencoder.encoder, last_encoder, dim=-1).mean().cpu().item()\n",
    "        last_encoder = autoencoder.encoder.clone().detach()\n",
    "        num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "        with torch.no_grad():\n",
    "            sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            # Count number of dead_features are zero\n",
    "            num_dead_features = (time_since_activation >= min(i, 200)).sum().item()\n",
    "        print(f\"Sparsity: {sparsity:.1f} | Dead Features: {num_dead_features} | Total Loss: {total_loss:.2f} | Reconstruction Loss: {reconstruction_loss:.2f} | L1 Loss: {cfg.l1_alpha*l1_loss:.2f} | l1_alpha: {cfg.l1_alpha:.2e} | Tokens: {num_tokens_so_far} | Self Similarity: {self_similarity:.2f}\")\n",
    "        wandb.log({\n",
    "            'Sparsity': sparsity,\n",
    "            'Dead Features': num_dead_features,\n",
    "            'Total Loss': total_loss.item(),\n",
    "            'Reconstruction Loss': reconstruction_loss.item(),\n",
    "            'L1 Loss': (cfg.l1_alpha*l1_loss).item(),\n",
    "            'l1_alpha': cfg.l1_alpha,\n",
    "            'Tokens': num_tokens_so_far,\n",
    "            'Self Similarity': self_similarity\n",
    "        })\n",
    "        \n",
    "        dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "        \n",
    "        if(num_tokens_so_far > max_num_tokens):\n",
    "            print(f\"Reached max number of tokens: {max_num_tokens}\")\n",
    "            break\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # resample_period = 10000\n",
    "    # if (i % resample_period == 0):\n",
    "    #     # RESAMPLING\n",
    "    #     with torch.no_grad():\n",
    "    #         # Count number of dead_features are zero\n",
    "    #         num_dead_features = (total_activations == 0).sum().item()\n",
    "    #         print(f\"Dead Features: {num_dead_features}\")\n",
    "            \n",
    "    #     if num_dead_features > 0:\n",
    "    #         print(\"Resampling!\")\n",
    "    #         # hyperparams:\n",
    "    #         max_resample_tokens = 1000 # the number of token activations that we consider for inserting into the dictionary\n",
    "    #         # compute loss of model on random subset of inputs\n",
    "    #         resample_loader = setup_token_data(cfg, tokenizer, model, seed=i)\n",
    "    #         num_resample_data = 0\n",
    "\n",
    "    #         resample_activations = torch.empty(0, activation_size)\n",
    "    #         resample_losses = torch.empty(0)\n",
    "\n",
    "    #         for resample_batch in resample_loader:\n",
    "    #             resample_tokens = resample_batch[\"input_ids\"].to(cfg.device)\n",
    "    #             with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model\n",
    "    #                 with Trace(model, tensor_names[0]) as ret:\n",
    "    #                     _ = model(resample_tokens)\n",
    "    #                     representation = ret.output\n",
    "    #                     if(isinstance(representation, tuple)):\n",
    "    #                         representation = representation[0]\n",
    "    #             layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "    #             resample_activations = torch.cat((resample_activations, layer_activations.detach().cpu()), dim=0)\n",
    "\n",
    "    #             c = autoencoder.encode(layer_activations)\n",
    "    #             x_hat = autoencoder.decode(c)\n",
    "                \n",
    "    #             reconstruction_loss = (x_hat - layer_activations).pow(2).mean(dim=-1)\n",
    "    #             l1_loss = torch.norm(c, 1, dim=-1)\n",
    "    #             temp_loss = reconstruction_loss + cfg.l1_alpha*l1_loss\n",
    "                \n",
    "    #             resample_losses = torch.cat((resample_losses, temp_loss.detach().cpu()), dim=0)\n",
    "                \n",
    "    #             num_resample_data +=layer_activations.shape[0]\n",
    "    #             if num_resample_data > max_resample_tokens:\n",
    "    #                 break\n",
    "\n",
    "                \n",
    "    #         # sample num_dead_features vectors of input activations\n",
    "    #         probabilities = resample_losses**2\n",
    "    #         probabilities /= probabilities.sum()\n",
    "    #         sampled_indices = torch.multinomial(probabilities, num_dead_features, replacement=True)\n",
    "    #         new_vectors = resample_activations[sampled_indices]\n",
    "\n",
    "    #         # calculate average encoder norm of alive neurons\n",
    "    #         alive_neurons = list((total_activations!=0))\n",
    "    #         modified_columns = total_activations==0\n",
    "    #         avg_norm = autoencoder.encoder.data[alive_neurons].norm(dim=-1).mean()\n",
    "\n",
    "    #         # replace dictionary and encoder weights with vectors\n",
    "    #         new_vectors = new_vectors / new_vectors.norm(dim=1, keepdim=True)\n",
    "            \n",
    "    #         params_to_modify = [autoencoder.encoder, autoencoder.encoder_bias]\n",
    "\n",
    "    #         current_weights = autoencoder.encoder.data\n",
    "    #         current_weights[modified_columns] = (new_vectors.to(cfg.device) * avg_norm * 0.02)\n",
    "    #         autoencoder.encoder.data = current_weights\n",
    "\n",
    "    #         current_weights = autoencoder.encoder_bias.data\n",
    "    #         current_weights[modified_columns] = 0\n",
    "    #         autoencoder.encoder_bias.data = current_weights\n",
    "            \n",
    "    #         if hasattr(autoencoder, 'decoder'):\n",
    "    #             current_weights = autoencoder.decoder.data\n",
    "    #             current_weights[modified_columns] = new_vectors.to(cfg.device)\n",
    "    #             autoencoder.decoder.data = current_weights\n",
    "    #             params_to_modify += [autoencoder.decoder]\n",
    "\n",
    "    #         for param_group in optimizer.param_groups:\n",
    "    #             for param in param_group['params']:\n",
    "    #                 if any(param is d_ for d_ in params_to_modify):\n",
    "    #                     # Extract the corresponding rows from m and v\n",
    "    #                     m = optimizer.state[param]['exp_avg']\n",
    "    #                     v = optimizer.state[param]['exp_avg_sq']\n",
    "                        \n",
    "    #                     # Update the m and v values for the modified columns\n",
    "    #                     m[modified_columns] = 0  # Reset moving average for modified columns\n",
    "    #                     v[modified_columns] = 0  # Reset squared moving average for modified columns\n",
    "        \n",
    "    #     total_activations = torch.zeros(autoencoder.encoder.shape[0])\n",
    "\n",
    "    if ((i+2) % save_every ==0): # save periodically but before big changes\n",
    "        model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "        save_name = f\"{model_save_name}_sp{cfg.sparsity}_r{cfg.ratio}_{tensor_names[0]}_ckpt{num_saved_so_far}\"  # trim year\n",
    "\n",
    "        # Make directory traiend_models if it doesn't exist\n",
    "        import os\n",
    "        if not os.path.exists(\"trained_models\"):\n",
    "            os.makedirs(\"trained_models\")\n",
    "        # Save model\n",
    "        torch.save(autoencoder, f\"trained_models/{save_name}.pt\")\n",
    "        \n",
    "        num_saved_so_far += 1\n",
    "\n",
    "    # Running sparsity check\n",
    "    # num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "    # if(num_tokens_so_far > 200000):\n",
    "    #     if(i % 100 == 0):\n",
    "    #         with torch.no_grad():\n",
    "    #             sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "    #         if sparsity > target_upper_sparsity:\n",
    "    #             cfg.l1_alpha *= (1 + adjustment_factor)\n",
    "    #         elif sparsity < target_lower_sparsity:\n",
    "    #             cfg.l1_alpha *= (1 - adjustment_factor)\n",
    "    #         # print(f\"Sparsity: {sparsity:.1f} | l1_alpha: {cfg.l1_alpha:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "save_name = f\"{model_save_name}_sp{cfg.sparsity}_r{cfg.ratio}_{tensor_names[0]}\"  # trim year\n",
    "\n",
    "# Make directory traiend_models if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(\"trained_models\"):\n",
    "    os.makedirs(\"trained_models\")\n",
    "# Save model\n",
    "torch.save(autoencoder, f\"trained_models/{save_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
