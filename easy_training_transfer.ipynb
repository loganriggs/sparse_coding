{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import argparse\n",
    "from utils import dotdict\n",
    "from activation_dataset import setup_token_data\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cfg = dotdict()\n",
    "# models: \"EleutherAI/pythia-6.9b\", \"usvsnsp/pythia-6.9b-ppo\", \"lomahony/eleuther-pythia6.9b-hh-sft\", \"reciprocate/dahoas-gptj-rm-static\"\n",
    "# \"EleutherAI/pythia-70m\", \"lomahony/eleuther-pythia70m-hh-sft\"\n",
    "cfg.model_name=\"EleutherAI/pythia-70m\"\n",
    "cfg.target_name=\"lomahony/eleuther-pythia70m-hh-sft\"\n",
    "cfg.layers=[0,1,2,3,4,5]\n",
    "cfg.setting=\"residual\"\n",
    "# cfg.tensor_name=\"gpt_neox.layers.{layer}\"\n",
    "cfg.tensor_name=\"gpt_neox.layers.{layer}\" # \"gpt_neox.layers.{layer}\" (pythia), \"transformer.h.{layer}\" (rm)\n",
    "cfg.target_tensor_name=\"gpt_neox.layers.{layer}\"\n",
    "original_l1_alpha = 8e-4\n",
    "cfg.l1_alpha=original_l1_alpha\n",
    "cfg.l1_alphas=[0, 1e-5, 2e-5, 4e-5, 8e-5, 1e-4, 2e-4, 4e-4, 8e-4, 1e-3, 2e-3, 4e-3, 8e-3]\n",
    "# cfg.l1_alphas=[0, 1e-5, 1e-4, 2e-4, 4e-4, 8e-4, 1e-3, 2e-3, 4e-3, 8e-3]\n",
    "cfg.sparsity=None\n",
    "cfg.num_epochs=10\n",
    "cfg.model_batch_size=8 * 8\n",
    "cfg.lr=1e-3\n",
    "cfg.kl=False\n",
    "cfg.reconstruction=False\n",
    "# cfg.dataset_name=\"NeelNanda/pile-10k\"\n",
    "cfg.dataset_name=\"Elriggs/openwebtext-100k\"\n",
    "cfg.device=\"cuda:0\"\n",
    "cfg.ratio = 4\n",
    "cfg.seed = 0\n",
    "cfg.max_length = 256\n",
    "# cfg.device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = [cfg.tensor_name.format(layer=layer) for layer in cfg.layers]\n",
    "target_tensor_names = [cfg.target_tensor_name.format(layer=layer) for layer in cfg.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.model_name)\n",
    "model = model.to(cfg.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# TODO iteratively grab dataset?\n",
    "cfg.max_length = 256\n",
    "token_loader = setup_token_data(cfg, tokenizer, model, seed=cfg.seed)\n",
    "num_tokens = cfg.max_length*cfg.model_batch_size*len(token_loader)\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1 datapoint on model to get the activation size\n",
    "from baukit import Trace\n",
    "\n",
    "text = \"1\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "# Your activation name will be different. In the next cells, we will show you how to find it.\n",
    "with torch.no_grad():\n",
    "    with Trace(model, tensor_names[0]) as ret:\n",
    "        _ = model(tokens)\n",
    "        representation = ret.output\n",
    "        # check if instance tuple\n",
    "        if(isinstance(representation, tuple)):\n",
    "            representation = representation[0]\n",
    "        activation_size = representation.shape[-1]\n",
    "print(f\"Activation size: {activation_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target sparsity to 10% of activation_size if not set\n",
    "if cfg.sparsity is None:\n",
    "    cfg.sparsity = int(activation_size*0.05)\n",
    "    print(f\"Target sparsity: {cfg.sparsity}\")\n",
    "\n",
    "target_lower_sparsity = cfg.sparsity * 0.9\n",
    "target_upper_sparsity = cfg.sparsity * 1.1\n",
    "adjustment_factor = 0.1  # You can set this to whatever you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def autoencoder_name_from_llm_name(llm_name):\n",
    "#     if llm_name == \"EleutherAI/pythia-6.9b\":\n",
    "#         return \"base_sae_6b\"\n",
    "#     if llm_name == \"EleutherAI/pythia-70m\":\n",
    "#         return \"base_sae_70m\"\n",
    "#     if llm_name == \"lomahony/eleuther-pythia6.9b-hh-sft\":\n",
    "#         return \"sft_sae_6b\"\n",
    "#     if llm_name == \"lomahony/eleuther-pythia70m-hh-sft\":\n",
    "#         return \"sft_sae_70m\"\n",
    "#     if llm_name == \"usvsnsp/pythia-6.9b-ppo\":\n",
    "#         return \"ppo_sae_6b\"\n",
    "#     if llm_name == \"reciprocate/dahoas-gptj-rm-static\":\n",
    "#         return \"rm_sae_gptj\"\n",
    "#     return \"Error\"\n",
    "\n",
    "def autoencoder_name_from_llm_name(llm_name, layer, l1_alpha):\n",
    "    if llm_name == \"EleutherAI/pythia-70m\":\n",
    "        return f\"base_sae_70m_{layer}_{l1_alpha}\"\n",
    "    if llm_name == \"lomahony/eleuther-pythia6.9b-hh-sft\":\n",
    "        pass\n",
    "\n",
    "def autoTED_name_from_llm_name(llm_name, layer, l1_alpha):\n",
    "    if llm_name == \"EleutherAI/pythia-6.9b\":\n",
    "        return \"base_autoTED_6b\"\n",
    "    if llm_name == \"EleutherAI/pythia-70m\":\n",
    "        model_save_name = llm_name.split(\"/\")[-1]\n",
    "        return f\"{model_save_name}_autoTED_{layer}_{l1_alpha}\" \n",
    "        # return \"base_autoTED_70m\"\n",
    "    if llm_name == \"lomahony/eleuther-pythia6.9b-hh-sft\":\n",
    "        return \"sft_autoTED_6b\"\n",
    "    if llm_name == \"lomahony/eleuther-pythia70m-hh-sft\":\n",
    "        return \"sft_autoTED_70m\"\n",
    "    if llm_name == \"usvsnsp/pythia-6.9b-ppo\":\n",
    "        return \"ppo_autoTED_6b\"\n",
    "    if llm_name == \"reciprocate/dahoas-gptj-rm-static\":\n",
    "        return \"rm_autoTED_gptj\"\n",
    "    return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize New autoencoder\n",
    "from autoencoders.learned_dict import TiedSAE, UntiedSAE, AnthropicSAE, TransferSAE\n",
    "from torch import nn\n",
    "\n",
    "if cfg.target_name == cfg.model_name:\n",
    "    target_model = model\n",
    "    print(\"Does target_model==model? \", target_model==model)\n",
    "else:\n",
    "    target_model = AutoModelForCausalLM.from_pretrained(cfg.target_name).cpu()\n",
    "\n",
    "model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "print(f\"Loading autoTED from model {model_save_name}\")\n",
    "save_name = autoTED_name_from_llm_name(cfg.model_name)\n",
    "autoencoder = torch.load(f\"trained_models/{save_name}.pt\")\n",
    "autoencoder.to_device(cfg.device)\n",
    "\n",
    "model_save_name = cfg.target_name.split(\"/\")[-1]\n",
    "print(f\"Loading target autoTED from model {model_save_name}\")\n",
    "save_name = autoTED_name_from_llm_name(cfg.target_name)\n",
    "target_autoencoder = torch.load(f\"trained_models/{save_name}.pt\")\n",
    "target_autoencoder.to_device(cfg.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize New transfer autoencoder\n",
    "from autoencoders.learned_dict import TiedSAE, UntiedSAE, AnthropicSAE, TransferSAE\n",
    "from torch import nn\n",
    "\n",
    "modes = [\"scale\", \"rotation\", \"bias\", \"free\"]\n",
    "transfer_autoencoders = []\n",
    "for mode in modes:\n",
    "    mode_tsae = TransferSAE(\n",
    "        # n_feats = n_dict_components, \n",
    "        # activation_size=activation_size,\n",
    "        autoencoder,\n",
    "        decoder=autoencoder.get_learned_dict().detach().clone(),\n",
    "        decoder_bias=autoencoder.decoder_bias.detach().clone(),\n",
    "        mode=mode,\n",
    "    )\n",
    "    mode_tsae.to_device(cfg.device)\n",
    "    transfer_autoencoders.append(mode_tsae)\n",
    "\n",
    "optimizers = []\n",
    "# Set gradient to true for decoder only- only training decoder on transfer\n",
    "for tsae in transfer_autoencoders:\n",
    "    tsae.set_grad()\n",
    "    optimizers.append(\n",
    "        torch.optim.Adam(tsae.parameters(), lr=cfg.lr)\n",
    "    )\n",
    "    \n",
    "# now add reverse direction\n",
    "\n",
    "reverse_autoencoders = []\n",
    "for mode in modes:\n",
    "    mode_tsae = TransferSAE(\n",
    "        # n_feats = n_dict_components, \n",
    "        # activation_size=activation_size,\n",
    "        target_autoencoder,\n",
    "        decoder=target_autoencoder.get_learned_dict().detach().clone(),\n",
    "        decoder_bias=target_autoencoder.shift_bias.detach().clone(),\n",
    "        mode=mode,\n",
    "    )\n",
    "    mode_tsae.to_device(cfg.device)\n",
    "    reverse_autoencoders.append(mode_tsae)\n",
    "\n",
    "reverse_optimizers = []\n",
    "# Set gradient to true for decoder only- only training decoder on transfer\n",
    "for tsae in reverse_autoencoders:\n",
    "    tsae.set_grad()\n",
    "    reverse_optimizers.append(\n",
    "        torch.optim.Adam(tsae.parameters(), lr=cfg.lr)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb setup\n",
    "secrets = json.load(open(\"secrets.json\"))\n",
    "wandb.login(key=secrets[\"wandb_key\"])\n",
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "wandb_run_name = f\"{cfg.target_name}_transfer_{start_time[4:]}_{cfg.sparsity}\"  # trim year\n",
    "print(f\"wandb_run_name: {wandb_run_name}\")\n",
    "wandb.init(project=\"sparse coding\", config=dict(cfg), name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_activations(model, inputs, layer_name):\n",
    "    acts = []\n",
    "    for tokens in inputs:\n",
    "        with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model\n",
    "            with Trace(model, layer_name) as ret:\n",
    "                _ = model(tokens)\n",
    "                representation = ret.output\n",
    "                if(isinstance(representation, tuple)):\n",
    "                    representation = representation[0]\n",
    "        layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "        acts.append(layer_activations.cpu())\n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_activations(model, target_model, token_loader, cfg, model_on_gpu=True, num_batches=500):\n",
    "    saved_inputs = []\n",
    "    for k, (batch) in enumerate(token_loader):\n",
    "        saved_inputs.append(batch[\"input_ids\"].to(cfg.device))\n",
    "        \n",
    "        if (k+1)%num_batches==0:\n",
    "            # compute base and target model activations\n",
    "            if model == target_model:\n",
    "                base_activations = compute_activations(model, saved_inputs, layer_name=tensor_names[0])\n",
    "                target_activations = base_activations\n",
    "            else:\n",
    "                if model_on_gpu:\n",
    "                    base_activations = compute_activations(model, saved_inputs, layer_name=tensor_names[0])\n",
    "                    model = model.cpu()\n",
    "                    target_model = target_model.to(cfg.device)\n",
    "                target_activations = compute_activations(target_model, saved_inputs, layer_name=target_tensor_names[0])\n",
    "                if not model_on_gpu:\n",
    "                    target_model = target_model.cpu()\n",
    "                    model = model.to(cfg.device)\n",
    "                    base_activations = compute_activations(model, saved_inputs, layer_name=tensor_names[0])\n",
    "                model_on_gpu = not model_on_gpu\n",
    "            \n",
    "            for base_activation, target_activation in zip(base_activations, target_activations):\n",
    "                yield base_activation, target_activation\n",
    "\n",
    "            # wipe saved inputs\n",
    "            saved_inputs = []\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(autoencoder, base_activation, target_activation, transfer_autoencoders, optimizers, step_number, dead_features, last_decoders, log_every=100, log_prefix=\"\"):\n",
    "    i = step_number\n",
    "    c = autoencoder.encode(base_activation.to(cfg.device))\n",
    "    x_hat = autoencoder.decode(c)\n",
    "    \n",
    "    autoencoder_loss = (x_hat - target_activation.to(cfg.device)).pow(2).mean()\n",
    "    dead_features += c.sum(dim=0).cpu()\n",
    "    \n",
    "    wandb_log = {}\n",
    "    \n",
    "    for tsae, mode, optimizer in zip(transfer_autoencoders, modes, optimizers):\n",
    "        x_hat = tsae.decode(c)\n",
    "        \n",
    "        reconstruction_loss = (x_hat - target_activation.to(cfg.device)).pow(2).mean()\n",
    "        total_loss = reconstruction_loss # NO L1 LOSS\n",
    "\n",
    "        if (i % log_every == 0): # Check here so first check is model w/o change\n",
    "            self_similarity = torch.cosine_similarity(tsae.decoder, last_decoders[mode], dim=-1).mean().cpu().item()\n",
    "            last_decoders[mode] = tsae.decoder.clone().detach()\n",
    "            num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "            with torch.no_grad():\n",
    "                sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            print(f\"{log_prefix}Reconstruction Loss: {reconstruction_loss:.2f} | Tokens: {num_tokens_so_far} | Self Similarity: {self_similarity:.2f}\")\n",
    "            wandb_log.update({\n",
    "                f'{log_prefix}{mode} Reconstruction Loss': reconstruction_loss.item(),\n",
    "                f'{log_prefix}{mode} Self Similarity': self_similarity\n",
    "            })\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (i % log_every == 0):\n",
    "        with torch.no_grad():\n",
    "            sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            # Count number of dead_features are zero\n",
    "            num_dead_features = (dead_features == 0).sum().item()\n",
    "        print(f\"{log_prefix}Sparsity: {sparsity:.1f} | Dead Features: {num_dead_features} | Reconstruction Loss: {autoencoder_loss:.2f} | Tokens: {num_tokens_so_far}\")\n",
    "        dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "        wandb_log.update({\n",
    "                f'{log_prefix}SAE Sparsity': sparsity,\n",
    "                f'{log_prefix}Dead Features': num_dead_features,\n",
    "                f'{log_prefix}SAE Reconstruction Loss': autoencoder_loss.item(),\n",
    "                f'{log_prefix}Tokens': num_tokens_so_far,\n",
    "            })\n",
    "        \n",
    "        # wandb.log(wandb_log)\n",
    "    \n",
    "    return wandb_log, dead_features, last_decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transfer autoencoder\n",
    "token_loader = setup_token_data(cfg, tokenizer, model, seed=cfg.seed)\n",
    "dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "target_dead_features = torch.zeros(target_autoencoder.encoder.shape[0])\n",
    "# auto_dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "\n",
    "max_num_tokens = 100_000_000\n",
    "log_every=100\n",
    "# Freeze model parameters \n",
    "model = model.to(cfg.device)\n",
    "model.eval()\n",
    "if target_model != model:\n",
    "    target_model = target_model.cpu()\n",
    "    target_model.eval()\n",
    "    target_model.requires_grad_(False)\n",
    "\n",
    "last_decoders = dict([(modes[i],transfer_autoencoders[i].decoder.clone().detach()) for i in range(len(transfer_autoencoders))])\n",
    "target_last_decoders = dict([(modes[i],reverse_autoencoders[i].decoder.clone().detach()) for i in range(len(reverse_autoencoders))])\n",
    "model_on_gpu = True\n",
    "\n",
    "saved_inputs = []\n",
    "i = 0 # counts all optimization steps\n",
    "num_saved_so_far = 0\n",
    "print(\"starting loop\")\n",
    "for (base_activation, target_activation) in tqdm(generate_activations(model, target_model, token_loader, cfg, model_on_gpu=model_on_gpu, num_batches=500), \n",
    "                                                 total=int(max_num_tokens/(cfg.max_length*cfg.model_batch_size))):\n",
    "    \n",
    "    wandb_log, dead_features, last_decoders = training_step(autoencoder, base_activation, target_activation, transfer_autoencoders, optimizers, i, \n",
    "                                                            dead_features, last_decoders, log_every, log_prefix=\"\")\n",
    "    reverse_wandb_log, target_dead_features, target_last_decoders = training_step(target_autoencoder, target_activation, base_activation, reverse_autoencoders, reverse_optimizers, i, \n",
    "                                                                                  target_dead_features, target_last_decoders, log_every, log_prefix=\"Reverse \")\n",
    "\n",
    "    if len(wandb_log) > 0:\n",
    "        wandb_log.update(reverse_wandb_log)\n",
    "        wandb.log(wandb_log)\n",
    "        \n",
    "    i+=1\n",
    "    \n",
    "    if ((i+2) % 4000==0): # save periodically but before big changes\n",
    "        for tsae, mode in zip(transfer_autoencoders, modes):\n",
    "            model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "            target_save_name = cfg.target_name.split(\"/\")[-1]\n",
    "            save_name = f\"{model_save_name}_{target_save_name}_{mode}_r{cfg.ratio}_{tensor_names[0]}_ckpt{num_saved_so_far}\" \n",
    "\n",
    "            # Make directory trained_models if it doesn't exist\n",
    "            import os\n",
    "            if not os.path.exists(\"trained_models\"):\n",
    "                os.makedirs(\"trained_models\")\n",
    "            # Save model\n",
    "            torch.save(tsae, f\"trained_models/{save_name}.pt\")\n",
    "            \n",
    "        for tsae, mode in zip(reverse_autoencoders, modes):\n",
    "            model_save_name = cfg.target_name.split(\"/\")[-1]\n",
    "            target_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "            save_name = f\"{model_save_name}_{target_save_name}_{mode}_r{cfg.ratio}_{tensor_names[0]}_ckpt{num_saved_so_far}\" \n",
    "\n",
    "            # Make directory trained_models if it doesn't exist\n",
    "            import os\n",
    "            if not os.path.exists(\"trained_models\"):\n",
    "                os.makedirs(\"trained_models\")\n",
    "            # Save model\n",
    "            torch.save(tsae, f\"trained_models/{save_name}.pt\")\n",
    "        \n",
    "        num_saved_so_far += 1\n",
    "                \n",
    "    \n",
    "    num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "    if(num_tokens_so_far > max_num_tokens):\n",
    "        print(f\"Reached max number of tokens: {max_num_tokens}\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model at end\n",
    "    \n",
    "for tsae, mode in zip(transfer_autoencoders, modes):\n",
    "    model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "    target_save_name = cfg.target_name.split(\"/\")[-1]\n",
    "    save_name = f\"{model_save_name}_{target_save_name}_{mode}_r{cfg.ratio}_{tensor_names[0]}\" \n",
    "\n",
    "    # Make directory trained_models if it doesn't exist\n",
    "    import os\n",
    "    if not os.path.exists(\"trained_models\"):\n",
    "        os.makedirs(\"trained_models\")\n",
    "    # Save model\n",
    "    torch.save(tsae, f\"trained_models/{save_name}.pt\")\n",
    "    \n",
    "for tsae, mode in zip(reverse_autoencoders, modes):\n",
    "    model_save_name = cfg.target_name.split(\"/\")[-1]\n",
    "    target_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "    save_name = f\"{model_save_name}_{target_save_name}_{mode}_r{cfg.ratio}_{tensor_names[0]}\" \n",
    "\n",
    "    # Make directory trained_models if it doesn't exist\n",
    "    import os\n",
    "    if not os.path.exists(\"trained_models\"):\n",
    "        os.makedirs(\"trained_models\")\n",
    "    # Save model\n",
    "    torch.save(tsae, f\"trained_models/{save_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
